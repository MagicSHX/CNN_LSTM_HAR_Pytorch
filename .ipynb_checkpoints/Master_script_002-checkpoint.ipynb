{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "download dataset online:\n",
    "Dataset 1: OPPORTUNITY dataset\n",
    "https://archive.ics.uci.edu/ml/datasets/OPPORTUNITY +Activity+Recognition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dependency:\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import urllib\n",
    "import json\n",
    "import wget\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from random import random\n",
    "import zipfile\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_update = 1\n",
    "if cfg_update == 1:\n",
    "    cfg = {}\n",
    "    cfg['OPPORTUNITY_DATASET'] = {}\n",
    "    cfg['OPPORTUNITY_DATASET']['Download_URL'] = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00226/OpportunityUCIDataset.zip'\n",
    "    cfg['OPPORTUNITY_DATASET']['Download_Destination'] = 'Dataset/OpportunityUCIDataset'\n",
    "    cfg['OPPORTUNITY_DATASET']['dataset_folder'] = 'Dataset/OpportunityUCIDataset/OpportunityUCIDataset/dataset'\n",
    "    cfg['OPPORTUNITY_DATASET']['Normalization_parameter'] = 'Dataset/OPPORTUNITY_norm_parameter.csv'\n",
    "    \n",
    "    cfg['SKODA_DATASET'] = {}\n",
    "    cfg['SKODA_DATASET']['Download_URL'] = 'http://har-dataset.org/lib/exe/fetch.php?media=wiki:dataset:skodaminicp:skodaminicp_2015_08.zip'\n",
    "    cfg['SKODA_DATASET']['Download_Destination'] = 'Dataset/SKODA_DATASET'\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #data = json.dumps(data, indent=4)\n",
    "    #print(data)\n",
    "    with open('cfg.json', 'w') as outfile:\n",
    "        json.dump(cfg, outfile)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_cfg_file(cfg_file_path):\n",
    "    with open(cfg_file_path) as json_file:\n",
    "        cfg = json.load(json_file)\n",
    "    return cfg\n",
    "\n",
    "def download_dataset(Source_URL, Destination):\n",
    "    if not os.path.isfile(Destination + '.zip'):\n",
    "        wget.download(Source_URL, Destination + '.zip')\n",
    "    with zipfile.ZipFile(Destination + '.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(Destination)\n",
    "    os.remove(Destination + '.zip')\n",
    "    \n",
    "    \n",
    "def OPPORTUNITY_Data_Loader(OPPORTUNITY_DATASET_FILE_LIST):\n",
    "    df_OPPORTUNITY_DATASET_All = ''\n",
    "    for file_name in OPPORTUNITY_DATASET_FILE_LIST:\n",
    "        if file_name.endswith(\".dat\"):\n",
    "            df_OPPORTUNITY_DATASET_current = pd.read_csv(OPPORTUNITY_DATASET_dataset_folder + '/' + file_name, sep = ' ', header = None)\n",
    "            df_OPPORTUNITY_DATASET_current.columns = OPPORTUNITY_DATASET_column_names\n",
    "            df_OPPORTUNITY_DATASET_current = df_OPPORTUNITY_DATASET_current.interpolate()\n",
    "            df_OPPORTUNITY_DATASET_current.replace(np.nan, 0.0, inplace=True)\n",
    "            df_OPPORTUNITY_DATASET_current['file_name'] = file_name\n",
    "            if type(df_OPPORTUNITY_DATASET_All) != pd.core.frame.DataFrame:\n",
    "                df_OPPORTUNITY_DATASET_All = df_OPPORTUNITY_DATASET_current\n",
    "            else:\n",
    "                df_OPPORTUNITY_DATASET_All = pd.concat([df_OPPORTUNITY_DATASET_All, df_OPPORTUNITY_DATASET_current])\n",
    "    return df_OPPORTUNITY_DATASET_All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [........................................................................] 84028489 / 84028489"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import cfg file:\n",
    "\n",
    "cfg_file_path = 'cfg.json'\n",
    "cfg = import_cfg_file(cfg_file_path)\n",
    "\n",
    "URL_OPPORTUNITY_DATASET = cfg['OPPORTUNITY_DATASET']['Download_URL']\n",
    "Download_Destination_OPPORTUNITY_DATASET = cfg['OPPORTUNITY_DATASET']['Download_Destination']\n",
    "\n",
    "URL_SKODA_DATASET = cfg['SKODA_DATASET']['Download_URL']\n",
    "Download_Destination_SKODA_DATASET = cfg['SKODA_DATASET']['Download_Destination']\n",
    "\n",
    "OPPORTUNITY_DATASET_dataset_folder = cfg['OPPORTUNITY_DATASET']['dataset_folder']\n",
    "\n",
    "OPPORTUNITY_DATASET_Normalization_parameter = cfg['OPPORTUNITY_DATASET']['Normalization_parameter']\n",
    "\n",
    "#download data: only if Dataset is not downloaded yet\n",
    "\"\"\"\n",
    "download_dataset(URL_OPPORTUNITY_DATASET, Download_Destination_OPPORTUNITY_DATASET)\n",
    "download_dataset(URL_SKODA_DATASET, Download_Destination_SKODA_DATASET)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'Dataset/OpportunityUCIDataset/OpportunityUCIDataset/dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-69b670d22f93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mOPPORTUNITY_DATASET_FILE_LIST\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOPPORTUNITY_DATASET_dataset_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mOPPORTUNITY_DATASET_FILE_LIST\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mOPPORTUNITY_DATASET_FILE_LIST_Training\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mOPPORTUNITY_DATASET_FILE_LIST_Validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'Dataset/OpportunityUCIDataset/OpportunityUCIDataset/dataset'"
     ]
    }
   ],
   "source": [
    "OPPORTUNITY_DATASET_FILE_LIST = os.listdir(OPPORTUNITY_DATASET_dataset_folder)\n",
    "OPPORTUNITY_DATASET_FILE_LIST\n",
    "\n",
    "OPPORTUNITY_DATASET_FILE_LIST_Training = []\n",
    "OPPORTUNITY_DATASET_FILE_LIST_Validation = []\n",
    "OPPORTUNITY_DATASET_FILE_LIST_Testing = []\n",
    "\n",
    "\n",
    "for file_name in OPPORTUNITY_DATASET_FILE_LIST:\n",
    "    if file_name.endswith(\".dat\"):\n",
    "        if 'S4' in file_name:\n",
    "            continue\n",
    "        if 'S1' in file_name:\n",
    "            OPPORTUNITY_DATASET_FILE_LIST_Training.append(file_name)\n",
    "        elif 'ADL3' in file_name:\n",
    "            OPPORTUNITY_DATASET_FILE_LIST_Validation.append(file_name)\n",
    "        elif ('ADL4' in file_name) or ('ADL5' in file_name):\n",
    "            OPPORTUNITY_DATASET_FILE_LIST_Testing.append(file_name)\n",
    "        else:\n",
    "            OPPORTUNITY_DATASET_FILE_LIST_Training.append(file_name)\n",
    "\n",
    "            \n",
    "OPPORTUNITY_DATASET_FILE_LIST_Training\n",
    "OPPORTUNITY_DATASET_FILE_LIST_Validation\n",
    "OPPORTUNITY_DATASET_FILE_LIST_Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load column names\n",
    "with open(OPPORTUNITY_DATASET_dataset_folder + '/column_names.txt') as txt_file:\n",
    "    OPPORTUNITY_DATASET_column_name = txt_file.read().splitlines()\n",
    "\n",
    "column_names = []\n",
    "for column_name in OPPORTUNITY_DATASET_column_name:\n",
    "    column_name = re.split('; |: ', column_name)\n",
    "    if column_name[0] == 'Column':\n",
    "        column_names.append(column_name[1])\n",
    "    #print(column_name)\n",
    "#OPPORTUNITY_DATASET_column_name\n",
    "OPPORTUNITY_DATASET_column_names = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPPORTUNITY_column_inclusive = ['InertialMeasurementUnit', 'Locomotion', 'ML_Both_Arms']\n",
    "OPPORTUNITY_column_exclusive = ['Quaternion']\n",
    "\n",
    "OPPORTUNITY_column_name_selected = OPPORTUNITY_DATASET_column_names[0: 37]\n",
    "for column_name in OPPORTUNITY_DATASET_column_names:\n",
    "    inclusive_index = 0\n",
    "    exclusive_index = 1\n",
    "    for keyword_inclusive in OPPORTUNITY_column_inclusive:\n",
    "        if column_name.find(keyword_inclusive) != -1:\n",
    "            inclusive_index = 1\n",
    "            break\n",
    "    for keyword_exclusive in OPPORTUNITY_column_exclusive:\n",
    "        if column_name.find(keyword_exclusive) != -1:\n",
    "            exclusive_index = 0\n",
    "            break\n",
    "    if inclusive_index * exclusive_index == 1:\n",
    "        OPPORTUNITY_column_name_selected.append(column_name)\n",
    "\n",
    "OPPORTUNITY_column_name_selected.append('file_name')\n",
    "OPPORTUNITY_column_name_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPPORTUNITY_DATASET_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load OPPORTUNITY_DATASET_Normalization_parameter\n",
    "if os.path.isfile(OPPORTUNITY_DATASET_Normalization_parameter):\n",
    "    df_OPPORTUNITY_DATASET_All_describe = pd.read_csv(OPPORTUNITY_DATASET_Normalization_parameter, index_col = 0)\n",
    "else:\n",
    "    df_OPPORTUNITY_DATASET_All = OPPORTUNITY_Data_Loader(OPPORTUNITY_DATASET_FILE_LIST)\n",
    "\n",
    "    #df_OPPORTUNITY_DATASET_All.describe()\n",
    "    df_OPPORTUNITY_DATASET_All_describe = df_OPPORTUNITY_DATASET_All.describe()\n",
    "    #release memory\n",
    "    df_OPPORTUNITY_DATASET_All = ''\n",
    "    df_OPPORTUNITY_DATASET_All_describe.to_csv(OPPORTUNITY_DATASET_Normalization_parameter)\n",
    "df_OPPORTUNITY_DATASET_All_describe\n",
    "\n",
    "NORM_MAX_THRESHOLDS = [3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,\n",
    "                       3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,\n",
    "                       3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,\n",
    "                       3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,\n",
    "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
    "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
    "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
    "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
    "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
    "                       250,    25,     200,    5000,   5000,   5000,   5000,   5000,   5000,\n",
    "                       10000,  10000,  10000,  10000,  10000,  10000,  250,    250,    25,\n",
    "                       200,    5000,   5000,   5000,   5000,   5000,   5000,   10000,  10000,\n",
    "                       10000,  10000,  10000,  10000,  250, ]\n",
    "\n",
    "NORM_MIN_THRESHOLDS = [-3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,\n",
    "                       -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,\n",
    "                       -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,\n",
    "                       -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,\n",
    "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
    "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
    "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
    "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
    "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
    "                       -250,   -100,   -200,   -5000,  -5000,  -5000,  -5000,  -5000,  -5000,\n",
    "                       -10000, -10000, -10000, -10000, -10000, -10000, -250,   -250,   -100,\n",
    "                       -200,   -5000,  -5000,  -5000,  -5000,  -5000,  -5000,  -10000, -10000,\n",
    "                       -10000, -10000, -10000, -10000, -250, ]\n",
    "\n",
    "\n",
    "df_OPPORTUNITY_DATASET_All_describe.loc[['min'], [col_name for col_name in OPPORTUNITY_column_name_selected[1: -3]]] = NORM_MIN_THRESHOLDS\n",
    "df_OPPORTUNITY_DATASET_All_describe.loc[['max'], [col_name for col_name in OPPORTUNITY_column_name_selected[1: -3]]] = NORM_MAX_THRESHOLDS\n",
    "\n",
    "df_OPPORTUNITY_DATASET_All_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPPORTUNITY_DATASET_FILE_LIST_Training\n",
    "OPPORTUNITY_DATASET_FILE_LIST_Validation\n",
    "OPPORTUNITY_DATASET_FILE_LIST_Testing\n",
    "\n",
    "\n",
    "df_OPPORTUNITY_DATASET_Training = OPPORTUNITY_Data_Loader(OPPORTUNITY_DATASET_FILE_LIST_Training)\n",
    "df_OPPORTUNITY_DATASET_Validation = OPPORTUNITY_Data_Loader(OPPORTUNITY_DATASET_FILE_LIST_Validation)\n",
    "df_OPPORTUNITY_DATASET_Testing = OPPORTUNITY_Data_Loader(OPPORTUNITY_DATASET_FILE_LIST_Testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = df_OPPORTUNITY_DATASET_Testing['33 Accelerometer LWR accY'].unique()\n",
    "for m in rr:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPPORTUNITY_DATASET_Training.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPPORTUNITY_DATASET_Testing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPPORTUNITY_DATASET_Validation.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPPORTUNITY_DATASET_Training['file_name'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPPORTUNITY_DATASET_All_describe['2 Accelerometer RKN^ accX']['count']\n",
    "df_OPPORTUNITY_DATASET_All_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPPORTUNITY_DATASET_Training['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#len(OPPORTUNITY_column_name_selected)\n",
    "#df_OPPORTUNITY_DATASET_WIP = df_OPPORTUNITY_DATASET[OPPORTUNITY_column_name_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPPORTUNITY_column_name_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OPPORTUNITY_Normalization(df_OPPORTUNITY_DATASET, df_OPPORTUNITY_DS_All_describe, OPPORTUNITY_col_name_selected):\n",
    "    for column_name in OPPORTUNITY_col_name_selected:\n",
    "        if column_name in ['1 MILLISEC', '244 Locomotion', '250 ML_Both_Arms', 'file_name']:\n",
    "            pass\n",
    "        else:\n",
    "            df_OPPORTUNITY_DATASET[column_name] = (df_OPPORTUNITY_DATASET[column_name] - df_OPPORTUNITY_DS_All_describe[column_name]['min']) / (df_OPPORTUNITY_DS_All_describe[column_name]['max'] - df_OPPORTUNITY_DS_All_describe[column_name]['min'])\n",
    "            df_OPPORTUNITY_DATASET[column_name].loc[df_OPPORTUNITY_DATASET[column_name] > 1] = 1\n",
    "            df_OPPORTUNITY_DATASET[column_name].loc[df_OPPORTUNITY_DATASET[column_name] < 0] = 0\n",
    "            \n",
    "    df_OPPORTUNITY_DATASET = df_OPPORTUNITY_DATASET[OPPORTUNITY_col_name_selected]\n",
    "    return df_OPPORTUNITY_DATASET\n",
    "    \n",
    "    \n",
    "df_OPPORTUNITY_DATASET_Training_WIP = OPPORTUNITY_Normalization(df_OPPORTUNITY_DATASET_Training, df_OPPORTUNITY_DATASET_All_describe, OPPORTUNITY_column_name_selected)\n",
    "df_OPPORTUNITY_DATASET_Testing_WIP = OPPORTUNITY_Normalization(df_OPPORTUNITY_DATASET_Testing, df_OPPORTUNITY_DATASET_All_describe, OPPORTUNITY_column_name_selected)\n",
    "df_OPPORTUNITY_DATASET_Validation_WIP = OPPORTUNITY_Normalization(df_OPPORTUNITY_DATASET_Validation, df_OPPORTUNITY_DATASET_All_describe, OPPORTUNITY_column_name_selected)\n",
    "\n",
    "\n",
    "Classifications = df_OPPORTUNITY_DATASET_Training_WIP['250 ML_Both_Arms'].unique()\n",
    "dict_18_classification = {}\n",
    "class_index = 0\n",
    "for Classitication in Classifications:\n",
    "    dict_18_classification[Classitication] = class_index\n",
    "    class_index += 1\n",
    "\n",
    "df_OPPORTUNITY_DATASET_Training_WIP['250 ML_Both_Arms'] = df_OPPORTUNITY_DATASET_Training_WIP['250 ML_Both_Arms'].apply(lambda  x: dict_18_classification[x])\n",
    "df_OPPORTUNITY_DATASET_Testing_WIP['250 ML_Both_Arms'] = df_OPPORTUNITY_DATASET_Testing_WIP['250 ML_Both_Arms'].apply(lambda  x: dict_18_classification[x])\n",
    "df_OPPORTUNITY_DATASET_Validation_WIP['250 ML_Both_Arms'] = df_OPPORTUNITY_DATASET_Validation_WIP['250 ML_Both_Arms'].apply(lambda  x: dict_18_classification[x])\n",
    "\n",
    "df_OPPORTUNITY_DATASET_Validation_WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SILDE_WINDOW = 24\n",
    "SILDE_WINDOW_STEP = 12\n",
    "\n",
    "\n",
    "def data_generator(df_OPPORTUNITY_DATASET, SILDE_WINDOW, SILDE_WINDOW_STEP, OPPORTUNITY_column_name_selected):\n",
    "    file_no = 0\n",
    "    for file_name in df_OPPORTUNITY_DATASET['file_name'].unique():\n",
    "        file_no += 1\n",
    "        print(file_no)\n",
    "        df_OPPORTUNITY_DATASET_WIP = df_OPPORTUNITY_DATASET.loc[df_OPPORTUNITY_DATASET['file_name'] == file_name]\n",
    "        df_OPPORTUNITY_DATASET_WIP = df_OPPORTUNITY_DATASET_WIP[OPPORTUNITY_column_name_selected[1: ]]\n",
    "        row_no_start = 0\n",
    "        \n",
    "        \n",
    "        while (row_no_start + SILDE_WINDOW - 1) <= df_OPPORTUNITY_DATASET_WIP['2 Accelerometer RKN^ accX'].count():\n",
    "            row_no_end = row_no_start + SILDE_WINDOW\n",
    "            \n",
    "            df_input = df_OPPORTUNITY_DATASET_WIP[OPPORTUNITY_column_name_selected[1: -3]][row_no_start: row_no_end]\n",
    "            current_input = torch.tensor([df_input.values])\n",
    "            \n",
    "            df_output_Locomotion = df_OPPORTUNITY_DATASET_WIP['244 Locomotion'][row_no_end - 1]\n",
    "            \n",
    "            current_output_Locomotion = torch.tensor([df_output_Locomotion])\n",
    "            #print(current_output_Locomotion)\n",
    "            df_output_ML_Both_Arms = df_OPPORTUNITY_DATASET_WIP['250 ML_Both_Arms'][row_no_end - 1]\n",
    "            current_output_ML_Both_Arms = torch.tensor([df_output_ML_Both_Arms])\n",
    "            \n",
    "            \n",
    "            if row_no_start == 0 and file_no == 1:\n",
    "                input_tensor = current_input\n",
    "                output_Locomotion_tensor = current_output_Locomotion\n",
    "                output_ML_Both_Arms_tensor = current_output_ML_Both_Arms\n",
    "            else:\n",
    "                input_tensor = torch.cat((input_tensor, current_input), 0)\n",
    "                output_Locomotion_tensor = torch.cat((output_Locomotion_tensor, current_output_Locomotion), 0)\n",
    "                output_ML_Both_Arms_tensor = torch.cat((output_ML_Both_Arms_tensor, current_output_ML_Both_Arms), 0)\n",
    "                \n",
    "            row_no_start += SILDE_WINDOW_STEP\n",
    "            #print(input_tensor.size())\n",
    "            #print(input_tensor)\n",
    "    return input_tensor, output_Locomotion_tensor, output_ML_Both_Arms_tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Training_x, Training_y_1, Training_y_2 = data_generator(df_OPPORTUNITY_DATASET_Training_WIP, SILDE_WINDOW, SILDE_WINDOW_STEP, OPPORTUNITY_column_name_selected)\n",
    "Training_x = torch.reshape(Training_x, (Training_x.size()[0], 1, Training_x.size()[1], Training_x.size()[2])).float()\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Testing_x, Testing_y_1, Testing_y_2 = data_generator(df_OPPORTUNITY_DATASET_Testing_WIP, SILDE_WINDOW, SILDE_WINDOW_STEP, OPPORTUNITY_column_name_selected)\n",
    "Testing_x = torch.reshape(Testing_x, (Testing_x.size()[0], 1, Testing_x.size()[1], Testing_x.size()[2])).float()\n",
    "\n",
    "Validation_x, Validation_y_1, Validation_y_2 = data_generator(df_OPPORTUNITY_DATASET_Validation_WIP, SILDE_WINDOW, SILDE_WINDOW_STEP, OPPORTUNITY_column_name_selected)\n",
    "Validation_x = torch.reshape(Validation_x, (Validation_x.size()[0], 1, Validation_x.size()[1], Validation_x.size()[2])).float()\n",
    "\"\"\"\n",
    "#def batch_generator():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_OPPORTUNITY_DATASET_Validation_WIP[OPPORTUNITY_column_name_selected[1: -3]][1: 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_OPPORTUNITY_DATASET_WIP['22 Accelerometer RKN_ accZ'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_csv(tensor_name, file_name):\n",
    "    x_np = tensor_name.cpu().detach().numpy()\n",
    "    x_df = pd.DataFrame(x_np)\n",
    "    x_df.to_csv('export/' + file_name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ..from file C:/Users/HX/Magic/oppChallenge_gestures.data\n",
      " ..reading instances: train (557963, 113), test (118750, 113)\n"
     ]
    }
   ],
   "source": [
    "#import lasagne\n",
    "#import theano\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import _pickle as cp\n",
    "#import theano.tensor as T\n",
    "\n",
    "def load_dataset(filename):\n",
    "\n",
    "    f = open(filename, 'rb')\n",
    "    data = cp.load(f)\n",
    "    f.close()\n",
    "\n",
    "    X_train, y_train = data[0]\n",
    "    X_test, y_test = data[1]\n",
    "\n",
    "    print(\" ..from file {}\".format(filename))\n",
    "    print(\" ..reading instances: train {0}, test {1}\".format(X_train.shape, X_test.shape))\n",
    "\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "\n",
    "    # The targets are casted to int8 for GPU compatibility.\n",
    "    y_train = y_train.astype(np.uint8)\n",
    "    y_test = y_test.astype(np.uint8)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "filename = 'C:/Users/HX/Magic/oppChallenge_gestures.data'\n",
    "X_train, y_train, X_test, y_test = load_dataset(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train[0:5000]).to_csv('sdf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SILDE_WINDOW = 24\n",
    "SILDE_WINDOW_STEP = 12\n",
    "\n",
    "\n",
    "def data_generator(ARRAY_OPPORTUNITY_DATASET_x, ARRAY_OPPORTUNITY_DATASET_y, SILDE_WINDOW, SILDE_WINDOW_STEP):\n",
    "    row_no_start = 0\n",
    "    count_index = 0\n",
    "    while (row_no_start + SILDE_WINDOW - 1) <= len(ARRAY_OPPORTUNITY_DATASET_x):\n",
    "        count_index += 1\n",
    "        if count_index % 500 == 0:\n",
    "            print(count_index)\n",
    "        row_no_end = row_no_start + SILDE_WINDOW\n",
    "        df_input = ARRAY_OPPORTUNITY_DATASET_x[row_no_start: row_no_end]\n",
    "        current_input = torch.tensor([df_input])\n",
    "        df_output_Locomotion = ARRAY_OPPORTUNITY_DATASET_y[row_no_end - 1]\n",
    "        df_output_ML_Both_Arms = torch.tensor([df_output_Locomotion])\n",
    "        current_output_ML_Both_Arms = torch.tensor([df_output_ML_Both_Arms])\n",
    "\n",
    "\n",
    "        if row_no_start == 0:\n",
    "            input_tensor = current_input\n",
    "            output_ML_Both_Arms_tensor = current_output_ML_Both_Arms\n",
    "        else:\n",
    "            input_tensor = torch.cat((input_tensor, current_input), 0)\n",
    "            output_ML_Both_Arms_tensor = torch.cat((output_ML_Both_Arms_tensor, current_output_ML_Both_Arms), 0)\n",
    "        row_no_start += SILDE_WINDOW_STEP\n",
    "    return input_tensor, output_ML_Both_Arms_tensor\n",
    "\n",
    "\n",
    "#X_train_tensor_1, y_train_tensor_1 = data_generator(X_train[0: 40000], y_train[0: 40000], SILDE_WINDOW, SILDE_WINDOW_STEP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-c1281caae379>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mX_train_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSILDE_WINDOW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSILDE_WINDOW_STEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'export/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'y_target'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "SILDE_WINDOW = 24\n",
    "SILDE_WINDOW_STEP = 12\n",
    "\n",
    "def data_generator(ARRAY_OPPORTUNITY_DATASET_x, ARRAY_OPPORTUNITY_DATASET_y, SILDE_WINDOW, SILDE_WINDOW_STEP):\n",
    "    ARRAY_OPPORTUNITY_DATASET_x = torch.tensor(ARRAY_OPPORTUNITY_DATASET_x).float()\n",
    "    ARRAY_OPPORTUNITY_DATASET_y = torch.tensor(ARRAY_OPPORTUNITY_DATASET_y)\n",
    "    print(type(ARRAY_OPPORTUNITY_DATASET_x))\n",
    "    array_len = len(ARRAY_OPPORTUNITY_DATASET_x)\n",
    "    shape_0 = int(array_len / SILDE_WINDOW)\n",
    "    shape_1 = SILDE_WINDOW\n",
    "    shape_2 = len(ARRAY_OPPORTUNITY_DATASET_x[0])\n",
    "    input_tensor = torch.reshape(ARRAY_OPPORTUNITY_DATASET_x[0: shape_0 * SILDE_WINDOW], (shape_0, shape_1, shape_2))\n",
    "    print(type(input_tensor))\n",
    "    output_tensor = torch.reshape(ARRAY_OPPORTUNITY_DATASET_y[0: shape_0 * SILDE_WINDOW], (shape_0, shape_1))\n",
    "    \n",
    "    output_tensor = output_tensor[:,-1]\n",
    "\n",
    "    ARRAY_OPPORTUNITY_DATASET_x = ARRAY_OPPORTUNITY_DATASET_x[SILDE_WINDOW_STEP: ]\n",
    "    array_len = len(ARRAY_OPPORTUNITY_DATASET_x)\n",
    "    shape_0 = int(array_len / SILDE_WINDOW)\n",
    "    shape_1 = SILDE_WINDOW\n",
    "    shape_2 = len(ARRAY_OPPORTUNITY_DATASET_x[0])\n",
    "    input_tensor = torch.cat((input_tensor, torch.reshape(ARRAY_OPPORTUNITY_DATASET_x[0: shape_0 * SILDE_WINDOW], (shape_0, shape_1, shape_2))), 0)\n",
    "    \n",
    "    ARRAY_OPPORTUNITY_DATASET_y = ARRAY_OPPORTUNITY_DATASET_y[SILDE_WINDOW_STEP: ]\n",
    "    \n",
    "    output_tensor_2 = torch.reshape(ARRAY_OPPORTUNITY_DATASET_y[0: shape_0 * SILDE_WINDOW], (shape_0, shape_1))\n",
    "    output_tensor = torch.cat((output_tensor, output_tensor_2[:,-1]), 0)\n",
    "    \n",
    "    return input_tensor, output_tensor\n",
    "    \n",
    "    \n",
    "X_train_tensor, y_train_tensor = data_generator(X_train, y_train, SILDE_WINDOW, SILDE_WINDOW_STEP)\n",
    "\n",
    "pd.DataFrame(y_train_tensor.numpy()).to_csv('export/' + 'y_target' + '.csv')\n",
    "\n",
    "df_target = pd.DataFrame(y_train_tensor.numpy())\n",
    "df_target.columns = ['classification']\n",
    "class_count = df_target['classification'].value_counts()\n",
    "class_count = class_count.sort_index(0)\n",
    "class_count = class_count.tolist()\n",
    "class_count = 1 / (np.array(class_count) / (sum(class_count) / len(class_count)))\n",
    "class_count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(class_count)\n",
    "#df_target['abc'].nunique()\n",
    "#df_target\n",
    "#print(X_train_tensor[0])\n",
    "#input_tensor.size()\n",
    "#output_tensor.size()\n",
    "#print(type(class_count))\n",
    "\n",
    "#Class_index = list(class_count.index.values)\n",
    "\n",
    "\n",
    "\n",
    "pd.DataFrame(X_train_tensor[0]).to_csv('0.csv')\n",
    "\n",
    "pd.DataFrame(y_train).to_csv('y.csv')\n",
    "\n",
    "Validation_x = X_train_tensor\n",
    "Validation_x = torch.reshape(Validation_x, (Validation_x.size()[0], 1, Validation_x.size()[1], Validation_x.size()[2])).float()\n",
    "\n",
    "Validation_y_2 = y_train_tensor\n",
    "\n",
    "\n",
    "pd.DataFrame(Validation_x[0, 0, :, :]).to_csv('0v.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216094, 30)\n",
      "[2, 3, 4, 9, 10, 11, 16, 17, 18, 23, 24, 25, 30, 31, 32, 37, 38, 39, 44, 45, 46, 51, 52, 53, 58, 59, 60, 65, 66, 67]\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "cfg_update = 1\n",
    "if cfg_update == 1:\n",
    "    cfg = {}\n",
    "    cfg['OPPORTUNITY_DATASET'] = {}\n",
    "    cfg['OPPORTUNITY_DATASET']['Download_URL'] = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00226/OpportunityUCIDataset.zip'\n",
    "    cfg['OPPORTUNITY_DATASET']['Download_Destination'] = 'Dataset/OpportunityUCIDataset'\n",
    "    cfg['OPPORTUNITY_DATASET']['dataset_folder'] = 'Dataset/OpportunityUCIDataset/OpportunityUCIDataset/dataset'\n",
    "    cfg['OPPORTUNITY_DATASET']['Normalization_parameter'] = 'Dataset/OPPORTUNITY_norm_parameter.csv'\n",
    "    \n",
    "    cfg['SKODA_DATASET'] = {}\n",
    "    cfg['SKODA_DATASET']['Download_URL'] = 'http://har-dataset.org/lib/exe/fetch.php?media=wiki:dataset:skodaminicp:skodaminicp_2015_08.zip'\n",
    "    cfg['SKODA_DATASET']['Download_Destination'] = 'Dataset/SKODA_DATASET'\n",
    "    cfg['SKODA_DATASET']['dataset_path'] = 'Dataset/SKODA_DATASET/SkodaMiniCP_2015_08/right_classall_clean.mat'\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #data = json.dumps(data, indent=4)\n",
    "    #print(data)\n",
    "    with open('cfg.json', 'w') as outfile:\n",
    "        json.dump(cfg, outfile)\n",
    "\n",
    "\n",
    "\n",
    "SKODA_DATASET_dataset_path = cfg['SKODA_DATASET']['dataset_path']\n",
    "\n",
    "import scipy.io\n",
    "right_classall_clean_mat = scipy.io.loadmat(SKODA_DATASET_dataset_path)\n",
    "right_classall_clean_array = right_classall_clean_mat['right_classall_clean']\n",
    "\n",
    "SKODA_DATASET_dataset_size = right_classall_clean_array.shape[0]\n",
    "\n",
    "\n",
    "SKODA_Selected_row = [int(k * (98 / 30)) for k in range(0, int(SKODA_DATASET_dataset_size / (98 / 30)) + 1)]\n",
    "#print(SKODA_Selected_row)\n",
    "\n",
    "no_of_sensor = 10\n",
    "SKODA_Selected_column_x = [2 + s * 7 for s in range(no_of_sensor)] + [3 + s * 7 for s in range(no_of_sensor)] + [4 + s * 7 for s in range(no_of_sensor)]\n",
    "SKODA_Selected_column_x.sort()\n",
    "SKODA_Selected_column_y = 0\n",
    "\n",
    "right_classall_clean_array = right_classall_clean_array[SKODA_Selected_row,:].astype(np.float)\n",
    "\n",
    "right_classall_clean_array_x = right_classall_clean_array[:, SKODA_Selected_column_x]\n",
    "right_classall_clean_array_y = right_classall_clean_array[:, SKODA_Selected_column_y]\n",
    "\n",
    "\n",
    "print(right_classall_clean_array_x.shape)\n",
    "print(SKODA_Selected_column_x)\n",
    "right_classall_clean_array\n",
    "SKODA_Selected_column_norm = []\n",
    "for column in range(len(right_classall_clean_array_x[0])):\n",
    "    max_value = max(right_classall_clean_array_x[:, column])\n",
    "    min_value = min(right_classall_clean_array_x[:, column])\n",
    "    SKODA_Selected_column_norm.append([max_value, min_value, (max_value - min_value)])\n",
    "    #right_classall_clean_array_x[:, column] = (right_classall_clean_array_x[:, column] - min_value) / (max_value - min_value)\n",
    "    \n",
    "len(SKODA_Selected_column_norm)\n",
    "\n",
    "right_classall_clean_array_x.shape\n",
    "\n",
    "\n",
    "SKODA_original_class = np.unique(right_classall_clean_array_y)\n",
    "dict_SKODA_class = {}\n",
    "for i in range(len(SKODA_original_class)):\n",
    "    dict_SKODA_class[SKODA_original_class[i]] = i\n",
    "for i in range(len(right_classall_clean_array_y)):\n",
    "    right_classall_clean_array_y[i] = dict_SKODA_class[right_classall_clean_array_y[i]]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "X_train_tensor, y_train_tensor = data_generator(right_classall_clean_array_x, right_classall_clean_array_y, SILDE_WINDOW, SILDE_WINDOW_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -460.,  -788.,   158.,  ...,   420.,  -778.,  -544.],\n",
       "         [ -460.,  -788.,   158.,  ...,   430.,  -778.,  -544.],\n",
       "         [ -460.,  -788.,   168.,  ...,   430.,  -788.,  -534.],\n",
       "         ...,\n",
       "         [ -550.,  -769.,   118.,  ...,   510.,  -788.,  -495.],\n",
       "         [ -540.,  -769.,    99.,  ...,   490.,  -798.,  -514.],\n",
       "         [ -550.,  -807.,    79.,  ...,   490.,  -807.,  -504.]],\n",
       "\n",
       "        [[ -570.,  -826.,    69.,  ...,   520.,  -826.,  -495.],\n",
       "         [ -560.,  -846.,    39.,  ...,   560.,  -875.,  -455.],\n",
       "         [ -530.,  -846.,    59.,  ...,   490.,  -875.,  -465.],\n",
       "         ...,\n",
       "         [ -430.,  -750.,   178.,  ...,   360.,  -759.,  -554.],\n",
       "         [ -450.,  -769.,   158.,  ...,   360.,  -778.,  -554.],\n",
       "         [ -420.,  -807.,   168.,  ...,   310.,  -788.,  -564.]],\n",
       "\n",
       "        [[ -390.,  -884.,   148.,  ...,   320.,  -846.,  -554.],\n",
       "         [ -410.,  -942.,    79.,  ...,   400.,  -932.,  -514.],\n",
       "         [ -350.,  -903.,    49.,  ...,   350.,  -923.,  -455.],\n",
       "         ...,\n",
       "         [ -440.,  -807.,    69.,  ...,   430.,  -817.,  -455.],\n",
       "         [ -450.,  -807.,    69.,  ...,   440.,  -817.,  -465.],\n",
       "         [ -450.,  -807.,    89.,  ...,   450.,  -817.,  -485.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ -780.,  -615.,    39.,  ...,   700.,  -576.,  -485.],\n",
       "         [ -770.,  -615.,    39.,  ...,   710.,  -548.,  -504.],\n",
       "         [ -800.,  -596.,    59.,  ...,   710.,  -528.,  -504.],\n",
       "         ...,\n",
       "         [ -780.,  -480.,    29.,  ...,   770.,  -432.,  -415.],\n",
       "         [ -810.,  -461.,   -29.,  ...,   780.,  -451.,  -425.],\n",
       "         [ -780.,  -500.,     9.,  ...,   780.,  -500.,  -435.]],\n",
       "\n",
       "        [[ -780.,  -596.,     0.,  ...,   730.,  -500.,  -465.],\n",
       "         [ -820.,  -519.,     0.,  ...,   730.,  -509.,  -455.],\n",
       "         [ -760.,  -500.,   -19.,  ...,   690.,  -538.,  -435.],\n",
       "         ...,\n",
       "         [ -980.,  -423.,  -178.,  ...,   910.,  -490.,  -336.],\n",
       "         [-1060.,  -346.,  -198.,  ...,  1040.,  -384.,  -227.],\n",
       "         [-1070.,  -326.,  -227.,  ...,  1060.,  -346.,  -227.]],\n",
       "\n",
       "        [[-1020.,  -307.,  -168.,  ...,   960.,  -307.,  -257.],\n",
       "         [ -950.,  -250.,  -198.,  ...,  1060.,  -269.,  -227.],\n",
       "         [ -980.,  -288.,  -198.,  ...,  1040.,  -230.,  -178.],\n",
       "         ...,\n",
       "         [-1050.,  -307.,   -89.,  ...,   960.,  -269.,  -316.],\n",
       "         [ -940.,  -384.,   -79.,  ...,   950.,  -326.,  -257.],\n",
       "         [ -920.,  -519.,   -59.,  ...,   780.,  -423.,  -386.]]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor.unique()\n",
    "X_train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "tensor(32., dtype=torch.float64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-5cf189400b9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0my_train_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_SKODA_class\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_train_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0my_train_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: tensor(32., dtype=torch.float64)"
     ]
    }
   ],
   "source": [
    "\n",
    "y_train_tensor.unique()\n",
    "#torch.save({'X_train_tensor': X_train_tensor, 'y_train_tensor': y_train_tensor}, 'C:/Users/HX/Magic/training_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.unique(right_classall_clean_array_y)\n",
    "#dict_SKODA_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data_loading = torch.load(\"D:/Installation/yolov4.pt\")\n",
    "X_train_tensor = data_loading['X_train_tensor']\n",
    "y_train_tensor = data_loading['y_train_tensor']\n",
    "data_loading = ''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_tensor.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pending items:\n",
    "    - batch size = 100\n",
    "    - sensor data were pre-processed to fill in missing values using linear interpolation\n",
    "    - model accuracy & loss function save into array\n",
    "    - Dashboard - to show performance\n",
    "    \n",
    "    - per channel normalization to interval [0,1]\n",
    "    https://stackoverflow.com/questions/26414913/normalize-columns-of-pandas-data-frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input = np.array([random() for i in range(24 * 113 * 1 * 6)]).reshape(6, 1, 24, 113)\n",
    "\n",
    "output = torch.Tensor([[1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                     [0.0, 1.0, 0.0, 0.0, 0.0],\n",
    "                     [0.0, 0.0, 0.0, 1.0, 0.0],\n",
    "                     [0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "                     [0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "                     [0.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "\n",
    "\n",
    "output = torch.Tensor([0, 1, 3, 2, 2, 4])\n",
    "\n",
    "final_input = torch.Tensor(input)\n",
    "final_output = torch.Tensor(output)\n",
    "\"\"\"\n",
    "\n",
    "class CNN_LSTM_HAR_MODEL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn2d_1 = nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size = (5, 1))\n",
    "        #self.conv_bn = nn.BatchNorm2d(64)\n",
    "        #self.relu = nn.ReLU()\n",
    "        self.cnn2d_2 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (5, 1))\n",
    "        self.cnn2d_3 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (5, 1))\n",
    "        self.cnn2d_4 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (5, 1))\n",
    "        \n",
    "        self.batch_size = 24 - 4 * (5 - 1)\n",
    "        self.hidden_dim = 128\n",
    "        self.input_size = 7232\n",
    "        self.input_size = 1920\n",
    "        self.n_layers = 2\n",
    "        \n",
    "        self.class_no = 11\n",
    "        \n",
    "        #self.lstm1 = nn.LSTM(batch_first = True, input_size  = 7232, hidden_size = self.hidden_dim, num_layers = self.n_layers)\n",
    "        self.lstm1 = nn.LSTM(batch_first = True, input_size  = self.input_size, hidden_size = self.hidden_dim, dropout = 0.5)\n",
    "        self.lstm2 = nn.LSTM(batch_first = True, input_size  = 128, hidden_size = self.hidden_dim, dropout = 0.5)\n",
    "        \n",
    "        #self.lstm1 = nn.LSTM(batch_first = True, input_size  = 7232, hidden_size = self.hidden_dim)\n",
    "        #self.lstm2 = nn.LSTM(batch_first = True, input_size  = 128, hidden_size = self.hidden_dim)\n",
    "        \n",
    "        #self.conv_bn2 = nn.BatchNorm1d(8)\n",
    "        #, dropout = 0.5\n",
    "        #self.input_size = 128\n",
    "        #self.n_layers = 1\n",
    "        #self.lstm2 = nn.LSTM(input_size  = self.input_size, hidden_size = self.hidden_dim, num_layers = self.n_layers, dropout = 0.5)\n",
    "        \n",
    "        #self.fc1 = nn.Linear(128, 18)\n",
    "        self.fc1 = nn.Linear(128, self.class_no)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        x = self.cnn2d_1(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.conv_bn(x)\n",
    "        #tensor_to_csv(x[0, -1, :, :], 'layer_1_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :, :], 'layer_1_data_100')\n",
    "        \n",
    "        x = self.cnn2d_2(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.conv_bn(x)\n",
    "        #tensor_to_csv(x[0, -1, :, :], 'layer_2_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :, :], 'layer_2_data_100')\n",
    "\n",
    "        x = self.cnn2d_3(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.conv_bn(x)\n",
    "        #tensor_to_csv(x[0, -1, :, :], 'layer_3_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :, :], 'layer_3_data_100')\n",
    "\n",
    "        x = self.cnn2d_4(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.conv_bn(x)\n",
    "        #tensor_to_csv(x[0, -1, :, :], 'layer_4_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :, :], 'layer_4_data_100')\n",
    "\n",
    "        #print(x.size())\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        #print(x.size())\n",
    "        x = torch.reshape(x, (x.size()[0], x.size()[1], -1))\n",
    "\n",
    "        #hidden = self.init_hidden()\n",
    "        #hidden = (torch.randn(2, x.size()[0], self.hidden_dim).cuda(), torch.randn(2, x.size()[0], self.hidden_dim).cuda())\n",
    "        #x, hidden = self.lstm1(x, hidden)\n",
    "        x, hidden = self.lstm1(x)\n",
    "        x, hidden = self.lstm2(x)\n",
    "        #print(x)\n",
    "        #x = self.conv_bn2(x)\n",
    "        #hidden = self.conv_bn2(hidden)\n",
    "        #tensor_to_csv(x[0, -1, :,], 'layer_lstm1_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :], 'layer_lstm1_data_100')\n",
    "        #print(x)\n",
    "        #hidden2 = (torch.zeros(self.n_layers, 8, self.hidden_dim).requires_grad_(), torch.zeros(self.n_layers, 8, self.hidden_dim).requires_grad_())\n",
    "        ###############################\n",
    "        #x, hidden = self.lstm2(x, hidden)\n",
    "        #print(x)\n",
    "        #x = self.conv_bn2(x)\n",
    "        \n",
    "        #tensor_to_csv(x[0, -1, :,], 'layer_lstm2_data_0')\n",
    "        #tensor_to_csv(x[15, -1, :], 'layer_lstm2_data_100')\n",
    "        #print(x.size())\n",
    "        x = torch.reshape(x, (-1, 128))\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.softmax(x, dim = 1)\n",
    "        #x = F.softmax(x)\n",
    "        x = torch.reshape(x, (int(x.size()[0] / 8), 8, self.class_no))\n",
    "        x = x[:, -1, :]\n",
    "        #print(x.size())\n",
    "        tensor_to_csv(x, 'layer_fc1_data')\n",
    "        \n",
    "        \n",
    "        tensor_to_csv(x, 'output_data')\n",
    "        \n",
    "        \n",
    "        #print(x)\n",
    "        return x\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        hidden = (weight.new(self.n_layers, self.batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, self.batch_size, self.hidden_dim).zero_())\n",
    "        return hidden\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN_LSTM_HAR_MODEL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn2d_1 = nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size = (5, 1))\n",
    "        self.conv_bn = nn.BatchNorm2d(64)\n",
    "        #self.relu = nn.ReLU()\n",
    "        self.cnn2d_2 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (5, 1))\n",
    "        self.cnn2d_3 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (5, 1))\n",
    "        self.cnn2d_4 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (5, 1))\n",
    "        \n",
    "        self.batch_size = 24 - 4 * (5 - 1)\n",
    "        self.hidden_dim = 128\n",
    "        self.input_size = 7232\n",
    "        self.input_size = 1920\n",
    "        self.n_layers = 2\n",
    "        \n",
    "        self.class_no = 11\n",
    "        \n",
    "        #self.lstm1 = nn.LSTM(batch_first = True, input_size  = 7232, hidden_size = self.hidden_dim, num_layers = self.n_layers)\n",
    "        self.lstm1 = nn.LSTM(batch_first = True, input_size  = self.input_size, hidden_size = self.hidden_dim, dropout = 0.5)\n",
    "        self.lstm2 = nn.LSTM(batch_first = True, input_size  = 128, hidden_size = self.hidden_dim, dropout = 0.5)\n",
    "        self.lstm_bn = nn.BatchNorm1d(8)\n",
    "        #self.lstm1 = nn.LSTM(batch_first = True, input_size  = 7232, hidden_size = self.hidden_dim)\n",
    "        #self.lstm2 = nn.LSTM(batch_first = True, input_size  = 128, hidden_size = self.hidden_dim)\n",
    "        \n",
    "        #self.conv_bn2 = nn.BatchNorm1d(8)\n",
    "        #, dropout = 0.5\n",
    "        #self.input_size = 128\n",
    "        #self.n_layers = 1\n",
    "        #self.lstm2 = nn.LSTM(input_size  = self.input_size, hidden_size = self.hidden_dim, num_layers = self.n_layers, dropout = 0.5)\n",
    "        \n",
    "        #self.fc1 = nn.Linear(128, 18)\n",
    "        self.fc1 = nn.Linear(128, self.class_no)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        x = self.cnn2d_1(x)\n",
    "        #x = self.relu(x)\n",
    "        x = self.conv_bn(x)\n",
    "        #tensor_to_csv(x[0, -1, :, :], 'layer_1_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :, :], 'layer_1_data_100')\n",
    "        \n",
    "        x = self.cnn2d_2(x)\n",
    "        #x = self.relu(x)\n",
    "        x = self.conv_bn(x)\n",
    "        #tensor_to_csv(x[0, -1, :, :], 'layer_2_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :, :], 'layer_2_data_100')\n",
    "\n",
    "        x = self.cnn2d_3(x)\n",
    "        #x = self.relu(x)\n",
    "        x = self.conv_bn(x)\n",
    "        #tensor_to_csv(x[0, -1, :, :], 'layer_3_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :, :], 'layer_3_data_100')\n",
    "\n",
    "        x = self.cnn2d_4(x)\n",
    "        #x = self.relu(x)\n",
    "        x = self.conv_bn(x)\n",
    "        #tensor_to_csv(x[0, -1, :, :], 'layer_4_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :, :], 'layer_4_data_100')\n",
    "\n",
    "        #print(x.size())\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        #print(x.size())\n",
    "        x = torch.reshape(x, (x.size()[0], x.size()[1], -1))\n",
    "\n",
    "        #hidden = self.init_hidden()\n",
    "        #hidden = (torch.randn(2, x.size()[0], self.hidden_dim).cuda(), torch.randn(2, x.size()[0], self.hidden_dim).cuda())\n",
    "        #x, hidden = self.lstm1(x, hidden)\n",
    "        x, hidden = self.lstm1(x)\n",
    "        \n",
    "        x = self.lstm_bn(x)\n",
    "        \n",
    "        x, hidden = self.lstm2(x)\n",
    "        \n",
    "        x = self.lstm_bn(x)\n",
    "        \n",
    "        #print(x)\n",
    "        #x = self.conv_bn2(x)\n",
    "        #hidden = self.conv_bn2(hidden)\n",
    "        #tensor_to_csv(x[0, -1, :,], 'layer_lstm1_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :], 'layer_lstm1_data_100')\n",
    "        #print(x)\n",
    "        #hidden2 = (torch.zeros(self.n_layers, 8, self.hidden_dim).requires_grad_(), torch.zeros(self.n_layers, 8, self.hidden_dim).requires_grad_())\n",
    "        ###############################\n",
    "        #x, hidden = self.lstm2(x, hidden)\n",
    "        #print(x)\n",
    "        #x = self.conv_bn2(x)\n",
    "        \n",
    "        #tensor_to_csv(x[0, -1, :,], 'layer_lstm2_data_0')\n",
    "        #tensor_to_csv(x[15, -1, :], 'layer_lstm2_data_100')\n",
    "        #print(x.size())\n",
    "        x = torch.reshape(x, (-1, 128))\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.softmax(x, dim = 1)\n",
    "        #x = F.softmax(x)\n",
    "        x = torch.reshape(x, (int(x.size()[0] / 8), 8, self.class_no))\n",
    "        #x = x[:, -1, :]\n",
    "        #print(x.size())\n",
    "        #tensor_to_csv(x, 'layer_fc1_data')\n",
    "        \n",
    "        \n",
    "        #tensor_to_csv(x, 'output_data')\n",
    "        \n",
    "        \n",
    "        #print(x)\n",
    "        return x[:, -1, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(Validation_y_2.size())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 32.841076, F_score: 0.501094, Accuracy: 0.501555\n",
      "Epoch 2/300, Loss: 32.928497, F_score: 0.467005, Accuracy: 0.500833\n",
      "Epoch 3/300, Loss: 32.974628, F_score: 0.482313, Accuracy: 0.472343\n",
      "Epoch 4/300, Loss: 33.475788, F_score: 0.420452, Accuracy: 0.478007\n",
      "Epoch 5/300, Loss: 32.996571, F_score: 0.489561, Accuracy: 0.501500\n",
      "Epoch 6/300, Loss: 34.142719, F_score: 0.353413, Accuracy: 0.381262\n",
      "Epoch 7/300, Loss: 33.060825, F_score: 0.479750, Accuracy: 0.482895\n",
      "Epoch 8/300, Loss: 34.033405, F_score: 0.438866, Accuracy: 0.469344\n",
      "Epoch 9/300, Loss: 33.579361, F_score: 0.468251, Accuracy: 0.488060\n",
      "Epoch 10/300, Loss: 33.069496, F_score: 0.447696, Accuracy: 0.481062\n",
      "Epoch 11/300, Loss: 32.822548, F_score: 0.513424, Accuracy: 0.532767\n",
      "Epoch 12/300, Loss: 32.415802, F_score: 0.507016, Accuracy: 0.500278\n",
      "Epoch 13/300, Loss: 32.271923, F_score: 0.532269, Accuracy: 0.552483\n",
      "Epoch 14/300, Loss: 32.446381, F_score: 0.505480, Accuracy: 0.498889\n",
      "Epoch 15/300, Loss: 32.574593, F_score: 0.507874, Accuracy: 0.539154\n",
      "Epoch 16/300, Loss: 32.652752, F_score: 0.494492, Accuracy: 0.487449\n",
      "Epoch 17/300, Loss: 32.242458, F_score: 0.527809, Accuracy: 0.558092\n",
      "Epoch 18/300, Loss: 32.219776, F_score: 0.507804, Accuracy: 0.500611\n",
      "Epoch 19/300, Loss: 32.301994, F_score: 0.529071, Accuracy: 0.559869\n",
      "Epoch 20/300, Loss: 32.315067, F_score: 0.512520, Accuracy: 0.499889\n",
      "Epoch 21/300, Loss: 32.903698, F_score: 0.504761, Accuracy: 0.530323\n",
      "Epoch 22/300, Loss: 32.328728, F_score: 0.523502, Accuracy: 0.510441\n",
      "Epoch 23/300, Loss: 32.316219, F_score: 0.537389, Accuracy: 0.552927\n",
      "Epoch 24/300, Loss: 32.399147, F_score: 0.486480, Accuracy: 0.483006\n",
      "Epoch 25/300, Loss: 32.623993, F_score: 0.494141, Accuracy: 0.505831\n",
      "Epoch 26/300, Loss: 32.498875, F_score: 0.485116, Accuracy: 0.490614\n",
      "Epoch 27/300, Loss: 32.369587, F_score: 0.497078, Accuracy: 0.501611\n",
      "Epoch 28/300, Loss: 33.931759, F_score: 0.359658, Accuracy: 0.429468\n",
      "Epoch 29/300, Loss: 32.574780, F_score: 0.485502, Accuracy: 0.468233\n",
      "Epoch 30/300, Loss: 32.570816, F_score: 0.460127, Accuracy: 0.477174\n",
      "Epoch 31/300, Loss: 33.365353, F_score: 0.452890, Accuracy: 0.442019\n",
      "Epoch 32/300, Loss: 32.871513, F_score: 0.483583, Accuracy: 0.509108\n",
      "Epoch 33/300, Loss: 33.437389, F_score: 0.443581, Accuracy: 0.427524\n",
      "Epoch 34/300, Loss: 33.096500, F_score: 0.496610, Accuracy: 0.519494\n",
      "Epoch 35/300, Loss: 35.523197, F_score: 0.256662, Accuracy: 0.299733\n",
      "Epoch 36/300, Loss: 32.426800, F_score: 0.503648, Accuracy: 0.514939\n",
      "Epoch 37/300, Loss: 32.381424, F_score: 0.537892, Accuracy: 0.546596\n",
      "Epoch 38/300, Loss: 31.930519, F_score: 0.556611, Accuracy: 0.560702\n",
      "Epoch 39/300, Loss: 31.864035, F_score: 0.557972, Accuracy: 0.555537\n",
      "Epoch 40/300, Loss: 32.917976, F_score: 0.501923, Accuracy: 0.505665\n",
      "Epoch 41/300, Loss: 33.198612, F_score: 0.477142, Accuracy: 0.481950\n",
      "Epoch 42/300, Loss: 33.014977, F_score: 0.480249, Accuracy: 0.465234\n",
      "Epoch 43/300, Loss: 33.121601, F_score: 0.452297, Accuracy: 0.497667\n",
      "Epoch 44/300, Loss: 32.475761, F_score: 0.515520, Accuracy: 0.518605\n",
      "Epoch 45/300, Loss: 32.358444, F_score: 0.527729, Accuracy: 0.551427\n",
      "Epoch 46/300, Loss: 32.772476, F_score: 0.481708, Accuracy: 0.471121\n",
      "Epoch 47/300, Loss: 32.762409, F_score: 0.491069, Accuracy: 0.527880\n",
      "Epoch 48/300, Loss: 32.404289, F_score: 0.492562, Accuracy: 0.488282\n",
      "Epoch 49/300, Loss: 32.762676, F_score: 0.488568, Accuracy: 0.509441\n",
      "Epoch 50/300, Loss: 32.554260, F_score: 0.481293, Accuracy: 0.485949\n",
      "Epoch 51/300, Loss: 34.323715, F_score: 0.368728, Accuracy: 0.417639\n",
      "Epoch 52/300, Loss: 32.120293, F_score: 0.521849, Accuracy: 0.515051\n",
      "Epoch 53/300, Loss: 32.022415, F_score: 0.541626, Accuracy: 0.554982\n",
      "Epoch 54/300, Loss: 32.648773, F_score: 0.477774, Accuracy: 0.459513\n",
      "Epoch 55/300, Loss: 33.006920, F_score: 0.447364, Accuracy: 0.476786\n",
      "Epoch 56/300, Loss: 33.353912, F_score: 0.443556, Accuracy: 0.415250\n",
      "Epoch 57/300, Loss: 32.938755, F_score: 0.415766, Accuracy: 0.460957\n",
      "Epoch 58/300, Loss: 35.042614, F_score: 0.354430, Accuracy: 0.348384\n",
      "Epoch 59/300, Loss: 32.837261, F_score: 0.476099, Accuracy: 0.518827\n",
      "Epoch 60/300, Loss: 32.811745, F_score: 0.459247, Accuracy: 0.468011\n",
      "Epoch 61/300, Loss: 33.400101, F_score: 0.461762, Accuracy: 0.507442\n",
      "Epoch 62/300, Loss: 32.425167, F_score: 0.493259, Accuracy: 0.485949\n",
      "Epoch 63/300, Loss: 32.509312, F_score: 0.522888, Accuracy: 0.544763\n",
      "Epoch 64/300, Loss: 32.226467, F_score: 0.499356, Accuracy: 0.491281\n",
      "Epoch 65/300, Loss: 32.058819, F_score: 0.546361, Accuracy: 0.553815\n",
      "Epoch 66/300, Loss: 32.185143, F_score: 0.524211, Accuracy: 0.510441\n",
      "Epoch 67/300, Loss: 32.160252, F_score: 0.530134, Accuracy: 0.540431\n",
      "Epoch 68/300, Loss: 32.845295, F_score: 0.496707, Accuracy: 0.485338\n",
      "Epoch 69/300, Loss: 32.255939, F_score: 0.526535, Accuracy: 0.542764\n",
      "Epoch 70/300, Loss: 32.559555, F_score: 0.512550, Accuracy: 0.501555\n",
      "Epoch 71/300, Loss: 31.974764, F_score: 0.542936, Accuracy: 0.551316\n",
      "Epoch 72/300, Loss: 32.096691, F_score: 0.535211, Accuracy: 0.529379\n",
      "Epoch 73/300, Loss: 32.165840, F_score: 0.525978, Accuracy: 0.540598\n",
      "Epoch 74/300, Loss: 32.610764, F_score: 0.472677, Accuracy: 0.472009\n",
      "Epoch 75/300, Loss: 33.135170, F_score: 0.466520, Accuracy: 0.481562\n",
      "Epoch 76/300, Loss: 32.155075, F_score: 0.520523, Accuracy: 0.504110\n",
      "Epoch 77/300, Loss: 32.210243, F_score: 0.528101, Accuracy: 0.546762\n",
      "Epoch 78/300, Loss: 32.203941, F_score: 0.517387, Accuracy: 0.508886\n",
      "Epoch 79/300, Loss: 32.007511, F_score: 0.535802, Accuracy: 0.546540\n",
      "Epoch 80/300, Loss: 32.154305, F_score: 0.506171, Accuracy: 0.506942\n",
      "Epoch 81/300, Loss: 32.644569, F_score: 0.492988, Accuracy: 0.499000\n",
      "Epoch 82/300, Loss: 32.161861, F_score: 0.508673, Accuracy: 0.516106\n",
      "Epoch 83/300, Loss: 32.951748, F_score: 0.479201, Accuracy: 0.492725\n",
      "Epoch 84/300, Loss: 32.625603, F_score: 0.492886, Accuracy: 0.497334\n",
      "Epoch 85/300, Loss: 34.223328, F_score: 0.402540, Accuracy: 0.468122\n",
      "Epoch 86/300, Loss: 34.418995, F_score: 0.369102, Accuracy: 0.395868\n",
      "Epoch 87/300, Loss: 33.221264, F_score: 0.453208, Accuracy: 0.481728\n",
      "Epoch 88/300, Loss: 32.993736, F_score: 0.425494, Accuracy: 0.445074\n",
      "Epoch 89/300, Loss: 32.516926, F_score: 0.508745, Accuracy: 0.529101\n",
      "Epoch 90/300, Loss: 33.071884, F_score: 0.449356, Accuracy: 0.425358\n",
      "Epoch 91/300, Loss: 33.076916, F_score: 0.425477, Accuracy: 0.460402\n",
      "Epoch 92/300, Loss: 32.575237, F_score: 0.480101, Accuracy: 0.466233\n",
      "Epoch 93/300, Loss: 32.892952, F_score: 0.437725, Accuracy: 0.459402\n",
      "Epoch 94/300, Loss: 32.972004, F_score: 0.453751, Accuracy: 0.446684\n",
      "Epoch 95/300, Loss: 32.445286, F_score: 0.490570, Accuracy: 0.498056\n",
      "Epoch 96/300, Loss: 32.201843, F_score: 0.512569, Accuracy: 0.506609\n",
      "Epoch 97/300, Loss: 31.987940, F_score: 0.510519, Accuracy: 0.523992\n",
      "Epoch 98/300, Loss: 32.194679, F_score: 0.501820, Accuracy: 0.497890\n",
      "Epoch 99/300, Loss: 32.226883, F_score: 0.501123, Accuracy: 0.516661\n",
      "Epoch 100/300, Loss: 32.906815, F_score: 0.470269, Accuracy: 0.473842\n",
      "Epoch 101/300, Loss: 31.957487, F_score: 0.531700, Accuracy: 0.528657\n",
      "Epoch 102/300, Loss: 31.778822, F_score: 0.549489, Accuracy: 0.553760\n",
      "Epoch 103/300, Loss: 31.763372, F_score: 0.538419, Accuracy: 0.538432\n",
      "Epoch 104/300, Loss: 31.849331, F_score: 0.537034, Accuracy: 0.548373\n",
      "Epoch 105/300, Loss: 32.581009, F_score: 0.482051, Accuracy: 0.475730\n",
      "Epoch 106/300, Loss: 33.299450, F_score: 0.441027, Accuracy: 0.471732\n",
      "Epoch 107/300, Loss: 32.803970, F_score: 0.498428, Accuracy: 0.479618\n",
      "Epoch 108/300, Loss: 33.062187, F_score: 0.440240, Accuracy: 0.453738\n",
      "Epoch 109/300, Loss: 32.560509, F_score: 0.493712, Accuracy: 0.484894\n",
      "Epoch 110/300, Loss: 34.173016, F_score: 0.306905, Accuracy: 0.348217\n",
      "Epoch 111/300, Loss: 33.622478, F_score: 0.410894, Accuracy: 0.411030\n",
      "Epoch 112/300, Loss: 34.720116, F_score: 0.204226, Accuracy: 0.283628\n",
      "Epoch 113/300, Loss: 33.067604, F_score: 0.464803, Accuracy: 0.459291\n",
      "Epoch 114/300, Loss: 32.964191, F_score: 0.416153, Accuracy: 0.452294\n",
      "Epoch 115/300, Loss: 32.191532, F_score: 0.514803, Accuracy: 0.504998\n",
      "Epoch 116/300, Loss: 32.309204, F_score: 0.479992, Accuracy: 0.495835\n",
      "Epoch 117/300, Loss: 32.570969, F_score: 0.484037, Accuracy: 0.474897\n",
      "Epoch 118/300, Loss: 31.862972, F_score: 0.530550, Accuracy: 0.541986\n",
      "Epoch 119/300, Loss: 32.044773, F_score: 0.507886, Accuracy: 0.507775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/300, Loss: 31.987511, F_score: 0.517710, Accuracy: 0.531601\n",
      "Epoch 121/300, Loss: 32.598316, F_score: 0.483025, Accuracy: 0.492447\n",
      "Epoch 122/300, Loss: 31.824041, F_score: 0.537098, Accuracy: 0.531323\n",
      "Epoch 123/300, Loss: 31.841093, F_score: 0.526829, Accuracy: 0.545429\n",
      "Epoch 124/300, Loss: 32.112427, F_score: 0.509143, Accuracy: 0.502444\n",
      "Epoch 125/300, Loss: 32.178394, F_score: 0.504280, Accuracy: 0.532489\n",
      "Epoch 126/300, Loss: 32.359055, F_score: 0.477889, Accuracy: 0.478007\n",
      "Epoch 127/300, Loss: 32.230728, F_score: 0.500911, Accuracy: 0.521271\n",
      "Epoch 128/300, Loss: 32.159721, F_score: 0.502967, Accuracy: 0.506664\n",
      "Epoch 129/300, Loss: 32.287682, F_score: 0.493577, Accuracy: 0.503332\n",
      "Epoch 130/300, Loss: 32.345695, F_score: 0.496352, Accuracy: 0.510941\n",
      "Epoch 131/300, Loss: 33.116520, F_score: 0.456573, Accuracy: 0.449017\n",
      "Epoch 132/300, Loss: 33.283054, F_score: 0.448658, Accuracy: 0.481395\n",
      "Epoch 133/300, Loss: 33.213657, F_score: 0.446458, Accuracy: 0.442130\n",
      "Epoch 134/300, Loss: 32.201683, F_score: 0.519729, Accuracy: 0.533767\n",
      "Epoch 135/300, Loss: 32.287029, F_score: 0.508802, Accuracy: 0.495446\n",
      "Epoch 136/300, Loss: 32.025620, F_score: 0.522282, Accuracy: 0.541208\n",
      "Epoch 137/300, Loss: 33.133686, F_score: 0.457687, Accuracy: 0.449295\n",
      "Epoch 138/300, Loss: 32.395420, F_score: 0.503505, Accuracy: 0.527657\n",
      "Epoch 139/300, Loss: 32.411121, F_score: 0.473481, Accuracy: 0.476841\n",
      "Epoch 140/300, Loss: 32.241333, F_score: 0.511668, Accuracy: 0.535322\n",
      "Epoch 141/300, Loss: 32.260597, F_score: 0.490073, Accuracy: 0.481006\n",
      "Epoch 142/300, Loss: 32.005314, F_score: 0.528678, Accuracy: 0.547151\n",
      "Epoch 143/300, Loss: 32.013725, F_score: 0.509881, Accuracy: 0.496168\n",
      "Epoch 144/300, Loss: 32.358963, F_score: 0.458206, Accuracy: 0.481006\n",
      "Epoch 145/300, Loss: 32.575478, F_score: 0.492783, Accuracy: 0.473731\n",
      "Epoch 146/300, Loss: 32.461040, F_score: 0.458056, Accuracy: 0.488448\n",
      "Epoch 147/300, Loss: 32.276234, F_score: 0.515969, Accuracy: 0.504721\n",
      "Epoch 148/300, Loss: 32.010525, F_score: 0.510525, Accuracy: 0.515051\n",
      "Epoch 149/300, Loss: 32.012283, F_score: 0.527164, Accuracy: 0.521160\n",
      "Epoch 150/300, Loss: 31.888363, F_score: 0.518069, Accuracy: 0.529546\n",
      "Epoch 151/300, Loss: 32.578362, F_score: 0.472574, Accuracy: 0.477119\n",
      "Epoch 152/300, Loss: 32.193600, F_score: 0.528838, Accuracy: 0.538487\n",
      "Epoch 153/300, Loss: 32.758667, F_score: 0.449094, Accuracy: 0.459069\n",
      "Epoch 154/300, Loss: 32.449722, F_score: 0.491576, Accuracy: 0.499500\n",
      "Epoch 155/300, Loss: 33.017090, F_score: 0.440586, Accuracy: 0.461679\n",
      "Epoch 156/300, Loss: 32.255398, F_score: 0.524831, Accuracy: 0.529046\n",
      "Epoch 157/300, Loss: 32.652721, F_score: 0.465594, Accuracy: 0.479118\n",
      "Epoch 158/300, Loss: 32.083057, F_score: 0.529584, Accuracy: 0.529379\n",
      "Epoch 159/300, Loss: 32.408779, F_score: 0.484450, Accuracy: 0.497556\n",
      "Epoch 160/300, Loss: 32.482010, F_score: 0.511800, Accuracy: 0.509886\n",
      "Epoch 161/300, Loss: 33.910500, F_score: 0.358759, Accuracy: 0.398923\n",
      "Epoch 162/300, Loss: 32.690998, F_score: 0.477566, Accuracy: 0.474286\n",
      "Epoch 163/300, Loss: 33.840290, F_score: 0.417038, Accuracy: 0.460347\n",
      "Epoch 164/300, Loss: 33.284340, F_score: 0.431140, Accuracy: 0.453460\n",
      "Epoch 165/300, Loss: 32.256775, F_score: 0.497345, Accuracy: 0.511163\n",
      "Epoch 166/300, Loss: 31.982048, F_score: 0.514930, Accuracy: 0.518772\n",
      "Epoch 167/300, Loss: 31.648453, F_score: 0.532339, Accuracy: 0.537654\n",
      "Epoch 168/300, Loss: 31.583452, F_score: 0.545180, Accuracy: 0.554926\n",
      "Epoch 169/300, Loss: 31.670742, F_score: 0.526067, Accuracy: 0.521659\n",
      "Epoch 170/300, Loss: 31.881100, F_score: 0.531597, Accuracy: 0.548595\n",
      "Epoch 171/300, Loss: 32.075027, F_score: 0.509087, Accuracy: 0.493724\n",
      "Epoch 172/300, Loss: 32.081028, F_score: 0.501274, Accuracy: 0.522437\n",
      "Epoch 173/300, Loss: 32.411961, F_score: 0.503582, Accuracy: 0.481451\n",
      "Epoch 174/300, Loss: 32.095444, F_score: 0.491591, Accuracy: 0.506664\n",
      "Epoch 175/300, Loss: 32.246620, F_score: 0.517999, Accuracy: 0.507886\n",
      "Epoch 176/300, Loss: 32.437019, F_score: 0.457399, Accuracy: 0.479951\n",
      "Epoch 177/300, Loss: 33.188301, F_score: 0.462604, Accuracy: 0.460235\n",
      "Epoch 178/300, Loss: 33.809799, F_score: 0.403637, Accuracy: 0.420471\n",
      "Epoch 179/300, Loss: 33.292351, F_score: 0.455399, Accuracy: 0.468066\n",
      "Epoch 180/300, Loss: 32.190838, F_score: 0.521038, Accuracy: 0.521049\n",
      "Epoch 181/300, Loss: 32.376808, F_score: 0.485444, Accuracy: 0.494446\n",
      "Epoch 182/300, Loss: 32.574680, F_score: 0.490855, Accuracy: 0.487393\n",
      "Epoch 183/300, Loss: 33.259087, F_score: 0.453739, Accuracy: 0.487948\n",
      "Epoch 184/300, Loss: 32.307003, F_score: 0.496311, Accuracy: 0.477285\n",
      "Epoch 185/300, Loss: 31.945053, F_score: 0.534149, Accuracy: 0.550594\n",
      "Epoch 186/300, Loss: 32.003769, F_score: 0.518318, Accuracy: 0.506553\n",
      "Epoch 187/300, Loss: 31.751108, F_score: 0.525276, Accuracy: 0.538487\n",
      "Epoch 188/300, Loss: 31.936953, F_score: 0.539346, Accuracy: 0.534433\n",
      "Epoch 189/300, Loss: 31.869316, F_score: 0.523900, Accuracy: 0.537543\n",
      "Epoch 190/300, Loss: 31.813480, F_score: 0.533942, Accuracy: 0.532100\n",
      "Epoch 191/300, Loss: 31.707645, F_score: 0.534451, Accuracy: 0.546762\n",
      "Epoch 192/300, Loss: 31.631166, F_score: 0.537244, Accuracy: 0.538598\n",
      "Epoch 193/300, Loss: 31.681456, F_score: 0.536765, Accuracy: 0.545263\n",
      "Epoch 194/300, Loss: 31.681393, F_score: 0.539490, Accuracy: 0.544596\n",
      "Epoch 195/300, Loss: 32.504894, F_score: 0.490732, Accuracy: 0.482228\n",
      "Epoch 196/300, Loss: 31.909531, F_score: 0.560265, Accuracy: 0.574975\n",
      "Epoch 197/300, Loss: 31.717285, F_score: 0.520272, Accuracy: 0.512662\n",
      "Epoch 198/300, Loss: 31.772543, F_score: 0.525865, Accuracy: 0.537154\n",
      "Epoch 199/300, Loss: 32.016758, F_score: 0.510430, Accuracy: 0.496890\n",
      "Epoch 200/300, Loss: 31.983250, F_score: 0.512497, Accuracy: 0.529601\n",
      "Epoch 201/300, Loss: 32.207127, F_score: 0.504261, Accuracy: 0.500666\n",
      "Epoch 202/300, Loss: 32.456669, F_score: 0.486445, Accuracy: 0.492947\n",
      "Epoch 203/300, Loss: 32.919781, F_score: 0.458135, Accuracy: 0.458236\n",
      "Epoch 204/300, Loss: 33.290028, F_score: 0.458517, Accuracy: 0.471010\n",
      "Epoch 205/300, Loss: 34.152199, F_score: 0.364885, Accuracy: 0.405476\n",
      "Epoch 206/300, Loss: 32.606255, F_score: 0.475384, Accuracy: 0.495168\n",
      "Epoch 207/300, Loss: 32.488041, F_score: 0.495334, Accuracy: 0.520660\n",
      "Epoch 208/300, Loss: 31.953188, F_score: 0.510101, Accuracy: 0.511330\n",
      "Epoch 209/300, Loss: 31.719494, F_score: 0.524105, Accuracy: 0.535988\n",
      "Epoch 210/300, Loss: 31.841879, F_score: 0.513069, Accuracy: 0.514662\n",
      "Epoch 211/300, Loss: 31.987516, F_score: 0.522637, Accuracy: 0.530157\n",
      "Epoch 212/300, Loss: 31.759838, F_score: 0.520815, Accuracy: 0.525769\n",
      "Epoch 213/300, Loss: 31.833378, F_score: 0.532058, Accuracy: 0.534933\n",
      "Epoch 214/300, Loss: 31.872332, F_score: 0.506986, Accuracy: 0.518938\n",
      "Epoch 215/300, Loss: 32.823917, F_score: 0.477501, Accuracy: 0.468733\n",
      "Epoch 216/300, Loss: 31.941420, F_score: 0.528285, Accuracy: 0.537099\n",
      "Epoch 217/300, Loss: 31.991199, F_score: 0.535807, Accuracy: 0.526213\n",
      "Epoch 218/300, Loss: 32.150963, F_score: 0.481862, Accuracy: 0.503832\n",
      "Epoch 219/300, Loss: 32.641922, F_score: 0.501288, Accuracy: 0.492169\n",
      "Epoch 220/300, Loss: 32.338722, F_score: 0.498184, Accuracy: 0.520937\n",
      "Epoch 221/300, Loss: 32.211716, F_score: 0.520255, Accuracy: 0.499056\n",
      "Epoch 222/300, Loss: 33.689957, F_score: 0.316426, Accuracy: 0.383928\n",
      "Epoch 223/300, Loss: 32.938408, F_score: 0.475678, Accuracy: 0.466844\n",
      "Epoch 224/300, Loss: 32.716354, F_score: 0.488610, Accuracy: 0.524048\n",
      "Epoch 225/300, Loss: 32.245979, F_score: 0.522280, Accuracy: 0.517161\n",
      "Epoch 226/300, Loss: 32.507710, F_score: 0.488969, Accuracy: 0.503055\n",
      "Epoch 227/300, Loss: 32.244755, F_score: 0.522884, Accuracy: 0.524270\n",
      "Epoch 228/300, Loss: 31.690086, F_score: 0.541483, Accuracy: 0.543930\n",
      "Epoch 229/300, Loss: 31.705889, F_score: 0.535340, Accuracy: 0.538654\n",
      "Epoch 230/300, Loss: 31.983072, F_score: 0.517535, Accuracy: 0.521604\n",
      "Epoch 231/300, Loss: 32.357830, F_score: 0.483786, Accuracy: 0.493613\n",
      "Epoch 232/300, Loss: 33.207962, F_score: 0.432326, Accuracy: 0.445518\n",
      "Epoch 233/300, Loss: 32.160900, F_score: 0.526203, Accuracy: 0.528435\n",
      "Epoch 234/300, Loss: 32.244663, F_score: 0.504920, Accuracy: 0.503332\n",
      "Epoch 235/300, Loss: 31.572262, F_score: 0.552374, Accuracy: 0.556259\n",
      "Epoch 236/300, Loss: 31.559023, F_score: 0.550672, Accuracy: 0.546373\n",
      "Epoch 237/300, Loss: 32.203777, F_score: 0.526557, Accuracy: 0.529324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238/300, Loss: 32.525467, F_score: 0.495372, Accuracy: 0.497168\n",
      "Epoch 239/300, Loss: 31.661386, F_score: 0.556518, Accuracy: 0.554593\n",
      "Epoch 240/300, Loss: 31.516834, F_score: 0.555859, Accuracy: 0.561646\n",
      "Epoch 241/300, Loss: 31.677284, F_score: 0.551594, Accuracy: 0.540153\n",
      "Epoch 242/300, Loss: 31.891716, F_score: 0.529711, Accuracy: 0.540653\n",
      "Epoch 243/300, Loss: 32.215115, F_score: 0.517940, Accuracy: 0.496390\n",
      "Epoch 244/300, Loss: 32.088520, F_score: 0.526822, Accuracy: 0.543874\n",
      "Epoch 245/300, Loss: 32.008972, F_score: 0.541505, Accuracy: 0.532489\n",
      "Epoch 246/300, Loss: 33.864536, F_score: 0.382389, Accuracy: 0.439742\n",
      "Epoch 247/300, Loss: 32.854984, F_score: 0.526819, Accuracy: 0.520937\n",
      "Epoch 248/300, Loss: 32.909481, F_score: 0.450345, Accuracy: 0.464623\n",
      "Epoch 249/300, Loss: 31.920507, F_score: 0.544380, Accuracy: 0.535821\n",
      "Epoch 250/300, Loss: 31.834621, F_score: 0.509112, Accuracy: 0.519771\n",
      "Epoch 251/300, Loss: 32.477554, F_score: 0.506181, Accuracy: 0.482672\n",
      "Epoch 252/300, Loss: 32.715214, F_score: 0.488473, Accuracy: 0.516550\n",
      "Epoch 253/300, Loss: 34.001575, F_score: 0.396860, Accuracy: 0.382206\n",
      "Epoch 254/300, Loss: 33.992439, F_score: 0.380631, Accuracy: 0.458292\n",
      "Epoch 255/300, Loss: 32.342251, F_score: 0.516821, Accuracy: 0.503221\n",
      "Epoch 256/300, Loss: 31.987204, F_score: 0.529429, Accuracy: 0.545207\n",
      "Epoch 257/300, Loss: 31.961006, F_score: 0.523058, Accuracy: 0.519049\n",
      "Epoch 258/300, Loss: 32.093575, F_score: 0.495847, Accuracy: 0.506498\n",
      "Epoch 259/300, Loss: 32.988430, F_score: 0.436384, Accuracy: 0.459680\n",
      "Epoch 260/300, Loss: 31.870722, F_score: 0.533512, Accuracy: 0.545429\n",
      "Epoch 261/300, Loss: 31.862816, F_score: 0.540572, Accuracy: 0.537432\n",
      "Epoch 262/300, Loss: 31.581541, F_score: 0.522127, Accuracy: 0.533933\n",
      "Epoch 263/300, Loss: 31.621958, F_score: 0.559683, Accuracy: 0.558925\n",
      "Epoch 264/300, Loss: 31.488060, F_score: 0.527704, Accuracy: 0.538209\n",
      "Epoch 265/300, Loss: 31.583090, F_score: 0.561836, Accuracy: 0.561757\n",
      "Epoch 266/300, Loss: 31.568636, F_score: 0.526554, Accuracy: 0.534044\n",
      "Epoch 267/300, Loss: 31.975321, F_score: 0.530843, Accuracy: 0.525880\n",
      "Epoch 268/300, Loss: 31.826670, F_score: 0.521043, Accuracy: 0.526769\n",
      "Epoch 269/300, Loss: 32.660248, F_score: 0.488641, Accuracy: 0.473509\n",
      "Epoch 270/300, Loss: 31.631096, F_score: 0.571148, Accuracy: 0.582861\n",
      "Epoch 271/300, Loss: 31.514719, F_score: 0.547683, Accuracy: 0.542264\n",
      "Epoch 272/300, Loss: 31.405479, F_score: 0.552398, Accuracy: 0.565811\n",
      "Epoch 273/300, Loss: 32.589100, F_score: 0.507163, Accuracy: 0.502166\n",
      "Epoch 274/300, Loss: 32.610615, F_score: 0.484540, Accuracy: 0.493058\n",
      "Epoch 275/300, Loss: 32.955708, F_score: 0.496799, Accuracy: 0.486227\n",
      "Epoch 276/300, Loss: 32.886993, F_score: 0.491333, Accuracy: 0.506664\n",
      "Epoch 277/300, Loss: 35.895649, F_score: 0.268764, Accuracy: 0.298845\n",
      "Epoch 278/300, Loss: 33.667706, F_score: 0.398818, Accuracy: 0.449517\n",
      "Epoch 279/300, Loss: 32.807732, F_score: 0.428502, Accuracy: 0.468288\n",
      "Epoch 280/300, Loss: 31.833721, F_score: 0.517374, Accuracy: 0.520549\n",
      "Epoch 281/300, Loss: 31.835476, F_score: 0.542618, Accuracy: 0.553926\n",
      "Epoch 282/300, Loss: 31.800594, F_score: 0.518832, Accuracy: 0.515051\n",
      "Epoch 283/300, Loss: 31.540567, F_score: 0.544292, Accuracy: 0.554260\n",
      "Epoch 284/300, Loss: 31.481810, F_score: 0.542149, Accuracy: 0.541708\n",
      "Epoch 285/300, Loss: 31.418854, F_score: 0.544622, Accuracy: 0.555315\n",
      "Epoch 286/300, Loss: 31.465540, F_score: 0.548961, Accuracy: 0.546373\n",
      "Epoch 287/300, Loss: 31.530142, F_score: 0.531995, Accuracy: 0.544096\n",
      "Epoch 288/300, Loss: 32.024384, F_score: 0.516199, Accuracy: 0.509886\n",
      "Epoch 289/300, Loss: 31.408573, F_score: 0.551247, Accuracy: 0.562146\n",
      "Epoch 290/300, Loss: 31.554529, F_score: 0.556592, Accuracy: 0.554815\n",
      "Epoch 291/300, Loss: 31.651484, F_score: 0.519446, Accuracy: 0.530601\n",
      "Epoch 292/300, Loss: 32.005569, F_score: 0.521367, Accuracy: 0.515773\n",
      "Epoch 293/300, Loss: 31.577621, F_score: 0.531164, Accuracy: 0.545707\n",
      "Epoch 294/300, Loss: 32.185635, F_score: 0.530112, Accuracy: 0.515495\n",
      "Epoch 295/300, Loss: 32.343132, F_score: 0.473476, Accuracy: 0.488615\n",
      "Epoch 296/300, Loss: 32.177917, F_score: 0.531534, Accuracy: 0.519827\n",
      "Epoch 297/300, Loss: 33.890461, F_score: 0.350964, Accuracy: 0.396812\n",
      "Epoch 298/300, Loss: 33.438549, F_score: 0.478016, Accuracy: 0.453460\n",
      "Epoch 299/300, Loss: 34.539875, F_score: 0.356550, Accuracy: 0.437299\n",
      "Epoch 300/300, Loss: 33.212673, F_score: 0.449487, Accuracy: 0.464179\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#y_true = [0, 1, 2, 0, 1, 2]\n",
    "#y_pred = [0, 2, 1, 0, 0, 1]\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1200\n",
    "\n",
    "\n",
    "CNN_LSTM_HAR_MODEL_WIP = CNN_LSTM_HAR_MODEL().cuda()\n",
    "learning_rate = 0.001\n",
    "#learning_rate = 0.02\n",
    "#loss_functioin = nn.CrossEntropyLoss(weight = (torch.tensor([0.2/0.8/17, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])/ (0.2/0.8/17 + 17)).cuda())\n",
    "#loss_functioin = nn.CrossEntropyLoss(weight = torch.tensor(class_count).float().cuda())\n",
    "#loss_functioin = nn.CrossEntropyLoss()\n",
    "\n",
    "SKODA_weight = [0.386610555,\n",
    "                1.024348618,\n",
    "                0.874884602,\n",
    "                0.909393939,\n",
    "                1.26013017,\n",
    "                2.056418456,\n",
    "                2.188381138,\n",
    "                1.18103109,\n",
    "                1.079755337,\n",
    "                0.92690209,\n",
    "                1.650109971,]\n",
    "\n",
    "loss_functioin = nn.CrossEntropyLoss(weight = torch.tensor(SKODA_weight).float().cuda())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = optim.SGD(CNN_LSTM_HAR_MODEL_WIP.parameters(), lr = learning_rate)\n",
    "#https://pytorch.org/docs/stable/optim.html\n",
    "optimizer = torch.optim.RMSprop(CNN_LSTM_HAR_MODEL_WIP.parameters(), lr = learning_rate, weight_decay = 0.9)\n",
    "CNN_LSTM_HAR_MODEL_WIP.load_state_dict(torch.load(\"model.pt\")['state_dict'])\n",
    "optimizer.load_state_dict(torch.load(\"model.pt\")['optimizer'])\n",
    "CNN_LSTM_HAR_MODEL_WIP.train()\n",
    "\n",
    "\n",
    "Training_set = 'Validation'\n",
    "Validation_x = X_train_tensor\n",
    "Validation_x = torch.reshape(Validation_x, (Validation_x.size()[0], 1, Validation_x.size()[1], Validation_x.size()[2])).float()\n",
    "\n",
    "Validation_y_2 = y_train_tensor\n",
    "\n",
    "\n",
    "#output = torch.Tensor([0, 1, 3, 2, 2, 4])\n",
    "\n",
    "#final_input = torch.Tensor(input)\n",
    "#final_output = torch.Tensor(output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "epoch_size = 300\n",
    "steps_for_printing_out_loss = 1\n",
    "\n",
    "data_size = list(Validation_y_2.size())[0]\n",
    "torch.cuda.empty_cache()\n",
    "for i in range(1, epoch_size + 1):\n",
    "    optimizer.zero_grad()\n",
    "    torch.cuda.empty_cache()\n",
    "    no_of_right = 0\n",
    "    epoch_loss = 0\n",
    "    for Batch_no in range(int(data_size / BATCH_SIZE) + 1):\n",
    "        \"\"\"\n",
    "        if Batch_no % 80 == 1:\n",
    "            print(\"Batch {}/{}\".format(str(Batch_no), int(data_size / BATCH_SIZE)))\n",
    "        \"\"\"\n",
    "        batch_start = Batch_no * BATCH_SIZE\n",
    "        batch_end = min(data_size, (Batch_no + 1) * BATCH_SIZE)\n",
    "\n",
    "        if Training_set == 'Validation':\n",
    "            input = Validation_x[batch_start: batch_end].cuda()\n",
    "            target = Validation_y_2[batch_start: batch_end].cuda()\n",
    "        elif Training_set == 'Testing':\n",
    "            input = Testing_x[batch_start: batch_end].cuda()\n",
    "            target = Testing_y_2[batch_start: batch_end].cuda()\n",
    "        elif Training_set == 'Training':\n",
    "            input = Training_x[batch_start: batch_end].cuda()\n",
    "            target = Training_y_2[batch_start: batch_end].cuda()\n",
    "        \n",
    "        output = CNN_LSTM_HAR_MODEL_WIP(input).cuda()\n",
    "        loss = loss_functioin(output, target.long()).cuda()\n",
    "        epoch_loss += loss\n",
    "        loss.backward()\n",
    "        if i == 1 or i % (steps_for_printing_out_loss) == 0:\n",
    "            #print('Loss (epoch: ' + str(i) + '): ' + str(loss.cpu().detach().numpy()))\n",
    "            \n",
    "            if Batch_no == 0:\n",
    "                All_output = output.argmax(1)\n",
    "            else:\n",
    "                All_output = torch.cat((All_output, output.argmax(1)), 0)\n",
    "            \n",
    "            \n",
    "            for k in range(len(target)):\n",
    "                if output.argmax(1)[k] == target[k]:\n",
    "                    #print('--------')\n",
    "                    #print(output[k])\n",
    "                    #print(target[k])\n",
    "                    no_of_right += 1\n",
    "    if i == 1 or i % (steps_for_printing_out_loss) == 0:\n",
    "        F_score = f1_score(y_train_tensor.numpy(), All_output.cpu().detach().numpy(), average='weighted')\n",
    "        \n",
    "        pd.DataFrame(y_train_tensor.numpy()).to_csv('export/' + 'y_target' + '.csv')\n",
    "        pd.DataFrame(All_output.cpu().detach().numpy()).to_csv('export/' + 'y_predict' + '.csv')\n",
    "        #print(CNN_LSTM_HAR_MODEL_WIP.cnn2d_3.weight.grad)\n",
    "        print(\"Epoch {}/{}, Loss: {:.6f}, F_score: {:.6f}, Accuracy: {:.6f}\".format(i, epoch_size, epoch_loss.cpu().detach().numpy(), F_score, no_of_right / data_size))\n",
    "    optimizer.step()\n",
    "torch.save({'state_dict': CNN_LSTM_HAR_MODEL_WIP.state_dict(),'optimizer': optimizer.state_dict()}, 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_csv(tensor_name):\n",
    "    x_np = tensor_name.numpy()\n",
    "    x_df = pd.DataFrame(x_np)\n",
    "    x_df.to_csv('tmp.csv')\n",
    "\n",
    "Validation_x.size()\n",
    "#output\n",
    "\n",
    "tensor_to_csv(output.detach())\n",
    "\n",
    "\n",
    "#Validation_x_to_csv = torch.reshape(Validation_x, (Validation_x.size()[0] * Validation_x.size()[1] * Validation_x.size()[2], Validation_x.size()[3])).float()\n",
    "#tensor_to_csv(Validation_x_to_csv.detach())\n",
    "#tensor_to_csv(Validation_x[1, 0,:, :])\n",
    "\n",
    "#tensor_to_csv(Validation_y_2)\n",
    "\n",
    "for row_no_end in range(0, 155):\n",
    "    print(row_no_end)\n",
    "    print(df_OPPORTUNITY_DATASET_Validation_WIP['250 ML_Both_Arms'][row_no_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input.size()\n",
    "target\n",
    "CNN_LSTM_HAR_MODEL_WIP.parameters()\n",
    "target\n",
    "Validation_y_2.sum()\n",
    "print(Validation_y_2)\n",
    "for i in Validation_y_2.long():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in CNN_LSTM_HAR_MODEL_WIP.parameters():\n",
    "    print(i.size())\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.isnan(Validation_x)\n",
    "\n",
    "\n",
    "\n",
    "x_np = Validation_x[0, 0,:, :].numpy()\n",
    "x_df = pd.DataFrame(x_np)\n",
    "x_df.to_csv('tmp.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> rnn = nn.LSTM(10, 20, 2)\n",
    ">>> input = torch.randn(5, 3, 10)\n",
    ">>> h0 = torch.randn(2, 3, 20)\n",
    ">>> c0 = torch.randn(2, 3, 20)\n",
    ">>> output, (hn, cn) = rnn(input, (h0, c0))\n",
    "\n",
    "output.size()\n",
    "#hn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216094, 30)\n",
      "[2, 3, 4, 9, 10, 11, 16, 17, 18, 23, 24, 25, 30, 31, 32, 37, 38, 39, 44, 45, 46, 51, 52, 53, 58, 59, 60, 65, 66, 67]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-d748f9fd04c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m \u001b[0mX_train_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSILDE_WINDOW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSILDE_WINDOW_STEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data_generator' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = torch.tensor([[[k* 100 + j * 10 + i for i in range (10)] for j in range(10)] for k in range(10)])\n",
    "kk\n",
    "#mm = torch.reshape(kk, (kk.size()[0], 1, kk.size()[1], kk.size()[2])).float()\n",
    "mm = torch.reshape(kk, (kk.size()[0], -1))\n",
    "\n",
    "\n",
    "print(mm.size())\n",
    "\n",
    "print(mm[0])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "mm[:, 0, :, :] == kk\n",
    "mm[5, 0, 4, 3]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for para in CNN_LSTM_HAR_MODEL_WIP.parameters():\n",
    "    print(para.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPPORTUNITY_DATASET_Training_WIP[OPPORTUNITY_column_name_selected][0:2000].to_csv('def.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.2/0.8/17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 5, 1])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 5, 1])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 5, 1])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 5, 1])\n",
      "torch.Size([64])\n",
      "torch.Size([512, 7232])\n",
      "torch.Size([512, 128])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 128])\n",
      "torch.Size([512, 128])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([18, 128])\n",
      "torch.Size([18])\n"
     ]
    }
   ],
   "source": [
    "for i in CNN_LSTM_HAR_MODEL_WIP.parameters():\n",
    "    print(i.size())\n",
    "    #print(i)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
