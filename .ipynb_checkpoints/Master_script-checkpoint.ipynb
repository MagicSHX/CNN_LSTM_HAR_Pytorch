{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "download dataset online:\n",
    "Dataset 1: OPPORTUNITY dataset\n",
    "https://archive.ics.uci.edu/ml/datasets/OPPORTUNITY +Activity+Recognition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dependency:\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import urllib\n",
    "import json\n",
    "import wget\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from random import random\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_update = 1\n",
    "if cfg_update == 1:\n",
    "    cfg = {}\n",
    "    cfg['OPPORTUNITY_DATASET'] = {}\n",
    "    cfg['OPPORTUNITY_DATASET']['Download_URL'] = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00226/OpportunityUCIDataset.zip'\n",
    "    cfg['OPPORTUNITY_DATASET']['Download_Destination'] = 'Dataset/OpportunityUCIDataset'\n",
    "    cfg['OPPORTUNITY_DATASET']['dataset_folder'] = 'Dataset/OpportunityUCIDataset/OpportunityUCIDataset/dataset'\n",
    "    cfg['OPPORTUNITY_DATASET']['Normalization_parameter'] = 'Dataset/OPPORTUNITY_norm_parameter.csv'\n",
    "    \n",
    "    cfg['SKODA_DATASET'] = {}\n",
    "    cfg['SKODA_DATASET']['Download_URL'] = 'http://har-dataset.org/lib/exe/fetch.php?media=wiki:dataset:skodaminicp:skodaminicp_2015_08.zip'\n",
    "    cfg['SKODA_DATASET']['Download_Destination'] = 'Dataset/SKODA_DATASET'\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #data = json.dumps(data, indent=4)\n",
    "    #print(data)\n",
    "    with open('cfg.json', 'w') as outfile:\n",
    "        json.dump(cfg, outfile)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_cfg_file(cfg_file_path):\n",
    "    with open(cfg_file_path) as json_file:\n",
    "        cfg = json.load(json_file)\n",
    "    return cfg\n",
    "\n",
    "def download_dataset(Source_URL, Destination):\n",
    "    if not os.path.isfile(Destination + '.zip'):\n",
    "        wget.download(Source_URL, Destination + '.zip')\n",
    "    with zipfile.ZipFile(Destination + '.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(Destination)\n",
    "    os.remove(Destination + '.zip')\n",
    "    \n",
    "    \n",
    "def OPPORTUNITY_Data_Loader(OPPORTUNITY_DATASET_FILE_LIST):\n",
    "    df_OPPORTUNITY_DATASET_All = ''\n",
    "    for file_name in OPPORTUNITY_DATASET_FILE_LIST:\n",
    "        if file_name.endswith(\".dat\"):\n",
    "            df_OPPORTUNITY_DATASET_current = pd.read_csv(OPPORTUNITY_DATASET_dataset_folder + '/' + file_name, sep = ' ', header = None)\n",
    "            df_OPPORTUNITY_DATASET_current.columns = OPPORTUNITY_DATASET_column_names\n",
    "            df_OPPORTUNITY_DATASET_current = df_OPPORTUNITY_DATASET_current.interpolate()\n",
    "            df_OPPORTUNITY_DATASET_current.replace(np.nan, 0.0, inplace=True)\n",
    "            df_OPPORTUNITY_DATASET_current['file_name'] = file_name\n",
    "            if type(df_OPPORTUNITY_DATASET_All) != pd.core.frame.DataFrame:\n",
    "                df_OPPORTUNITY_DATASET_All = df_OPPORTUNITY_DATASET_current\n",
    "            else:\n",
    "                df_OPPORTUNITY_DATASET_All = pd.concat([df_OPPORTUNITY_DATASET_All, df_OPPORTUNITY_DATASET_current])\n",
    "    return df_OPPORTUNITY_DATASET_All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cfg file:\n",
    "\n",
    "cfg_file_path = 'cfg.json'\n",
    "cfg = import_cfg_file(cfg_file_path)\n",
    "\n",
    "URL_OPPORTUNITY_DATASET = cfg['OPPORTUNITY_DATASET']['Download_URL']\n",
    "Download_Destination_OPPORTUNITY_DATASET = cfg['OPPORTUNITY_DATASET']['Download_Destination']\n",
    "\n",
    "URL_SKODA_DATASET = cfg['SKODA_DATASET']['Download_URL']\n",
    "Download_Destination_SKODA_DATASET = cfg['SKODA_DATASET']['Download_Destination']\n",
    "\n",
    "OPPORTUNITY_DATASET_dataset_folder = cfg['OPPORTUNITY_DATASET']['dataset_folder']\n",
    "\n",
    "OPPORTUNITY_DATASET_Normalization_parameter = cfg['OPPORTUNITY_DATASET']['Normalization_parameter']\n",
    "\n",
    "#download data: only if Dataset is not downloaded yet\n",
    "\"\"\"\n",
    "download_dataset(URL_OPPORTUNITY_DATASET, Download_Destination_OPPORTUNITY_DATASET)\n",
    "download_dataset(URL_SKODA_DATASET, Download_Destination_SKODA_DATASET)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPPORTUNITY_DATASET_FILE_LIST = os.listdir(OPPORTUNITY_DATASET_dataset_folder)\n",
    "OPPORTUNITY_DATASET_FILE_LIST\n",
    "\n",
    "OPPORTUNITY_DATASET_FILE_LIST_Training = []\n",
    "OPPORTUNITY_DATASET_FILE_LIST_Validation = []\n",
    "OPPORTUNITY_DATASET_FILE_LIST_Testing = []\n",
    "\n",
    "\n",
    "for file_name in OPPORTUNITY_DATASET_FILE_LIST:\n",
    "    if file_name.endswith(\".dat\"):\n",
    "        if 'S4' in file_name:\n",
    "            continue\n",
    "        if 'S1' in file_name:\n",
    "            OPPORTUNITY_DATASET_FILE_LIST_Training.append(file_name)\n",
    "        elif 'ADL3' in file_name:\n",
    "            OPPORTUNITY_DATASET_FILE_LIST_Validation.append(file_name)\n",
    "        elif ('ADL4' in file_name) or ('ADL5' in file_name):\n",
    "            OPPORTUNITY_DATASET_FILE_LIST_Testing.append(file_name)\n",
    "        else:\n",
    "            OPPORTUNITY_DATASET_FILE_LIST_Training.append(file_name)\n",
    "\n",
    "            \n",
    "OPPORTUNITY_DATASET_FILE_LIST_Training\n",
    "OPPORTUNITY_DATASET_FILE_LIST_Validation\n",
    "OPPORTUNITY_DATASET_FILE_LIST_Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load column names\n",
    "with open(OPPORTUNITY_DATASET_dataset_folder + '/column_names.txt') as txt_file:\n",
    "    OPPORTUNITY_DATASET_column_name = txt_file.read().splitlines()\n",
    "\n",
    "column_names = []\n",
    "for column_name in OPPORTUNITY_DATASET_column_name:\n",
    "    column_name = re.split('; |: ', column_name)\n",
    "    if column_name[0] == 'Column':\n",
    "        column_names.append(column_name[1])\n",
    "    #print(column_name)\n",
    "#OPPORTUNITY_DATASET_column_name\n",
    "OPPORTUNITY_DATASET_column_names = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPPORTUNITY_column_inclusive = ['InertialMeasurementUnit', 'Locomotion', 'ML_Both_Arms']\n",
    "OPPORTUNITY_column_exclusive = ['Quaternion']\n",
    "\n",
    "OPPORTUNITY_column_name_selected = OPPORTUNITY_DATASET_column_names[0: 37]\n",
    "for column_name in OPPORTUNITY_DATASET_column_names:\n",
    "    inclusive_index = 0\n",
    "    exclusive_index = 1\n",
    "    for keyword_inclusive in OPPORTUNITY_column_inclusive:\n",
    "        if column_name.find(keyword_inclusive) != -1:\n",
    "            inclusive_index = 1\n",
    "            break\n",
    "    for keyword_exclusive in OPPORTUNITY_column_exclusive:\n",
    "        if column_name.find(keyword_exclusive) != -1:\n",
    "            exclusive_index = 0\n",
    "            break\n",
    "    if inclusive_index * exclusive_index == 1:\n",
    "        OPPORTUNITY_column_name_selected.append(column_name)\n",
    "\n",
    "OPPORTUNITY_column_name_selected.append('file_name')\n",
    "OPPORTUNITY_column_name_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPPORTUNITY_DATASET_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load OPPORTUNITY_DATASET_Normalization_parameter\n",
    "if os.path.isfile(OPPORTUNITY_DATASET_Normalization_parameter):\n",
    "    df_OPPORTUNITY_DATASET_All_describe = pd.read_csv(OPPORTUNITY_DATASET_Normalization_parameter, index_col = 0)\n",
    "else:\n",
    "    df_OPPORTUNITY_DATASET_All = OPPORTUNITY_Data_Loader(OPPORTUNITY_DATASET_FILE_LIST)\n",
    "\n",
    "    #df_OPPORTUNITY_DATASET_All.describe()\n",
    "    df_OPPORTUNITY_DATASET_All_describe = df_OPPORTUNITY_DATASET_All.describe()\n",
    "    #release memory\n",
    "    df_OPPORTUNITY_DATASET_All = ''\n",
    "    df_OPPORTUNITY_DATASET_All_describe.to_csv(OPPORTUNITY_DATASET_Normalization_parameter)\n",
    "df_OPPORTUNITY_DATASET_All_describe\n",
    "\n",
    "NORM_MAX_THRESHOLDS = [3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,\n",
    "                       3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,\n",
    "                       3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,\n",
    "                       3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,\n",
    "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
    "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
    "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
    "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
    "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
    "                       250,    25,     200,    5000,   5000,   5000,   5000,   5000,   5000,\n",
    "                       10000,  10000,  10000,  10000,  10000,  10000,  250,    250,    25,\n",
    "                       200,    5000,   5000,   5000,   5000,   5000,   5000,   10000,  10000,\n",
    "                       10000,  10000,  10000,  10000,  250, ]\n",
    "\n",
    "NORM_MIN_THRESHOLDS = [-3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,\n",
    "                       -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,\n",
    "                       -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,\n",
    "                       -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,\n",
    "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
    "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
    "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
    "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
    "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
    "                       -250,   -100,   -200,   -5000,  -5000,  -5000,  -5000,  -5000,  -5000,\n",
    "                       -10000, -10000, -10000, -10000, -10000, -10000, -250,   -250,   -100,\n",
    "                       -200,   -5000,  -5000,  -5000,  -5000,  -5000,  -5000,  -10000, -10000,\n",
    "                       -10000, -10000, -10000, -10000, -250, ]\n",
    "\n",
    "\n",
    "df_OPPORTUNITY_DATASET_All_describe.loc[['min'], [col_name for col_name in OPPORTUNITY_column_name_selected[1: -3]]] = NORM_MIN_THRESHOLDS\n",
    "df_OPPORTUNITY_DATASET_All_describe.loc[['max'], [col_name for col_name in OPPORTUNITY_column_name_selected[1: -3]]] = NORM_MAX_THRESHOLDS\n",
    "\n",
    "df_OPPORTUNITY_DATASET_All_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPPORTUNITY_DATASET_FILE_LIST_Training\n",
    "OPPORTUNITY_DATASET_FILE_LIST_Validation\n",
    "OPPORTUNITY_DATASET_FILE_LIST_Testing\n",
    "\n",
    "\n",
    "df_OPPORTUNITY_DATASET_Training = OPPORTUNITY_Data_Loader(OPPORTUNITY_DATASET_FILE_LIST_Training)\n",
    "df_OPPORTUNITY_DATASET_Validation = OPPORTUNITY_Data_Loader(OPPORTUNITY_DATASET_FILE_LIST_Validation)\n",
    "df_OPPORTUNITY_DATASET_Testing = OPPORTUNITY_Data_Loader(OPPORTUNITY_DATASET_FILE_LIST_Testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = df_OPPORTUNITY_DATASET_Testing['33 Accelerometer LWR accY'].unique()\n",
    "for m in rr:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPPORTUNITY_DATASET_Training.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPPORTUNITY_DATASET_Testing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPPORTUNITY_DATASET_Validation.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPPORTUNITY_DATASET_Training['file_name'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPPORTUNITY_DATASET_All_describe['2 Accelerometer RKN^ accX']['count']\n",
    "df_OPPORTUNITY_DATASET_All_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPPORTUNITY_DATASET_Training['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#len(OPPORTUNITY_column_name_selected)\n",
    "#df_OPPORTUNITY_DATASET_WIP = df_OPPORTUNITY_DATASET[OPPORTUNITY_column_name_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPPORTUNITY_column_name_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OPPORTUNITY_Normalization(df_OPPORTUNITY_DATASET, df_OPPORTUNITY_DS_All_describe, OPPORTUNITY_col_name_selected):\n",
    "    for column_name in OPPORTUNITY_col_name_selected:\n",
    "        if column_name in ['1 MILLISEC', '244 Locomotion', '250 ML_Both_Arms', 'file_name']:\n",
    "            pass\n",
    "        else:\n",
    "            df_OPPORTUNITY_DATASET[column_name] = (df_OPPORTUNITY_DATASET[column_name] - df_OPPORTUNITY_DS_All_describe[column_name]['min']) / (df_OPPORTUNITY_DS_All_describe[column_name]['max'] - df_OPPORTUNITY_DS_All_describe[column_name]['min'])\n",
    "            df_OPPORTUNITY_DATASET[column_name].loc[df_OPPORTUNITY_DATASET[column_name] > 1] = 1\n",
    "            df_OPPORTUNITY_DATASET[column_name].loc[df_OPPORTUNITY_DATASET[column_name] < 0] = 0\n",
    "            \n",
    "    df_OPPORTUNITY_DATASET = df_OPPORTUNITY_DATASET[OPPORTUNITY_col_name_selected]\n",
    "    return df_OPPORTUNITY_DATASET\n",
    "    \n",
    "    \n",
    "df_OPPORTUNITY_DATASET_Training_WIP = OPPORTUNITY_Normalization(df_OPPORTUNITY_DATASET_Training, df_OPPORTUNITY_DATASET_All_describe, OPPORTUNITY_column_name_selected)\n",
    "df_OPPORTUNITY_DATASET_Testing_WIP = OPPORTUNITY_Normalization(df_OPPORTUNITY_DATASET_Testing, df_OPPORTUNITY_DATASET_All_describe, OPPORTUNITY_column_name_selected)\n",
    "df_OPPORTUNITY_DATASET_Validation_WIP = OPPORTUNITY_Normalization(df_OPPORTUNITY_DATASET_Validation, df_OPPORTUNITY_DATASET_All_describe, OPPORTUNITY_column_name_selected)\n",
    "\n",
    "\n",
    "Classifications = df_OPPORTUNITY_DATASET_Training_WIP['250 ML_Both_Arms'].unique()\n",
    "dict_18_classification = {}\n",
    "class_index = 0\n",
    "for Classitication in Classifications:\n",
    "    dict_18_classification[Classitication] = class_index\n",
    "    class_index += 1\n",
    "\n",
    "df_OPPORTUNITY_DATASET_Training_WIP['250 ML_Both_Arms'] = df_OPPORTUNITY_DATASET_Training_WIP['250 ML_Both_Arms'].apply(lambda  x: dict_18_classification[x])\n",
    "df_OPPORTUNITY_DATASET_Testing_WIP['250 ML_Both_Arms'] = df_OPPORTUNITY_DATASET_Testing_WIP['250 ML_Both_Arms'].apply(lambda  x: dict_18_classification[x])\n",
    "df_OPPORTUNITY_DATASET_Validation_WIP['250 ML_Both_Arms'] = df_OPPORTUNITY_DATASET_Validation_WIP['250 ML_Both_Arms'].apply(lambda  x: dict_18_classification[x])\n",
    "\n",
    "df_OPPORTUNITY_DATASET_Validation_WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPPORTUNITY_DATASET_Training_WIP['1 MILLISEC'].count()\n",
    "df_OPPORTUNITY_DATASET_Training_WIP['file_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SILDE_WINDOW = 24\n",
    "SILDE_WINDOW_STEP = 12\n",
    "\n",
    "\n",
    "def data_generator(df_OPPORTUNITY_DATASET, SILDE_WINDOW, SILDE_WINDOW_STEP, OPPORTUNITY_column_name_selected):\n",
    "    file_no = 0\n",
    "    for file_name in df_OPPORTUNITY_DATASET['file_name'].unique():\n",
    "        file_no += 1\n",
    "        print(file_no)\n",
    "        df_OPPORTUNITY_DATASET_WIP = df_OPPORTUNITY_DATASET.loc[df_OPPORTUNITY_DATASET['file_name'] == file_name]\n",
    "        df_OPPORTUNITY_DATASET_WIP = df_OPPORTUNITY_DATASET_WIP[OPPORTUNITY_column_name_selected[1: ]]\n",
    "        row_no_start = 0\n",
    "        \n",
    "        \n",
    "        while (row_no_start + SILDE_WINDOW - 1) <= df_OPPORTUNITY_DATASET_WIP['2 Accelerometer RKN^ accX'].count():\n",
    "            row_no_end = row_no_start + SILDE_WINDOW\n",
    "            \n",
    "            df_input = df_OPPORTUNITY_DATASET_WIP[OPPORTUNITY_column_name_selected[1: -3]][row_no_start: row_no_end]\n",
    "            current_input = torch.tensor([df_input.values])\n",
    "            \n",
    "            df_output_Locomotion = df_OPPORTUNITY_DATASET_WIP['244 Locomotion'][row_no_end - 1]\n",
    "            \n",
    "            current_output_Locomotion = torch.tensor([df_output_Locomotion])\n",
    "            #print(current_output_Locomotion)\n",
    "            df_output_ML_Both_Arms = df_OPPORTUNITY_DATASET_WIP['250 ML_Both_Arms'][row_no_end - 1]\n",
    "            current_output_ML_Both_Arms = torch.tensor([df_output_ML_Both_Arms])\n",
    "            \n",
    "            \n",
    "            if row_no_start == 0 and file_no == 1:\n",
    "                input_tensor = current_input\n",
    "                output_Locomotion_tensor = current_output_Locomotion\n",
    "                output_ML_Both_Arms_tensor = current_output_ML_Both_Arms\n",
    "            else:\n",
    "                input_tensor = torch.cat((input_tensor, current_input), 0)\n",
    "                output_Locomotion_tensor = torch.cat((output_Locomotion_tensor, current_output_Locomotion), 0)\n",
    "                output_ML_Both_Arms_tensor = torch.cat((output_ML_Both_Arms_tensor, current_output_ML_Both_Arms), 0)\n",
    "                \n",
    "            row_no_start += SILDE_WINDOW_STEP\n",
    "            #print(input_tensor.size())\n",
    "            #print(input_tensor)\n",
    "    return input_tensor, output_Locomotion_tensor, output_ML_Both_Arms_tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Training_x, Training_y_1, Training_y_2 = data_generator(df_OPPORTUNITY_DATASET_Training_WIP, SILDE_WINDOW, SILDE_WINDOW_STEP, OPPORTUNITY_column_name_selected)\n",
    "Training_x = torch.reshape(Training_x, (Training_x.size()[0], 1, Training_x.size()[1], Training_x.size()[2])).float()\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Testing_x, Testing_y_1, Testing_y_2 = data_generator(df_OPPORTUNITY_DATASET_Testing_WIP, SILDE_WINDOW, SILDE_WINDOW_STEP, OPPORTUNITY_column_name_selected)\n",
    "Testing_x = torch.reshape(Testing_x, (Testing_x.size()[0], 1, Testing_x.size()[1], Testing_x.size()[2])).float()\n",
    "\n",
    "Validation_x, Validation_y_1, Validation_y_2 = data_generator(df_OPPORTUNITY_DATASET_Validation_WIP, SILDE_WINDOW, SILDE_WINDOW_STEP, OPPORTUNITY_column_name_selected)\n",
    "Validation_x = torch.reshape(Validation_x, (Validation_x.size()[0], 1, Validation_x.size()[1], Validation_x.size()[2])).float()\n",
    "\"\"\"\n",
    "#def batch_generator():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPPORTUNITY_DATASET_Validation_WIP[OPPORTUNITY_column_name_selected[1: -3]][1: 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_OPPORTUNITY_DATASET_WIP['22 Accelerometer RKN_ accZ'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_csv(tensor_name, file_name):\n",
    "    x_np = tensor_name.cpu().detach().numpy()\n",
    "    x_df = pd.DataFrame(x_np)\n",
    "    x_df.to_csv('export/' + file_name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import lasagne\n",
    "#import theano\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import _pickle as cp\n",
    "#import theano.tensor as T\n",
    "\n",
    "def load_dataset(filename):\n",
    "\n",
    "    f = open(filename, 'rb')\n",
    "    data = cp.load(f)\n",
    "    f.close()\n",
    "\n",
    "    X_train, y_train = data[0]\n",
    "    X_test, y_test = data[1]\n",
    "\n",
    "    print(\" ..from file {}\".format(filename))\n",
    "    print(\" ..reading instances: train {0}, test {1}\".format(X_train.shape, X_test.shape))\n",
    "\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "\n",
    "    # The targets are casted to int8 for GPU compatibility.\n",
    "    y_train = y_train.astype(np.uint8)\n",
    "    y_test = y_test.astype(np.uint8)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "filename = 'C:/Users/HX/Magic/oppChallenge_gestures.data'\n",
    "X_train, y_train, X_test, y_test = load_dataset(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SILDE_WINDOW = 24\n",
    "SILDE_WINDOW_STEP = 12\n",
    "\n",
    "\n",
    "def data_generator(ARRAY_OPPORTUNITY_DATASET_x, ARRAY_OPPORTUNITY_DATASET_y, SILDE_WINDOW, SILDE_WINDOW_STEP):\n",
    "    row_no_start = 0\n",
    "    count_index = 0\n",
    "    while (row_no_start + SILDE_WINDOW - 1) <= len(ARRAY_OPPORTUNITY_DATASET_x):\n",
    "        count_index += 1\n",
    "        if count_index % 500 == 0:\n",
    "            print(count_index)\n",
    "        row_no_end = row_no_start + SILDE_WINDOW\n",
    "        df_input = ARRAY_OPPORTUNITY_DATASET_x[row_no_start: row_no_end]\n",
    "        current_input = torch.tensor([df_input])\n",
    "        df_output_Locomotion = ARRAY_OPPORTUNITY_DATASET_y[row_no_end - 1]\n",
    "        df_output_ML_Both_Arms = torch.tensor([df_output_Locomotion])\n",
    "        current_output_ML_Both_Arms = torch.tensor([df_output_ML_Both_Arms])\n",
    "\n",
    "\n",
    "        if row_no_start == 0:\n",
    "            input_tensor = current_input\n",
    "            output_ML_Both_Arms_tensor = current_output_ML_Both_Arms\n",
    "        else:\n",
    "            input_tensor = torch.cat((input_tensor, current_input), 0)\n",
    "            output_ML_Both_Arms_tensor = torch.cat((output_ML_Both_Arms_tensor, current_output_ML_Both_Arms), 0)\n",
    "        row_no_start += SILDE_WINDOW_STEP\n",
    "    return input_tensor, output_ML_Both_Arms_tensor\n",
    "\n",
    "\n",
    "#X_train_tensor_1, y_train_tensor_1 = data_generator(X_train[0: 40000], y_train[0: 40000], SILDE_WINDOW, SILDE_WINDOW_STEP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SILDE_WINDOW = 24\n",
    "SILDE_WINDOW_STEP = 12\n",
    "\n",
    "def data_generator(ARRAY_OPPORTUNITY_DATASET_x, ARRAY_OPPORTUNITY_DATASET_y, SILDE_WINDOW, SILDE_WINDOW_STEP):\n",
    "    array_len = len(ARRAY_OPPORTUNITY_DATASET_x)\n",
    "    shape_0 = int(array_len / SILDE_WINDOW)\n",
    "    shape_1 = SILDE_WINDOW\n",
    "    shape_2 = len(ARRAY_OPPORTUNITY_DATASET_x[0])\n",
    "    input_tensor = torch.reshape(torch.tensor(ARRAY_OPPORTUNITY_DATASET_x[0: shape_0 * SILDE_WINDOW]), (shape_0, shape_1, shape_2))\n",
    "    \n",
    "    output_tensor = torch.reshape(torch.tensor(ARRAY_OPPORTUNITY_DATASET_y[0: shape_0 * SILDE_WINDOW]), (shape_0, shape_1))\n",
    "    \n",
    "    output_tensor = output_tensor[:,-1]\n",
    "\n",
    "    ARRAY_OPPORTUNITY_DATASET_x = ARRAY_OPPORTUNITY_DATASET_x[SILDE_WINDOW_STEP: ]\n",
    "    array_len = len(ARRAY_OPPORTUNITY_DATASET_x)\n",
    "    shape_0 = int(array_len / SILDE_WINDOW)\n",
    "    shape_1 = SILDE_WINDOW\n",
    "    shape_2 = len(ARRAY_OPPORTUNITY_DATASET_x[0])\n",
    "    input_tensor = torch.cat((input_tensor, torch.reshape(torch.tensor(ARRAY_OPPORTUNITY_DATASET_x[0: shape_0 * SILDE_WINDOW]), (shape_0, shape_1, shape_2))), 0)\n",
    "    \n",
    "    ARRAY_OPPORTUNITY_DATASET_y = ARRAY_OPPORTUNITY_DATASET_y[SILDE_WINDOW_STEP: ]\n",
    "    \n",
    "    output_tensor_2 = torch.reshape(torch.tensor(ARRAY_OPPORTUNITY_DATASET_y[0: shape_0 * SILDE_WINDOW]), (shape_0, shape_1))\n",
    "    output_tensor = torch.cat((output_tensor, output_tensor_2[:,-1]), 0)\n",
    "    \n",
    "    return input_tensor, output_tensor\n",
    "    \n",
    "    \n",
    "X_train_tensor, y_train_tensor = data_generator(X_train[0:200000], y_train[0:200000], SILDE_WINDOW, SILDE_WINDOW_STEP)\n",
    "\n",
    "\n",
    "#print(X_train_tensor[0])\n",
    "#input_tensor.size()\n",
    "#output_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#torch.save({'X_train_tensor': X_train_tensor, 'y_train_tensor': y_train_tensor}, 'C:/Users/HX/Magic/training_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data_loading = torch.load(\"D:/Installation/yolov4.pt\")\n",
    "X_train_tensor = data_loading['X_train_tensor']\n",
    "y_train_tensor = data_loading['y_train_tensor']\n",
    "data_loading = ''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_tensor.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pending items:\n",
    "    - batch size = 100\n",
    "    - sensor data were pre-processed to fill in missing values using linear interpolation\n",
    "    - model accuracy & loss function save into array\n",
    "    - Dashboard - to show performance\n",
    "    \n",
    "    - per channel normalization to interval [0,1]\n",
    "    https://stackoverflow.com/questions/26414913/normalize-columns-of-pandas-data-frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input = np.array([random() for i in range(24 * 113 * 1 * 6)]).reshape(6, 1, 24, 113)\n",
    "\n",
    "output = torch.Tensor([[1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                     [0.0, 1.0, 0.0, 0.0, 0.0],\n",
    "                     [0.0, 0.0, 0.0, 1.0, 0.0],\n",
    "                     [0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "                     [0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "                     [0.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "\n",
    "\n",
    "output = torch.Tensor([0, 1, 3, 2, 2, 4])\n",
    "\n",
    "final_input = torch.Tensor(input)\n",
    "final_output = torch.Tensor(output)\n",
    "\"\"\"\n",
    "\n",
    "class CNN_LSTM_HAR_MODEL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn2d_1 = nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size = (5, 1))\n",
    "        self.conv_bn = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.cnn2d_2 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (5, 1))\n",
    "        self.cnn2d_3 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (5, 1))\n",
    "        self.cnn2d_4 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (5, 1))\n",
    "        \n",
    "        self.batch_size = 24 - 4 * (5 - 1)\n",
    "        self.hidden_dim = 128\n",
    "        self.input_size = 7232\n",
    "        self.n_layers = 2\n",
    "        self.lstm1 = nn.LSTM(input_size  = 7232, hidden_size = self.hidden_dim, num_layers = self.n_layers, dropout = 0.5)\n",
    "        self.conv_bn2 = nn.BatchNorm1d(8)\n",
    "        #\n",
    "        self.input_size = 128\n",
    "        #self.n_layers = 1\n",
    "        self.lstm2 = nn.LSTM(input_size  = self.input_size, hidden_size = self.hidden_dim, num_layers = self.n_layers, dropout = 0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128, 18)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        x = self.cnn2d_1(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.conv_bn(x)\n",
    "        #tensor_to_csv(x[0, -1, :, :], 'layer_1_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :, :], 'layer_1_data_100')\n",
    "        \n",
    "        x = self.cnn2d_2(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.conv_bn(x)\n",
    "        #tensor_to_csv(x[0, -1, :, :], 'layer_2_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :, :], 'layer_2_data_100')\n",
    "\n",
    "        x = self.cnn2d_3(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.conv_bn(x)\n",
    "        #tensor_to_csv(x[0, -1, :, :], 'layer_3_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :, :], 'layer_3_data_100')\n",
    "\n",
    "        x = self.cnn2d_4(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.conv_bn(x)\n",
    "        #tensor_to_csv(x[0, -1, :, :], 'layer_4_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :, :], 'layer_4_data_100')\n",
    "\n",
    "        \n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = torch.reshape(x, (x.size()[0], x.size()[1], -1))\n",
    "\n",
    "        #hidden = self.init_hidden()\n",
    "        hidden = (torch.randn(self.n_layers, 8, self.hidden_dim).cuda().requires_grad_(), torch.randn(self.n_layers, 8, self.hidden_dim).cuda().requires_grad_())\n",
    "        x, hidden = self.lstm1(x, hidden)\n",
    "        #x = self.conv_bn2(x)\n",
    "        #hidden = self.conv_bn2(hidden)\n",
    "        #tensor_to_csv(x[0, -1, :,], 'layer_lstm1_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :], 'layer_lstm1_data_100')\n",
    "        #print(x)\n",
    "        #hidden2 = (torch.zeros(self.n_layers, 8, self.hidden_dim).requires_grad_(), torch.zeros(self.n_layers, 8, self.hidden_dim).requires_grad_())\n",
    "        ###############################\n",
    "        #x, hidden = self.lstm2(x, hidden)\n",
    "        #print(x)\n",
    "        #x = self.conv_bn2(x)\n",
    "        \n",
    "        tensor_to_csv(x[0, -1, :,], 'layer_lstm2_data_0')\n",
    "        tensor_to_csv(x[15, -1, :], 'layer_lstm2_data_100')\n",
    "        \n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc1(x)\n",
    "        tensor_to_csv(x, 'layer_fc1_data')\n",
    "        \n",
    "        x = F.softmax(x, dim = 1)\n",
    "        tensor_to_csv(x, 'output_data')\n",
    "        \n",
    "        \n",
    "        #print(x)\n",
    "        return x\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        hidden = (weight.new(self.n_layers, self.batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, self.batch_size, self.hidden_dim).zero_())\n",
    "        return hidden\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(Validation_y_2.size())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 200\n",
    "\n",
    "\n",
    "CNN_LSTM_HAR_MODEL_WIP = CNN_LSTM_HAR_MODEL().cuda()\n",
    "learning_rate = 0.001\n",
    "#learning_rate = 0.02\n",
    "#loss_functioin = nn.CrossEntropyLoss(weight = (torch.tensor([0.2/0.8/17, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])/ (0.2/0.8/17 + 17)).cuda())\n",
    "#loss_functioin = nn.CrossEntropyLoss(weight = ((torch.tensor([0.2/0.8/17, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])/(0.2/0.8/17)).cuda()))\n",
    "loss_functioin = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer = optim.SGD(CNN_LSTM_HAR_MODEL_WIP.parameters(), lr = learning_rate)\n",
    "#https://pytorch.org/docs/stable/optim.html\n",
    "optimizer = torch.optim.RMSprop(CNN_LSTM_HAR_MODEL_WIP.parameters(), lr = learning_rate, weight_decay = 0.9)\n",
    "\n",
    "\n",
    "\n",
    "Training_set = 'Validation'\n",
    "Validation_x = X_train_tensor\n",
    "Validation_x = torch.reshape(Validation_x, (Validation_x.size()[0], 1, Validation_x.size()[1], Validation_x.size()[2])).float()\n",
    "\n",
    "Validation_y_2 = y_train_tensor\n",
    "\n",
    "\n",
    "epoch_size = 30\n",
    "steps_for_printing_out_loss = 1\n",
    "\n",
    "data_size = list(Validation_y_2.size())[0]\n",
    "torch.cuda.empty_cache()\n",
    "for i in range(1, epoch_size + 1):\n",
    "    optimizer.zero_grad()\n",
    "    torch.cuda.empty_cache()\n",
    "    no_of_right = 0\n",
    "    epoch_loss = 0\n",
    "    for Batch_no in range(int(data_size / BATCH_SIZE) + 1):\n",
    "        if Batch_no % 20 == 0:\n",
    "            print(\"Batch {}/{}\".format(str(Batch_no), int(data_size / BATCH_SIZE)))\n",
    "        batch_start = Batch_no * BATCH_SIZE\n",
    "        batch_end = min(data_size, (Batch_no + 1) * BATCH_SIZE)\n",
    "\n",
    "        if Training_set == 'Validation':\n",
    "            input = Validation_x[batch_start: batch_end].cuda()\n",
    "            target = Validation_y_2[batch_start: batch_end].cuda()\n",
    "        elif Training_set == 'Testing':\n",
    "            input = Testing_x[batch_start: batch_end].cuda()\n",
    "            target = Testing_y_2[batch_start: batch_end].cuda()\n",
    "        elif Training_set == 'Training':\n",
    "            input = Training_x[batch_start: batch_end].cuda()\n",
    "            target = Training_y_2[batch_start: batch_end].cuda()\n",
    "        \n",
    "        output = CNN_LSTM_HAR_MODEL_WIP(input).cuda()\n",
    "        loss = loss_functioin(output, target.long()).cuda()\n",
    "        epoch_loss += loss\n",
    "        loss.backward()\n",
    "        if i == 1 or i % (steps_for_printing_out_loss) == 0:\n",
    "            #print('Loss (epoch: ' + str(i) + '): ' + str(loss.cpu().detach().numpy()))\n",
    "            \n",
    "            for k in range(len(target)):\n",
    "                if output.argmax(1)[k] == target[k]:\n",
    "                    #print('--------')\n",
    "                    #print(output[k])\n",
    "                    #print(target[k])\n",
    "                    no_of_right += 1\n",
    "    if i == 1 or i % (steps_for_printing_out_loss) == 0:\n",
    "        print(\"Epoch {}/{}, Loss: {:.6f}, Accuracy: {:.6f}\".format(i, epoch_size, epoch_loss.cpu().detach().numpy(), no_of_right / data_size))\n",
    "    optimizer.step()\n",
    "torch.save({'state_dict': CNN_LSTM_HAR_MODEL_WIP.state_dict(),'optimizer': optimizer.state_dict()}, 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_csv(tensor_name):\n",
    "    x_np = tensor_name.numpy()\n",
    "    x_df = pd.DataFrame(x_np)\n",
    "    x_df.to_csv('tmp.csv')\n",
    "\n",
    "Validation_x.size()\n",
    "#output\n",
    "\n",
    "tensor_to_csv(output.detach())\n",
    "\n",
    "\n",
    "#Validation_x_to_csv = torch.reshape(Validation_x, (Validation_x.size()[0] * Validation_x.size()[1] * Validation_x.size()[2], Validation_x.size()[3])).float()\n",
    "#tensor_to_csv(Validation_x_to_csv.detach())\n",
    "#tensor_to_csv(Validation_x[1, 0,:, :])\n",
    "\n",
    "#tensor_to_csv(Validation_y_2)\n",
    "\n",
    "for row_no_end in range(0, 155):\n",
    "    print(row_no_end)\n",
    "    print(df_OPPORTUNITY_DATASET_Validation_WIP['250 ML_Both_Arms'][row_no_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input.size()\n",
    "target\n",
    "CNN_LSTM_HAR_MODEL_WIP.parameters()\n",
    "target\n",
    "Validation_y_2.sum()\n",
    "print(Validation_y_2)\n",
    "for i in Validation_y_2.long():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in CNN_LSTM_HAR_MODEL_WIP.parameters():\n",
    "    print(i.size())\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.isnan(Validation_x)\n",
    "\n",
    "\n",
    "\n",
    "x_np = Validation_x[0, 0,:, :].numpy()\n",
    "x_df = pd.DataFrame(x_np)\n",
    "x_df.to_csv('tmp.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> rnn = nn.LSTM(10, 20, 2)\n",
    ">>> input = torch.randn(5, 3, 10)\n",
    ">>> h0 = torch.randn(2, 3, 20)\n",
    ">>> c0 = torch.randn(2, 3, 20)\n",
    ">>> output, (hn, cn) = rnn(input, (h0, c0))\n",
    "\n",
    "output.size()\n",
    "#hn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "mat = scipy.io.loadmat('Dataset/SKODA_DATASET/SkodaMiniCP_2015_08/left_classall_clean.mat')\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = torch.tensor([[[k* 100 + j * 10 + i for i in range (10)] for j in range(10)] for k in range(10)])\n",
    "kk\n",
    "#mm = torch.reshape(kk, (kk.size()[0], 1, kk.size()[1], kk.size()[2])).float()\n",
    "mm = torch.reshape(kk, (kk.size()[0], -1))\n",
    "\n",
    "\n",
    "print(mm.size())\n",
    "\n",
    "print(mm[0])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "mm[:, 0, :, :] == kk\n",
    "mm[5, 0, 4, 3]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for para in CNN_LSTM_HAR_MODEL_WIP.parameters():\n",
    "    print(para.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPPORTUNITY_DATASET_Training_WIP[OPPORTUNITY_column_name_selected][0:2000].to_csv('def.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.2/0.8/17"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
