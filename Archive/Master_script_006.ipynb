{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency:\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Library\n",
    "\n",
    "import json\n",
    "import wget\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from random import random\n",
    "import zipfile\n",
    "from sklearn.metrics import f1_score\n",
    "import scipy.io\n",
    "import time\n",
    "import _pickle as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some thought:\n",
    "    - OPPORTUNITY Dataset Norm\n",
    "    - OPPORTUNITY slide window training data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available. Have fun!\n"
     ]
    }
   ],
   "source": [
    "#MASTER CONTROL PANEL\n",
    "\n",
    "#Create/ update cfg file - Select \"True\" if there is a need to update cfg Json file\n",
    "cfg_update_is_required = True\n",
    "#cfg_update_is_required = False\n",
    "\n",
    "#Download Dataset - Select \"True\" if there is a need to download Dataset into Project folder\n",
    "Dataset_download_is_required = True\n",
    "#Dataset_download_is_required = False\n",
    "\n",
    "#Select scope of the script to run\n",
    "#Select \"True\" if OPPORTUNITY DATASET related Script is required\n",
    "OPPORTUNITY_DATASET_related_Script_is_required = True\n",
    "OPPORTUNITY_DATASET_related_Script_is_required = False\n",
    "\n",
    "#Select \"True\" if SKODA DATASET related Script is required\n",
    "SKODA_DATASET_related_Script_is_required = True\n",
    "#SKODA_DATASET_related_Script_is_required = False\n",
    "\n",
    "#Auto select device(cpu/ gpu) to run\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available. Have fun!\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, CPU will be used...\")\n",
    "\n",
    "#Select Training DATASET\n",
    "#Training_DATASET = 'OPPORTUNITY - 5 classes'\n",
    "#Training_DATASET = 'OPPORTUNITY - 18 classes'\n",
    "Training_DATASET = 'SKODA'\n",
    "\n",
    "\n",
    "#Select Data Preprocessing approach for OPPORTUNITY DATASET\n",
    "#OPPORTUNITY_DATASET_Proprocessing_approach = \"Hongxu\"\n",
    "OPPORTUNITY_DATASET_Proprocessing_approach = \"Paper\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create/ update cfg file - Run this if there is a need to update cfg Json file\n",
    "\n",
    "if cfg_update_is_required == True:\n",
    "    cfg = {}\n",
    "    cfg['OPPORTUNITY_DATASET'] = {}\n",
    "    cfg['OPPORTUNITY_DATASET']['Download_URL'] = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00226/OpportunityUCIDataset.zip'\n",
    "    cfg['OPPORTUNITY_DATASET']['Download_Destination'] = 'Dataset/OpportunityUCIDataset'\n",
    "    cfg['OPPORTUNITY_DATASET']['dataset_folder'] = 'Dataset/OpportunityUCIDataset/OpportunityUCIDataset/dataset'\n",
    "    cfg['OPPORTUNITY_DATASET']['Normalization_parameter'] = 'Dataset/OPPORTUNITY_norm_parameter.csv'\n",
    "    \n",
    "    cfg['SKODA_DATASET'] = {}\n",
    "    cfg['SKODA_DATASET']['Download_URL'] = 'http://har-dataset.org/lib/exe/fetch.php?media=wiki:dataset:skodaminicp:skodaminicp_2015_08.zip'\n",
    "    cfg['SKODA_DATASET']['Download_Destination'] = 'Dataset/SKODA_DATASET'\n",
    "    cfg['SKODA_DATASET']['dataset_path'] = 'Dataset/SKODA_DATASET/SkodaMiniCP_2015_08/right_classall_clean.mat'\n",
    "    \n",
    "    with open('cfg.json', 'w') as outfile:\n",
    "        json.dump(cfg, outfile)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "SILDE_WINDOW = 24\n",
    "SILDE_WINDOW_STEP = 12\n",
    "\n",
    "#to be automated\n",
    "SKODA_Classification_weight = [0.386610555,\n",
    "                                1.024348618,\n",
    "                                0.874884602,\n",
    "                                0.909393939,\n",
    "                                1.26013017,\n",
    "                                2.056418456,\n",
    "                                2.188381138,\n",
    "                                1.18103109,\n",
    "                                1.079755337,\n",
    "                                0.92690209,\n",
    "                                1.650109971,]\n",
    "\n",
    "\n",
    "NORM_MAX_THRESHOLDS = [3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,\n",
    "                       3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,\n",
    "                       3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,\n",
    "                       3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,\n",
    "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
    "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
    "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
    "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
    "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
    "                       250,    25,     200,    5000,   5000,   5000,   5000,   5000,   5000,\n",
    "                       10000,  10000,  10000,  10000,  10000,  10000,  250,    250,    25,\n",
    "                       200,    5000,   5000,   5000,   5000,   5000,   5000,   10000,  10000,\n",
    "                       10000,  10000,  10000,  10000,  250, ]\n",
    "\n",
    "NORM_MIN_THRESHOLDS = [-3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,\n",
    "                       -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,\n",
    "                       -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,\n",
    "                       -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,\n",
    "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
    "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
    "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
    "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
    "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
    "                       -250,   -100,   -200,   -5000,  -5000,  -5000,  -5000,  -5000,  -5000,\n",
    "                       -10000, -10000, -10000, -10000, -10000, -10000, -250,   -250,   -100,\n",
    "                       -200,   -5000,  -5000,  -5000,  -5000,  -5000,  -5000,  -10000, -10000,\n",
    "                       -10000, -10000, -10000, -10000, -250, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load configuration parameter from Json file into a dict\n",
    "def import_cfg_file(cfg_file_path):\n",
    "    with open(cfg_file_path) as json_file:\n",
    "        cfg = json.load(json_file)\n",
    "    return cfg\n",
    "\n",
    "#Download dataset from given URL into local folder and unzip\n",
    "def download_dataset(Source_URL, Destination):\n",
    "    if not os.path.isfile(Destination + '.zip'):\n",
    "        wget.download(Source_URL, Destination + '.zip')\n",
    "    with zipfile.ZipFile(Destination + '.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(Destination)\n",
    "    #os.remove(Destination + '.zip')\n",
    "    print(' --- Dataset has been successfully downloaded into: ' + Destination)\n",
    "    return 0\n",
    "\n",
    "#Load OPPORTUNITY multi data files (DAT format), based on given file list\n",
    "#convert loaded data into pandas df, assign column names to df\n",
    "#replace NaN into 0.0\n",
    "def OPPORTUNITY_Data_Loader(OPPORTUNITY_DATASET_FILE_LIST, OPPORTUNITY_DATASET_column_names):\n",
    "    df_OPPORTUNITY_DATASET_All = ''\n",
    "    for file_name in OPPORTUNITY_DATASET_FILE_LIST:\n",
    "        if file_name.endswith(\".dat\"):\n",
    "            df_OPPORTUNITY_DATASET_current = pd.read_csv(OPPORTUNITY_DATASET_dataset_folder + '/' + file_name, sep = ' ', header = None)\n",
    "            df_OPPORTUNITY_DATASET_current.columns = OPPORTUNITY_DATASET_column_names\n",
    "            df_OPPORTUNITY_DATASET_current = df_OPPORTUNITY_DATASET_current.interpolate()\n",
    "            df_OPPORTUNITY_DATASET_current.replace(np.nan, 0.0, inplace=True)\n",
    "            df_OPPORTUNITY_DATASET_current['file_name'] = file_name\n",
    "            if type(df_OPPORTUNITY_DATASET_All) != pd.core.frame.DataFrame:\n",
    "                df_OPPORTUNITY_DATASET_All = df_OPPORTUNITY_DATASET_current\n",
    "            else:\n",
    "                df_OPPORTUNITY_DATASET_All = pd.concat([df_OPPORTUNITY_DATASET_All, df_OPPORTUNITY_DATASET_current])\n",
    "    return df_OPPORTUNITY_DATASET_All\n",
    "\n",
    "#Using raw data to generate training data based on slide window apprach\n",
    "def data_generator(ARRAY_OPPORTUNITY_DATASET_x, ARRAY_OPPORTUNITY_DATASET_y, SILDE_WINDOW, SILDE_WINDOW_STEP):\n",
    "    ARRAY_OPPORTUNITY_DATASET_x = torch.tensor(ARRAY_OPPORTUNITY_DATASET_x).float()\n",
    "    ARRAY_OPPORTUNITY_DATASET_y = torch.tensor(ARRAY_OPPORTUNITY_DATASET_y)\n",
    "    #print(type(ARRAY_OPPORTUNITY_DATASET_x))\n",
    "    array_len = len(ARRAY_OPPORTUNITY_DATASET_x)\n",
    "    shape_0 = int(array_len / SILDE_WINDOW)\n",
    "    shape_1 = SILDE_WINDOW\n",
    "    shape_2 = len(ARRAY_OPPORTUNITY_DATASET_x[0])\n",
    "    input_tensor = torch.reshape(ARRAY_OPPORTUNITY_DATASET_x[0: shape_0 * SILDE_WINDOW], (shape_0, shape_1, shape_2))\n",
    "    #print(type(input_tensor))\n",
    "    output_tensor = torch.reshape(ARRAY_OPPORTUNITY_DATASET_y[0: shape_0 * SILDE_WINDOW], (shape_0, shape_1))[:,-1]\n",
    "\n",
    "    \n",
    "\n",
    "    ARRAY_OPPORTUNITY_DATASET_x = ARRAY_OPPORTUNITY_DATASET_x[SILDE_WINDOW_STEP: ]\n",
    "    array_len = len(ARRAY_OPPORTUNITY_DATASET_x)\n",
    "    shape_0 = int(array_len / SILDE_WINDOW)\n",
    "    shape_1 = SILDE_WINDOW\n",
    "    shape_2 = len(ARRAY_OPPORTUNITY_DATASET_x[0])\n",
    "    input_tensor_part_2 = torch.reshape(ARRAY_OPPORTUNITY_DATASET_x[0: shape_0 * SILDE_WINDOW], (shape_0, shape_1, shape_2))\n",
    "    input_tensor = torch.cat((input_tensor, input_tensor_part_2), 0)\n",
    "    \n",
    "    ARRAY_OPPORTUNITY_DATASET_y = ARRAY_OPPORTUNITY_DATASET_y[SILDE_WINDOW_STEP: ]\n",
    "    output_tensor_part_2 = torch.reshape(ARRAY_OPPORTUNITY_DATASET_y[0: shape_0 * SILDE_WINDOW], (shape_0, shape_1))[:,-1]\n",
    "    output_tensor = torch.cat((output_tensor, output_tensor_part_2), 0)\n",
    "    \n",
    "    return input_tensor, output_tensor\n",
    "\n",
    "#Export Tensor into csv file\n",
    "def tensor_to_csv(tensor_name, file_name):\n",
    "    x_np = tensor_name.cpu().detach().numpy()\n",
    "    x_df = pd.DataFrame(x_np)\n",
    "    x_df.to_csv('export/' + file_name + '.csv')\n",
    "    \n",
    "    \n",
    "#Channel wise Normalization on OPPORTUNITY DATASET\n",
    "def OPPORTUNITY_Normalization(df_OPPORTUNITY_DATASET, df_OPPORTUNITY_DS_All_describe, OPPORTUNITY_col_name_selected):\n",
    "    for column_name in OPPORTUNITY_col_name_selected:\n",
    "        if column_name in ['1 MILLISEC', '244 Locomotion', '250 ML_Both_Arms', 'file_name']:\n",
    "            pass\n",
    "        else:\n",
    "            df_OPPORTUNITY_DATASET[column_name] = (df_OPPORTUNITY_DATASET[column_name] - df_OPPORTUNITY_DS_All_describe[column_name]['min']) / (df_OPPORTUNITY_DS_All_describe[column_name]['max'] - df_OPPORTUNITY_DS_All_describe[column_name]['min'])\n",
    "            df_OPPORTUNITY_DATASET[column_name].loc[df_OPPORTUNITY_DATASET[column_name] > 1] = 1\n",
    "            df_OPPORTUNITY_DATASET[column_name].loc[df_OPPORTUNITY_DATASET[column_name] < 0] = 0\n",
    "            \n",
    "    df_OPPORTUNITY_DATASET = df_OPPORTUNITY_DATASET[OPPORTUNITY_col_name_selected]\n",
    "    return df_OPPORTUNITY_DATASET\n",
    "\n",
    "\n",
    "def LOAD_OPPORTUNITY_DATASET_from_dot_DATA_file(filename):\n",
    "    f = open(filename, 'rb')\n",
    "    data = cp.load(f)\n",
    "    f.close()\n",
    "\n",
    "    X_train, y_train = data[0]\n",
    "    X_test, y_test = data[1]\n",
    "    print(\"training data: {0}, testing data: {1}\".format(X_train.shape, X_test.shape))\n",
    "\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "    y_train = y_train.astype(np.uint8)\n",
    "    y_test = y_test.astype(np.uint8)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df data generator - OPPORTUNITY Dataset\n",
    "#########################Pending###########################\n",
    "\n",
    "def df_data_generator(df_OPPORTUNITY_DATASET, SILDE_WINDOW, SILDE_WINDOW_STEP, OPPORTUNITY_column_name_selected):\n",
    "    file_no = 0\n",
    "    for file_name in df_OPPORTUNITY_DATASET['file_name'].unique():\n",
    "        file_no += 1\n",
    "        print(file_no)\n",
    "        df_OPPORTUNITY_DATASET_WIP = df_OPPORTUNITY_DATASET.loc[df_OPPORTUNITY_DATASET['file_name'] == file_name]\n",
    "        df_OPPORTUNITY_DATASET_WIP = df_OPPORTUNITY_DATASET_WIP[OPPORTUNITY_column_name_selected[1: ]]\n",
    "        row_no_start = 0\n",
    "        \n",
    "        \n",
    "        while (row_no_start + SILDE_WINDOW - 1) <= df_OPPORTUNITY_DATASET_WIP['2 Accelerometer RKN^ accX'].count():\n",
    "            row_no_end = row_no_start + SILDE_WINDOW\n",
    "            \n",
    "            df_input = df_OPPORTUNITY_DATASET_WIP[OPPORTUNITY_column_name_selected[1: -3]][row_no_start: row_no_end]\n",
    "            current_input = torch.tensor([df_input.values])\n",
    "            \n",
    "            df_output_Locomotion = df_OPPORTUNITY_DATASET_WIP['244 Locomotion'][row_no_end - 1]\n",
    "            \n",
    "            current_output_Locomotion = torch.tensor([df_output_Locomotion])\n",
    "            #print(current_output_Locomotion)\n",
    "            df_output_ML_Both_Arms = df_OPPORTUNITY_DATASET_WIP['250 ML_Both_Arms'][row_no_end - 1]\n",
    "            current_output_ML_Both_Arms = torch.tensor([df_output_ML_Both_Arms])\n",
    "            \n",
    "            \n",
    "            if row_no_start == 0 and file_no == 1:\n",
    "                input_tensor = current_input\n",
    "                output_Locomotion_tensor = current_output_Locomotion\n",
    "                output_ML_Both_Arms_tensor = current_output_ML_Both_Arms\n",
    "            else:\n",
    "                input_tensor = torch.cat((input_tensor, current_input), 0)\n",
    "                output_Locomotion_tensor = torch.cat((output_Locomotion_tensor, current_output_Locomotion), 0)\n",
    "                output_ML_Both_Arms_tensor = torch.cat((output_ML_Both_Arms_tensor, current_output_ML_Both_Arms), 0)\n",
    "                \n",
    "            row_no_start += SILDE_WINDOW_STEP\n",
    "            #print(input_tensor.size())\n",
    "            #print(input_tensor)\n",
    "    return input_tensor, output_Locomotion_tensor, output_ML_Both_Arms_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cfg file:\n",
    "\n",
    "cfg_file_path = 'cfg.json'\n",
    "cfg = import_cfg_file(cfg_file_path)\n",
    "\n",
    "#OPPORTUNITY Dataset related configuration\n",
    "URL_OPPORTUNITY_DATASET = cfg['OPPORTUNITY_DATASET']['Download_URL']\n",
    "Download_Destination_OPPORTUNITY_DATASET = cfg['OPPORTUNITY_DATASET']['Download_Destination']\n",
    "OPPORTUNITY_DATASET_dataset_folder = cfg['OPPORTUNITY_DATASET']['dataset_folder']\n",
    "OPPORTUNITY_DATASET_Normalization_parameter = cfg['OPPORTUNITY_DATASET']['Normalization_parameter']\n",
    "\n",
    "#SKODA Dataset related configuration\n",
    "URL_SKODA_DATASET = cfg['SKODA_DATASET']['Download_URL']\n",
    "Download_Destination_SKODA_DATASET = cfg['SKODA_DATASET']['Download_Destination']\n",
    "SKODA_DATASET_dataset_path = cfg['SKODA_DATASET']['dataset_path']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Dataset has been successfully downloaded into: Dataset/SKODA_DATASET\n"
     ]
    }
   ],
   "source": [
    "#Run this if there is a need to download Dataset into Project folder\n",
    "\n",
    "if Dataset_download_is_required == True:\n",
    "    if OPPORTUNITY_DATASET_related_Script_is_required == True:\n",
    "        download_dataset(URL_OPPORTUNITY_DATASET, Download_Destination_OPPORTUNITY_DATASET)\n",
    "    if SKODA_DATASET_related_Script_is_required == True:\n",
    "        download_dataset(URL_SKODA_DATASET, Download_Destination_SKODA_DATASET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Opportunity Dataset column names\n",
    "\n",
    "\n",
    "if OPPORTUNITY_DATASET_Proprocessing_approach == \"Hongxu\":\n",
    "    if OPPORTUNITY_DATASET_related_Script_is_required == True:\n",
    "        with open(OPPORTUNITY_DATASET_dataset_folder + '/column_names.txt') as txt_file:\n",
    "            OPPORTUNITY_DATASET_column_name = txt_file.read().splitlines()\n",
    "\n",
    "        column_names = []\n",
    "        for column_name in OPPORTUNITY_DATASET_column_name:\n",
    "            column_name = re.split('; |: ', column_name)\n",
    "            if column_name[0] == 'Column':\n",
    "                column_names.append(column_name[1])\n",
    "            #print(column_name)\n",
    "        OPPORTUNITY_DATASET_column_names = column_names\n",
    "\n",
    "        #filter out column names selected for training\n",
    "\n",
    "        OPPORTUNITY_column_inclusive = ['InertialMeasurementUnit', 'Locomotion', 'ML_Both_Arms']\n",
    "        OPPORTUNITY_column_exclusive = ['Quaternion']\n",
    "\n",
    "        OPPORTUNITY_column_name_selected = OPPORTUNITY_DATASET_column_names[0: 37]\n",
    "        for column_name in OPPORTUNITY_DATASET_column_names:\n",
    "            inclusive_index = 0\n",
    "            exclusive_index = 1\n",
    "            for keyword_inclusive in OPPORTUNITY_column_inclusive:\n",
    "                if column_name.find(keyword_inclusive) != -1:\n",
    "                    inclusive_index = 1\n",
    "                    break\n",
    "            for keyword_exclusive in OPPORTUNITY_column_exclusive:\n",
    "                if column_name.find(keyword_exclusive) != -1:\n",
    "                    exclusive_index = 0\n",
    "                    break\n",
    "            if inclusive_index * exclusive_index == 1:\n",
    "                OPPORTUNITY_column_name_selected.append(column_name)\n",
    "\n",
    "        OPPORTUNITY_column_name_selected.append('file_name')\n",
    "\n",
    "        OPPORTUNITY_column_name_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Opportunity Dataset dat file into 3 Groups based on paper: Training, Validation, Testing\n",
    "\n",
    "if OPPORTUNITY_DATASET_Proprocessing_approach == \"Hongxu\":\n",
    "    if OPPORTUNITY_DATASET_related_Script_is_required == True:\n",
    "        OPPORTUNITY_DATASET_FILE_LIST = os.listdir(OPPORTUNITY_DATASET_dataset_folder)\n",
    "        OPPORTUNITY_DATASET_FILE_LIST\n",
    "\n",
    "        OPPORTUNITY_DATASET_FILE_LIST_Training = []\n",
    "        OPPORTUNITY_DATASET_FILE_LIST_Validation = []\n",
    "        OPPORTUNITY_DATASET_FILE_LIST_Testing = []\n",
    "\n",
    "        for file_name in OPPORTUNITY_DATASET_FILE_LIST:\n",
    "            if file_name.endswith(\".dat\"):\n",
    "                if 'S4' in file_name:\n",
    "                    continue\n",
    "                if 'S1' in file_name:\n",
    "                    OPPORTUNITY_DATASET_FILE_LIST_Training.append(file_name)\n",
    "                elif 'ADL3' in file_name:\n",
    "                    OPPORTUNITY_DATASET_FILE_LIST_Validation.append(file_name)\n",
    "                elif ('ADL4' in file_name) or ('ADL5' in file_name):\n",
    "                    OPPORTUNITY_DATASET_FILE_LIST_Testing.append(file_name)\n",
    "                else:\n",
    "                    OPPORTUNITY_DATASET_FILE_LIST_Training.append(file_name)\n",
    "\n",
    "        OPPORTUNITY_DATASET_FILE_LIST_Training\n",
    "        OPPORTUNITY_DATASET_FILE_LIST_Validation\n",
    "        OPPORTUNITY_DATASET_FILE_LIST_Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPPORTUNITY_DATASET Normalization\n",
    "Using_Paper_OPPORTUNITY_Threshold = True\n",
    "#Using_Paper_OPPORTUNITY_Threshold = False\n",
    "\n",
    "if OPPORTUNITY_DATASET_Proprocessing_approach == \"Hongxu\":\n",
    "    if OPPORTUNITY_DATASET_related_Script_is_required == True:\n",
    "\n",
    "        #load OPPORTUNITY_DATASET_Normalization_parameter\n",
    "        if os.path.isfile(OPPORTUNITY_DATASET_Normalization_parameter):\n",
    "            df_OPPORTUNITY_DATASET_All_describe = pd.read_csv(OPPORTUNITY_DATASET_Normalization_parameter, index_col = 0)\n",
    "        else:\n",
    "            df_OPPORTUNITY_DATASET_All = OPPORTUNITY_Data_Loader(OPPORTUNITY_DATASET_FILE_LIST, OPPORTUNITY_DATASET_column_names)\n",
    "\n",
    "            #df_OPPORTUNITY_DATASET_All.describe()\n",
    "            df_OPPORTUNITY_DATASET_All_describe = df_OPPORTUNITY_DATASET_All.describe()\n",
    "            #release memory\n",
    "            df_OPPORTUNITY_DATASET_All = ''\n",
    "            df_OPPORTUNITY_DATASET_All_describe.to_csv(OPPORTUNITY_DATASET_Normalization_parameter)\n",
    "\n",
    "        if Using_Paper_OPPORTUNITY_Threshold == True:\n",
    "            df_OPPORTUNITY_DATASET_All_describe.loc[['min'], [col_name for col_name in OPPORTUNITY_column_name_selected[1: -3]]] = NORM_MIN_THRESHOLDS\n",
    "            df_OPPORTUNITY_DATASET_All_describe.loc[['max'], [col_name for col_name in OPPORTUNITY_column_name_selected[1: -3]]] = NORM_MAX_THRESHOLDS\n",
    "\n",
    "\n",
    "        df_OPPORTUNITY_DATASET_Training = OPPORTUNITY_Data_Loader(OPPORTUNITY_DATASET_FILE_LIST_Training, OPPORTUNITY_DATASET_column_names)\n",
    "        df_OPPORTUNITY_DATASET_Validation = OPPORTUNITY_Data_Loader(OPPORTUNITY_DATASET_FILE_LIST_Validation, OPPORTUNITY_DATASET_column_names)\n",
    "        df_OPPORTUNITY_DATASET_Testing = OPPORTUNITY_Data_Loader(OPPORTUNITY_DATASET_FILE_LIST_Testing, OPPORTUNITY_DATASET_column_names)\n",
    "\n",
    "        df_OPPORTUNITY_DATASET_Training_WIP = OPPORTUNITY_Normalization(df_OPPORTUNITY_DATASET_Training, df_OPPORTUNITY_DATASET_All_describe, OPPORTUNITY_column_name_selected)\n",
    "        df_OPPORTUNITY_DATASET_Validation_WIP = OPPORTUNITY_Normalization(df_OPPORTUNITY_DATASET_Validation, df_OPPORTUNITY_DATASET_All_describe, OPPORTUNITY_column_name_selected)\n",
    "        df_OPPORTUNITY_DATASET_Testing_WIP = OPPORTUNITY_Normalization(df_OPPORTUNITY_DATASET_Testing, df_OPPORTUNITY_DATASET_All_describe, OPPORTUNITY_column_name_selected)\n",
    "\n",
    "        Classifications = df_OPPORTUNITY_DATASET_Training_WIP['250 ML_Both_Arms'].unique()\n",
    "        dict_18_classification = {}\n",
    "        class_index = 0\n",
    "        for Classitication in Classifications:\n",
    "            dict_18_classification[Classitication] = class_index\n",
    "            class_index += 1\n",
    "\n",
    "        df_OPPORTUNITY_DATASET_Training_WIP['250 ML_Both_Arms'] = df_OPPORTUNITY_DATASET_Training_WIP['250 ML_Both_Arms'].apply(lambda  x: dict_18_classification[x])\n",
    "        df_OPPORTUNITY_DATASET_Testing_WIP['250 ML_Both_Arms'] = df_OPPORTUNITY_DATASET_Testing_WIP['250 ML_Both_Arms'].apply(lambda  x: dict_18_classification[x])\n",
    "        df_OPPORTUNITY_DATASET_Validation_WIP['250 ML_Both_Arms'] = df_OPPORTUNITY_DATASET_Validation_WIP['250 ML_Both_Arms'].apply(lambda  x: dict_18_classification[x])\n",
    "\n",
    "        \"\"\"\n",
    "        Training_x, Training_y_1, Training_y_2 = df_data_generator(df_OPPORTUNITY_DATASET_Training_WIP, SILDE_WINDOW, SILDE_WINDOW_STEP, OPPORTUNITY_column_name_selected)\n",
    "        Training_x = torch.reshape(Training_x, (Training_x.size()[0], 1, Training_x.size()[1], Training_x.size()[2])).float()\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Testing_x, Testing_y_1, Testing_y_2 = df_data_generator(df_OPPORTUNITY_DATASET_Testing_WIP, SILDE_WINDOW, SILDE_WINDOW_STEP, OPPORTUNITY_column_name_selected)\n",
    "        Testing_x = torch.reshape(Testing_x, (Testing_x.size()[0], 1, Testing_x.size()[1], Testing_x.size()[2])).float()\n",
    "\n",
    "        Validation_x, Validation_y_1, Validation_y_2 = df_data_generator(df_OPPORTUNITY_DATASET_Validation_WIP, SILDE_WINDOW, SILDE_WINDOW_STEP, OPPORTUNITY_column_name_selected)\n",
    "        Validation_x = torch.reshape(Validation_x, (Validation_x.size()[0], 1, Validation_x.size()[1], Validation_x.size()[2])).float()\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert OPPORTUNITY Dateset into .data file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data pre-processing - OPPORTUNITY Dataset - Paper approach\n",
    "\n",
    "if OPPORTUNITY_DATASET_Proprocessing_approach == \"Paper\":\n",
    "    if OPPORTUNITY_DATASET_related_Script_is_required == True:\n",
    "        OPPORTUNITY_dot_DATA_file_path = 'Dataset/oppChallenge_gestures.data'\n",
    "        if not os.path.isfile(OPPORTUNITY_dot_DATA_file_path):\n",
    "            print('Data pre processing in progress...')\n",
    "            !python preprocess_data.py -i Dataset/OpportunityUCIDataset.zip -o oppChallenge_gestures.data\n",
    "            print('Data pre processing completed!')\n",
    "\n",
    "        OPPORTUNITY_X_train, OPPORTUNITY_y_train, OPPORTUNITY_X_test, OPPORTUNITY_y_test = LOAD_OPPORTUNITY_DATASET_from_dot_DATA_file(OPPORTUNITY_dot_DATA_file_path)\n",
    "\n",
    "        df_OPPORTUNITY_y_target = pd.DataFrame(OPPORTUNITY_y_train)\n",
    "        df_OPPORTUNITY_y_target.columns = ['classification']\n",
    "        OPPORTUNITY_class_count = df_OPPORTUNITY_y_target['classification'].value_counts()\n",
    "        OPPORTUNITY_class_count = OPPORTUNITY_class_count.sort_index(0).tolist()\n",
    "        OPPORTUNITY_class_weight = 1 / (np.array(OPPORTUNITY_class_count) / (sum(OPPORTUNITY_class_count) / len(OPPORTUNITY_class_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SKODA DATASET - preprocessing\n",
    "\n",
    "if SKODA_DATASET_related_Script_is_required == True:\n",
    "    right_classall_clean_mat = scipy.io.loadmat(SKODA_DATASET_dataset_path)\n",
    "    right_classall_clean_array = right_classall_clean_mat['right_classall_clean']\n",
    "\n",
    "    SKODA_DATASET_dataset_size = right_classall_clean_array.shape[0]\n",
    "    SKODA_Selected_row = [int(k * (98 / 30)) for k in range(0, int(SKODA_DATASET_dataset_size / (98 / 30)) + 1)]\n",
    "    #print(SKODA_Selected_row)\n",
    "\n",
    "    no_of_sensor = 10\n",
    "    SKODA_Selected_column_x = [2 + s * 7 for s in range(no_of_sensor)] + [3 + s * 7 for s in range(no_of_sensor)] + [4 + s * 7 for s in range(no_of_sensor)]\n",
    "    SKODA_Selected_column_x.sort()\n",
    "    SKODA_Selected_column_y = 0\n",
    "\n",
    "    right_classall_clean_array = right_classall_clean_array[SKODA_Selected_row,:].astype(np.float)\n",
    "    right_classall_clean_array_x = right_classall_clean_array[:, SKODA_Selected_column_x]\n",
    "    right_classall_clean_array_y = right_classall_clean_array[:, SKODA_Selected_column_y]\n",
    "\n",
    "    #print(right_classall_clean_array_x.shape)\n",
    "    #print(SKODA_Selected_column_x)\n",
    "    right_classall_clean_array\n",
    "    SKODA_Selected_column_norm = []\n",
    "    for column in range(len(right_classall_clean_array_x[0])):\n",
    "        max_value = max(right_classall_clean_array_x[:, column])\n",
    "        min_value = min(right_classall_clean_array_x[:, column])\n",
    "        SKODA_Selected_column_norm.append([max_value, min_value, (max_value - min_value)])\n",
    "        #Norm\n",
    "        #right_classall_clean_array_x[:, column] = (right_classall_clean_array_x[:, column] - min_value) / (max_value - min_value)\n",
    "\n",
    "    #len(SKODA_Selected_column_norm)\n",
    "    #right_classall_clean_array_x.shape\n",
    "\n",
    "    SKODA_original_class = np.unique(right_classall_clean_array_y)\n",
    "    dict_SKODA_class = {}\n",
    "    for i in range(len(SKODA_original_class)):\n",
    "        dict_SKODA_class[SKODA_original_class[i]] = i\n",
    "    for i in range(len(right_classall_clean_array_y)):\n",
    "        right_classall_clean_array_y[i] = dict_SKODA_class[right_classall_clean_array_y[i]]\n",
    "\n",
    "    SKODA_X_train_tensor, SKODA_y_train_tensor = data_generator(right_classall_clean_array_x, right_classall_clean_array_y, SILDE_WINDOW, SILDE_WINDOW_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN_LSTM_HAR_MODEL(nn.Module):\n",
    "    def __init__(self, SILDE_WINDOW, no_of_class, no_of_sensor_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.no_of_class = no_of_class\n",
    "        \n",
    "        self.conv_channel = 64\n",
    "        self.kernel_size = (5, 1)\n",
    "        self.hidden_dim = 128\n",
    "        self.no_of_conv_layer = 4\n",
    "        self.lstm_dropout = 0.5\n",
    "        self.final_sequence_length = SILDE_WINDOW - self.no_of_conv_layer * (self.kernel_size[0] - 1)\n",
    "        self.lstm1_input_size = self.conv_channel * no_of_sensor_channel\n",
    "        \n",
    "        self.input_bn = nn.BatchNorm2d(no_of_sensor_channel, affine = False)\n",
    "        \n",
    "        self.cnn2d_1 = nn.Conv2d(in_channels = 1, out_channels = self.conv_channel, kernel_size = self.kernel_size)\n",
    "        self.conv1_bn = nn.BatchNorm2d(self.conv_channel)\n",
    "        \n",
    "        self.cnn2d_2 = nn.Conv2d(in_channels = self.conv_channel, out_channels = self.conv_channel, kernel_size = self.kernel_size)\n",
    "        self.conv2_bn = nn.BatchNorm2d(self.conv_channel)\n",
    "        \n",
    "        self.cnn2d_3 = nn.Conv2d(in_channels = self.conv_channel, out_channels = self.conv_channel, kernel_size = self.kernel_size)\n",
    "        self.conv3_bn = nn.BatchNorm2d(self.conv_channel)\n",
    "        \n",
    "        self.cnn2d_4 = nn.Conv2d(in_channels = self.conv_channel, out_channels = self.conv_channel, kernel_size = self.kernel_size)\n",
    "        self.conv4_bn = nn.BatchNorm2d(self.conv_channel)\n",
    "        \n",
    "        #self.conv_bn = nn.BatchNorm1d(self.final_sequence_length)\n",
    "        self.conv_bn = nn.BatchNorm2d(no_of_sensor_channel, affine = False)\n",
    "        #self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(batch_first = True, num_layers = 2, input_size  = self.lstm1_input_size, hidden_size = self.hidden_dim)\n",
    "        self.lstm_bn = nn.BatchNorm1d(self.final_sequence_length, affine = False)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.lstm1 = nn.LSTM(batch_first = True, input_size  = self.lstm1_input_size, hidden_size = self.hidden_dim)\n",
    "        self.lstm1_bn = nn.BatchNorm1d(self.final_sequence_length)\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(batch_first = True, input_size  = self.hidden_dim, hidden_size = self.hidden_dim)\n",
    "        self.lstm2_bn = nn.BatchNorm1d(self.final_sequence_length)\n",
    "        \"\"\"\n",
    "        self.fc1 = nn.Linear(128, self.no_of_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        \n",
    "        x = torch.transpose(x, 1, 3)\n",
    "        x = self.input_bn(x)\n",
    "        x = torch.transpose(x, 1, 3)\n",
    "        \n",
    "        x = self.cnn2d_1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.conv1_bn(x)\n",
    "        #tensor_to_csv(x[0, -1, :, :], 'layer_1_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :, :], 'layer_1_data_100')\n",
    "        \n",
    "        x = self.cnn2d_2(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.conv2_bn(x)\n",
    "        #tensor_to_csv(x[0, -1, :, :], 'layer_2_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :, :], 'layer_2_data_100')\n",
    "\n",
    "        x = self.cnn2d_3(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.conv3_bn(x)\n",
    "        #tensor_to_csv(x[0, -1, :, :], 'layer_3_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :, :], 'layer_3_data_100')\n",
    "\n",
    "        x = self.cnn2d_4(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.conv4_bn(x)\n",
    "        #tensor_to_csv(x[0, -1, :, :], 'layer_4_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :, :], 'layer_4_data_100')\n",
    "        \n",
    "        x = torch.transpose(x, 1, 3)\n",
    "        x = self.conv_bn(x)\n",
    "        x = torch.transpose(x, 1, 3)\n",
    "        \n",
    "        #print(x.size())\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        #print(x.size())\n",
    "        x = torch.reshape(x, (x.size()[0], x.size()[1], -1))\n",
    "        \n",
    "        #hidden = self.init_hidden()\n",
    "        #hidden = (torch.randn(2, x.size()[0], self.hidden_dim).cuda(), torch.randn(2, x.size()[0], self.hidden_dim).cuda())\n",
    "        #x, hidden = self.lstm1(x, hidden)\n",
    "        \n",
    "        x, hidden = self.lstm(x)\n",
    "        x = self.lstm_bn(x)\n",
    "        \n",
    "        \"\"\"\n",
    "        x, hidden = self.lstm1(x)\n",
    "        x = self.lstm1_bn(x)\n",
    "        #hidden = self.lstm1_bn(hidden)\n",
    "        x, hidden = self.lstm2(x)\n",
    "        x = self.lstm2_bn(x)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        #print(x)\n",
    "        #x = self.conv_bn2(x)\n",
    "        #hidden = self.conv_bn2(hidden)\n",
    "        #tensor_to_csv(x[0, -1, :,], 'layer_lstm1_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :], 'layer_lstm1_data_100')\n",
    "        #print(x)\n",
    "        #hidden2 = (torch.zeros(self.n_layers, 8, self.hidden_dim).requires_grad_(), torch.zeros(self.n_layers, 8, self.hidden_dim).requires_grad_())\n",
    "        ###############################\n",
    "        #x, hidden = self.lstm2(x, hidden)\n",
    "        #print(x)\n",
    "        #x = self.conv_bn2(x)\n",
    "        #tensor_to_csv(x[0, -1, :,], 'layer_lstm2_data_0')\n",
    "        #tensor_to_csv(x[15, -1, :], 'layer_lstm2_data_100')\n",
    "        \n",
    "        #print(x.size())\n",
    "        x = torch.reshape(x, (-1, self.hidden_dim))\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.softmax(x, dim = 1)\n",
    "        #x = F.softmax(x)\n",
    "        #int(x.size()[0] / self.final_sequence_length)\n",
    "        x = torch.reshape(x, (-1, self.final_sequence_length, self.no_of_class))\n",
    "        #x = x[:, -1, :]\n",
    "        #print(x.size())\n",
    "        #tensor_to_csv(x, 'layer_fc1_data')\n",
    "        #tensor_to_csv(x, 'output_data')\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 29.727339, F_score: 0.680895, Accuracy: 0.700100\n",
      "Epoch 2/200, Loss: 29.161751, F_score: 0.707073, Accuracy: 0.723259\n",
      "Epoch 3/200, Loss: 28.988562, F_score: 0.715793, Accuracy: 0.729257\n",
      "Epoch 4/200, Loss: 28.854145, F_score: 0.721653, Accuracy: 0.733033\n",
      "Epoch 5/200, Loss: 28.749649, F_score: 0.725758, Accuracy: 0.735477\n",
      "Epoch 6/200, Loss: 28.663624, F_score: 0.729128, Accuracy: 0.737976\n",
      "Epoch 7/200, Loss: 28.582685, F_score: 0.733323, Accuracy: 0.741197\n",
      "Epoch 8/200, Loss: 28.500010, F_score: 0.738505, Accuracy: 0.745696\n",
      "Epoch 9/200, Loss: 28.416290, F_score: 0.743922, Accuracy: 0.750361\n",
      "Epoch 10/200, Loss: 28.334105, F_score: 0.749446, Accuracy: 0.755415\n",
      "Epoch 11/200, Loss: 28.254230, F_score: 0.753581, Accuracy: 0.759136\n",
      "Epoch 12/200, Loss: 28.174931, F_score: 0.757665, Accuracy: 0.762968\n",
      "Epoch 13/200, Loss: 28.095879, F_score: 0.761525, Accuracy: 0.766633\n",
      "Epoch 14/200, Loss: 28.015625, F_score: 0.765903, Accuracy: 0.770799\n",
      "Epoch 15/200, Loss: 27.904806, F_score: 0.767833, Accuracy: 0.772187\n",
      "Epoch 16/200, Loss: 27.803154, F_score: 0.766208, Accuracy: 0.770021\n",
      "Epoch 17/200, Loss: 27.801069, F_score: 0.770462, Accuracy: 0.774631\n",
      "Epoch 18/200, Loss: 27.969009, F_score: 0.746988, Accuracy: 0.755193\n",
      "Epoch 19/200, Loss: 28.319944, F_score: 0.756144, Accuracy: 0.763745\n",
      "Epoch 20/200, Loss: 27.730516, F_score: 0.768981, Accuracy: 0.773242\n",
      "Epoch 21/200, Loss: 27.668028, F_score: 0.773572, Accuracy: 0.778018\n",
      "Epoch 22/200, Loss: 27.629250, F_score: 0.774995, Accuracy: 0.779851\n",
      "Epoch 23/200, Loss: 27.599594, F_score: 0.777386, Accuracy: 0.782184\n",
      "Epoch 24/200, Loss: 27.577919, F_score: 0.778517, Accuracy: 0.783628\n",
      "Epoch 25/200, Loss: 27.551683, F_score: 0.781291, Accuracy: 0.786182\n",
      "Epoch 26/200, Loss: 27.525785, F_score: 0.781247, Accuracy: 0.786627\n",
      "Epoch 27/200, Loss: 27.507496, F_score: 0.784971, Accuracy: 0.789737\n",
      "Epoch 28/200, Loss: 27.491562, F_score: 0.781446, Accuracy: 0.787460\n",
      "Epoch 29/200, Loss: 27.511923, F_score: 0.789694, Accuracy: 0.793180\n",
      "Epoch 30/200, Loss: 27.622950, F_score: 0.765526, Accuracy: 0.774686\n",
      "Epoch 31/200, Loss: 27.847910, F_score: 0.782323, Accuracy: 0.784849\n",
      "Epoch 32/200, Loss: 27.590754, F_score: 0.768339, Accuracy: 0.775464\n",
      "Epoch 33/200, Loss: 27.452881, F_score: 0.788371, Accuracy: 0.792736\n",
      "Epoch 34/200, Loss: 27.415125, F_score: 0.785944, Accuracy: 0.791792\n",
      "Epoch 35/200, Loss: 27.397484, F_score: 0.789694, Accuracy: 0.794735\n",
      "Epoch 36/200, Loss: 27.381025, F_score: 0.789575, Accuracy: 0.795179\n",
      "Epoch 37/200, Loss: 27.370296, F_score: 0.792367, Accuracy: 0.797179\n",
      "Epoch 38/200, Loss: 27.360003, F_score: 0.790605, Accuracy: 0.796623\n",
      "Epoch 39/200, Loss: 27.364672, F_score: 0.795519, Accuracy: 0.799289\n",
      "Epoch 40/200, Loss: 27.379375, F_score: 0.788729, Accuracy: 0.795513\n",
      "Epoch 41/200, Loss: 27.477230, F_score: 0.794118, Accuracy: 0.796012\n",
      "Epoch 42/200, Loss: 27.485319, F_score: 0.783844, Accuracy: 0.791347\n",
      "Epoch 43/200, Loss: 27.623348, F_score: 0.784243, Accuracy: 0.785627\n",
      "Epoch 44/200, Loss: 27.354357, F_score: 0.796621, Accuracy: 0.801344\n",
      "Epoch 45/200, Loss: 27.316805, F_score: 0.793330, Accuracy: 0.799234\n",
      "Epoch 46/200, Loss: 27.307476, F_score: 0.801664, Accuracy: 0.805898\n",
      "Epoch 47/200, Loss: 27.316538, F_score: 0.788581, Accuracy: 0.796179\n",
      "Epoch 48/200, Loss: 27.368406, F_score: 0.807701, Accuracy: 0.810008\n",
      "Epoch 49/200, Loss: 27.564108, F_score: 0.760054, Accuracy: 0.773464\n",
      "Epoch 50/200, Loss: 27.623032, F_score: 0.801810, Accuracy: 0.804621\n",
      "Epoch 51/200, Loss: 27.391094, F_score: 0.781563, Accuracy: 0.788459\n",
      "Epoch 52/200, Loss: 27.298031, F_score: 0.800011, Accuracy: 0.804121\n",
      "Epoch 53/200, Loss: 27.277788, F_score: 0.798483, Accuracy: 0.803954\n",
      "Epoch 54/200, Loss: 27.270714, F_score: 0.803274, Accuracy: 0.807286\n",
      "Epoch 55/200, Loss: 27.271717, F_score: 0.797851, Accuracy: 0.803899\n",
      "Epoch 56/200, Loss: 27.297611, F_score: 0.804598, Accuracy: 0.807175\n",
      "Epoch 57/200, Loss: 27.313026, F_score: 0.793827, Accuracy: 0.801011\n",
      "Epoch 58/200, Loss: 27.394886, F_score: 0.804947, Accuracy: 0.806398\n",
      "Epoch 59/200, Loss: 27.341230, F_score: 0.792590, Accuracy: 0.799956\n",
      "Epoch 60/200, Loss: 27.369341, F_score: 0.806326, Accuracy: 0.808064\n",
      "Epoch 61/200, Loss: 27.284327, F_score: 0.795968, Accuracy: 0.802566\n",
      "Epoch 62/200, Loss: 27.277174, F_score: 0.808808, Accuracy: 0.811618\n",
      "Epoch 63/200, Loss: 27.252676, F_score: 0.798638, Accuracy: 0.804843\n",
      "Epoch 64/200, Loss: 27.287848, F_score: 0.807338, Accuracy: 0.809897\n",
      "Epoch 65/200, Loss: 27.294697, F_score: 0.800524, Accuracy: 0.806287\n",
      "Epoch 66/200, Loss: 27.397459, F_score: 0.799387, Accuracy: 0.803066\n",
      "Epoch 67/200, Loss: 27.329199, F_score: 0.807805, Accuracy: 0.811729\n",
      "Epoch 68/200, Loss: 27.278963, F_score: 0.801788, Accuracy: 0.806120\n",
      "Epoch 69/200, Loss: 27.228855, F_score: 0.809170, Accuracy: 0.813729\n",
      "Epoch 70/200, Loss: 27.243834, F_score: 0.808130, Accuracy: 0.811230\n",
      "Epoch 71/200, Loss: 27.234903, F_score: 0.805941, Accuracy: 0.811563\n",
      "Epoch 72/200, Loss: 27.292238, F_score: 0.810530, Accuracy: 0.812618\n",
      "Epoch 73/200, Loss: 27.271441, F_score: 0.801951, Accuracy: 0.808453\n",
      "Epoch 74/200, Loss: 27.364431, F_score: 0.810981, Accuracy: 0.812285\n",
      "Epoch 75/200, Loss: 27.312355, F_score: 0.797694, Accuracy: 0.804676\n",
      "Epoch 76/200, Loss: 27.356627, F_score: 0.817402, Accuracy: 0.818560\n",
      "Epoch 77/200, Loss: 27.365383, F_score: 0.780655, Accuracy: 0.791681\n",
      "Epoch 78/200, Loss: 27.381199, F_score: 0.821497, Accuracy: 0.822948\n",
      "Epoch 79/200, Loss: 27.438635, F_score: 0.779891, Accuracy: 0.789237\n",
      "Epoch 80/200, Loss: 27.334499, F_score: 0.817805, Accuracy: 0.819893\n",
      "Epoch 81/200, Loss: 27.281052, F_score: 0.795138, Accuracy: 0.802955\n",
      "Epoch 82/200, Loss: 27.240856, F_score: 0.816720, Accuracy: 0.818449\n",
      "Epoch 83/200, Loss: 27.270893, F_score: 0.798483, Accuracy: 0.806676\n",
      "Epoch 84/200, Loss: 27.338495, F_score: 0.814402, Accuracy: 0.815728\n",
      "Epoch 85/200, Loss: 27.306698, F_score: 0.795837, Accuracy: 0.804565\n",
      "Epoch 86/200, Loss: 27.324148, F_score: 0.817230, Accuracy: 0.817838\n",
      "Epoch 87/200, Loss: 27.227203, F_score: 0.802275, Accuracy: 0.810008\n",
      "Epoch 88/200, Loss: 27.238026, F_score: 0.818902, Accuracy: 0.820171\n",
      "Epoch 89/200, Loss: 27.253696, F_score: 0.800949, Accuracy: 0.808730\n",
      "Epoch 90/200, Loss: 27.330105, F_score: 0.814629, Accuracy: 0.816172\n",
      "Epoch 91/200, Loss: 27.333323, F_score: 0.800838, Accuracy: 0.808453\n",
      "Epoch 92/200, Loss: 27.326775, F_score: 0.815437, Accuracy: 0.816395\n",
      "Epoch 93/200, Loss: 27.233614, F_score: 0.804390, Accuracy: 0.811563\n",
      "Epoch 94/200, Loss: 27.238285, F_score: 0.820942, Accuracy: 0.821559\n",
      "Epoch 95/200, Loss: 27.231316, F_score: 0.802139, Accuracy: 0.810397\n",
      "Epoch 96/200, Loss: 27.305578, F_score: 0.824270, Accuracy: 0.823781\n",
      "Epoch 97/200, Loss: 27.286434, F_score: 0.795121, Accuracy: 0.805176\n",
      "Epoch 98/200, Loss: 27.430481, F_score: 0.824896, Accuracy: 0.823670\n",
      "Epoch 99/200, Loss: 27.427656, F_score: 0.783165, Accuracy: 0.795568\n",
      "Epoch 100/200, Loss: 27.537529, F_score: 0.820498, Accuracy: 0.818838\n",
      "Epoch 101/200, Loss: 27.384399, F_score: 0.770392, Accuracy: 0.787182\n",
      "Epoch 102/200, Loss: 27.335533, F_score: 0.821337, Accuracy: 0.821448\n",
      "Epoch 103/200, Loss: 27.311375, F_score: 0.800066, Accuracy: 0.808342\n",
      "Epoch 104/200, Loss: 27.385921, F_score: 0.814284, Accuracy: 0.815506\n",
      "Epoch 105/200, Loss: 27.290928, F_score: 0.815720, Accuracy: 0.819671\n",
      "Epoch 106/200, Loss: 27.442417, F_score: 0.793303, Accuracy: 0.799622\n",
      "Epoch 107/200, Loss: 27.435678, F_score: 0.823100, Accuracy: 0.825003\n",
      "Epoch 108/200, Loss: 27.555387, F_score: 0.784204, Accuracy: 0.790625\n",
      "Epoch 109/200, Loss: 27.344959, F_score: 0.825150, Accuracy: 0.827391\n",
      "Epoch 110/200, Loss: 27.321096, F_score: 0.799317, Accuracy: 0.805620\n",
      "Epoch 111/200, Loss: 27.243559, F_score: 0.827658, Accuracy: 0.829168\n",
      "Epoch 112/200, Loss: 27.218674, F_score: 0.809884, Accuracy: 0.815673\n",
      "Epoch 113/200, Loss: 27.209509, F_score: 0.828894, Accuracy: 0.830501\n",
      "Epoch 114/200, Loss: 27.241413, F_score: 0.807480, Accuracy: 0.814895\n",
      "Epoch 115/200, Loss: 27.332043, F_score: 0.830166, Accuracy: 0.829834\n",
      "Epoch 116/200, Loss: 27.392653, F_score: 0.788693, Accuracy: 0.800622\n",
      "Epoch 117/200, Loss: 27.477962, F_score: 0.829381, Accuracy: 0.827669\n",
      "Epoch 118/200, Loss: 27.421427, F_score: 0.771097, Accuracy: 0.788626\n",
      "Epoch 119/200, Loss: 27.564787, F_score: 0.820395, Accuracy: 0.819838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200, Loss: 27.485426, F_score: 0.776122, Accuracy: 0.790237\n",
      "Epoch 121/200, Loss: 27.496202, F_score: 0.820454, Accuracy: 0.819171\n",
      "Epoch 122/200, Loss: 27.430706, F_score: 0.801058, Accuracy: 0.808008\n",
      "Epoch 123/200, Loss: 27.389139, F_score: 0.821354, Accuracy: 0.821559\n",
      "Epoch 124/200, Loss: 27.317156, F_score: 0.815444, Accuracy: 0.819782\n",
      "Epoch 125/200, Loss: 27.322865, F_score: 0.819285, Accuracy: 0.820449\n",
      "Epoch 126/200, Loss: 27.318174, F_score: 0.815886, Accuracy: 0.820560\n",
      "Epoch 127/200, Loss: 27.428770, F_score: 0.815100, Accuracy: 0.816617\n",
      "Epoch 128/200, Loss: 27.395298, F_score: 0.811435, Accuracy: 0.816783\n",
      "Epoch 129/200, Loss: 27.481653, F_score: 0.807382, Accuracy: 0.810119\n",
      "Epoch 130/200, Loss: 27.379381, F_score: 0.817135, Accuracy: 0.821337\n",
      "Epoch 131/200, Loss: 27.445601, F_score: 0.809391, Accuracy: 0.811618\n",
      "Epoch 132/200, Loss: 27.390497, F_score: 0.820336, Accuracy: 0.824003\n",
      "Epoch 133/200, Loss: 27.420494, F_score: 0.812185, Accuracy: 0.814173\n",
      "Epoch 134/200, Loss: 27.370398, F_score: 0.824305, Accuracy: 0.827169\n",
      "Epoch 135/200, Loss: 27.450262, F_score: 0.796942, Accuracy: 0.802288\n",
      "Epoch 136/200, Loss: 27.477158, F_score: 0.835250, Accuracy: 0.835333\n",
      "Epoch 137/200, Loss: 27.610489, F_score: 0.772937, Accuracy: 0.786071\n",
      "Epoch 138/200, Loss: 27.781483, F_score: 0.835111, Accuracy: 0.833389\n",
      "Epoch 139/200, Loss: 28.141577, F_score: 0.747043, Accuracy: 0.763968\n",
      "Epoch 140/200, Loss: 31.100737, F_score: 0.667078, Accuracy: 0.706098\n",
      "Epoch 141/200, Loss: 30.960512, F_score: 0.587731, Accuracy: 0.625403\n",
      "Epoch 142/200, Loss: 30.280458, F_score: 0.692994, Accuracy: 0.703432\n",
      "Epoch 143/200, Loss: 28.373470, F_score: 0.772179, Accuracy: 0.771576\n",
      "Epoch 144/200, Loss: 27.581879, F_score: 0.796997, Accuracy: 0.802066\n",
      "Epoch 145/200, Loss: 27.442160, F_score: 0.821343, Accuracy: 0.821559\n",
      "Epoch 146/200, Loss: 27.378878, F_score: 0.806501, Accuracy: 0.812840\n",
      "Epoch 147/200, Loss: 27.345781, F_score: 0.828331, Accuracy: 0.828668\n",
      "Epoch 148/200, Loss: 27.329760, F_score: 0.798498, Accuracy: 0.808231\n",
      "Epoch 149/200, Loss: 27.347092, F_score: 0.832851, Accuracy: 0.831556\n",
      "Epoch 150/200, Loss: 27.425983, F_score: 0.754655, Accuracy: 0.779462\n",
      "Epoch 151/200, Loss: 27.542982, F_score: 0.829474, Accuracy: 0.827113\n",
      "Epoch 152/200, Loss: 27.646791, F_score: 0.716799, Accuracy: 0.756192\n",
      "Epoch 153/200, Loss: 27.594973, F_score: 0.821745, Accuracy: 0.820893\n",
      "Epoch 154/200, Loss: 27.508722, F_score: 0.764839, Accuracy: 0.783794\n",
      "Epoch 155/200, Loss: 27.524864, F_score: 0.824123, Accuracy: 0.823115\n",
      "Epoch 156/200, Loss: 27.546204, F_score: 0.780089, Accuracy: 0.793069\n",
      "Epoch 157/200, Loss: 27.647198, F_score: 0.818028, Accuracy: 0.817783\n",
      "Epoch 158/200, Loss: 27.954227, F_score: 0.791249, Accuracy: 0.796512\n",
      "Epoch 159/200, Loss: 27.773464, F_score: 0.802001, Accuracy: 0.805009\n",
      "Epoch 160/200, Loss: 27.541344, F_score: 0.825097, Accuracy: 0.825558\n",
      "Epoch 161/200, Loss: 27.517479, F_score: 0.807226, Accuracy: 0.812229\n",
      "Epoch 162/200, Loss: 27.442162, F_score: 0.827054, Accuracy: 0.827891\n",
      "Epoch 163/200, Loss: 27.446348, F_score: 0.807061, Accuracy: 0.813062\n",
      "Epoch 164/200, Loss: 27.392792, F_score: 0.828458, Accuracy: 0.829057\n",
      "Epoch 165/200, Loss: 27.406721, F_score: 0.800105, Accuracy: 0.809064\n",
      "Epoch 166/200, Loss: 27.474310, F_score: 0.828409, Accuracy: 0.827335\n",
      "Epoch 167/200, Loss: 27.572735, F_score: 0.775191, Accuracy: 0.791903\n",
      "Epoch 168/200, Loss: 27.916309, F_score: 0.810634, Accuracy: 0.808397\n",
      "Epoch 169/200, Loss: 27.607550, F_score: 0.783519, Accuracy: 0.796845\n",
      "Epoch 170/200, Loss: 27.723751, F_score: 0.812833, Accuracy: 0.810785\n",
      "Epoch 171/200, Loss: 27.515575, F_score: 0.805731, Accuracy: 0.813118\n",
      "Epoch 172/200, Loss: 27.604366, F_score: 0.805850, Accuracy: 0.807620\n",
      "Epoch 173/200, Loss: 27.546885, F_score: 0.825649, Accuracy: 0.827280\n",
      "Epoch 174/200, Loss: 27.702713, F_score: 0.778060, Accuracy: 0.787071\n",
      "Epoch 175/200, Loss: 27.664925, F_score: 0.837205, Accuracy: 0.835444\n",
      "Epoch 176/200, Loss: 27.666527, F_score: 0.745133, Accuracy: 0.768466\n",
      "Epoch 177/200, Loss: 27.538820, F_score: 0.832631, Accuracy: 0.831556\n",
      "Epoch 178/200, Loss: 27.474329, F_score: 0.784558, Accuracy: 0.796901\n",
      "Epoch 179/200, Loss: 27.487442, F_score: 0.828257, Accuracy: 0.828057\n",
      "Epoch 180/200, Loss: 27.466732, F_score: 0.797286, Accuracy: 0.807064\n",
      "Epoch 181/200, Loss: 27.570456, F_score: 0.824221, Accuracy: 0.824003\n",
      "Epoch 182/200, Loss: 27.528633, F_score: 0.783812, Accuracy: 0.797679\n",
      "Epoch 183/200, Loss: 27.740362, F_score: 0.823367, Accuracy: 0.821337\n",
      "Epoch 184/200, Loss: 27.997814, F_score: 0.750006, Accuracy: 0.770632\n",
      "Epoch 185/200, Loss: 28.751080, F_score: 0.779626, Accuracy: 0.773076\n",
      "Epoch 186/200, Loss: 29.030500, F_score: 0.715463, Accuracy: 0.734144\n",
      "Epoch 187/200, Loss: 30.799772, F_score: 0.686468, Accuracy: 0.685327\n",
      "Epoch 188/200, Loss: 33.910191, F_score: 0.354716, Accuracy: 0.448239\n",
      "Epoch 189/200, Loss: 28.924520, F_score: 0.762916, Accuracy: 0.765134\n",
      "Epoch 190/200, Loss: 28.125376, F_score: 0.802075, Accuracy: 0.799567\n",
      "Epoch 191/200, Loss: 27.952993, F_score: 0.800456, Accuracy: 0.802177\n",
      "Epoch 192/200, Loss: 28.072098, F_score: 0.809387, Accuracy: 0.807009\n",
      "Epoch 193/200, Loss: 27.805004, F_score: 0.788188, Accuracy: 0.795402\n",
      "Epoch 194/200, Loss: 27.732010, F_score: 0.822587, Accuracy: 0.820560\n",
      "Epoch 195/200, Loss: 27.617212, F_score: 0.779404, Accuracy: 0.792069\n",
      "Epoch 196/200, Loss: 27.693968, F_score: 0.823357, Accuracy: 0.820726\n",
      "Epoch 197/200, Loss: 27.622929, F_score: 0.772212, Accuracy: 0.787238\n",
      "Epoch 198/200, Loss: 27.787468, F_score: 0.816039, Accuracy: 0.813895\n",
      "Epoch 199/200, Loss: 27.631475, F_score: 0.770940, Accuracy: 0.786349\n",
      "Epoch 200/200, Loss: 27.778149, F_score: 0.814945, Accuracy: 0.814062\n"
     ]
    }
   ],
   "source": [
    "#Model Training\n",
    "\n",
    "Continue_training_from_last_saved_model = True\n",
    "#Continue_training_from_last_saved_model = False\n",
    "\n",
    "if Training_DATASET == 'SKODA':\n",
    "    no_of_class = 11\n",
    "    no_of_sensor_channel = 30\n",
    "    model_classification_weight = SKODA_Classification_weight\n",
    "    model_training_input = SKODA_X_train_tensor\n",
    "    model_training_target = SKODA_y_train_tensor\n",
    "elif Training_DATASET[0: 11] == 'OPPORTUNITY':\n",
    "    no_of_class = int(Training_DATASET.split(' ')[2])\n",
    "    no_of_sensor_channel = 113\n",
    "    model_classification_weight = OPPORTUNITY_class_weight\n",
    "    model_training_input = OPPORTUNITY_X_train\n",
    "    model_training_target = OPPORTUNITY_y_train\n",
    "    model_testing_input = OPPORTUNITY_X_test\n",
    "    model_testing_target = OPPORTUNITY_y_test\n",
    "else:\n",
    "    print('Please select a Dataset to train the model.')\n",
    "\n",
    "Trained_Model_saving_path = 'Trained_model_pt/' + Training_DATASET + '_Class_' + str(no_of_class) + '.pt'\n",
    "\n",
    "\n",
    "\n",
    "model_training_input = torch.reshape(model_training_input, (model_training_input.size()[0], 1, model_training_input.size()[1], model_training_input.size()[2])).float()\n",
    "\n",
    "\n",
    "epoch_size = 200\n",
    "steps_for_printing_out_loss = 1\n",
    "BATCH_SIZE = 1200\n",
    "#BATCH_SIZE = 100\n",
    "learning_rate = 0.001\n",
    "#learning_rate = 0.02\n",
    "\n",
    "CNN_LSTM_HAR_MODEL_WIP = CNN_LSTM_HAR_MODEL(SILDE_WINDOW, no_of_class, no_of_sensor_channel).to(device)\n",
    "\n",
    "#loss_functioin = nn.CrossEntropyLoss(weight = (torch.tensor([0.2/0.8/17, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])/ (0.2/0.8/17 + 17)).cuda())\n",
    "#loss_functioin = nn.CrossEntropyLoss(weight = torch.tensor(class_count).float().cuda())\n",
    "#loss_functioin = nn.CrossEntropyLoss()\n",
    "loss_functioin = nn.CrossEntropyLoss(weight = torch.tensor(model_classification_weight).float().to(device))\n",
    "\n",
    "#optimizer = optim.SGD(CNN_LSTM_HAR_MODEL_WIP.parameters(), lr = learning_rate)\n",
    "#https://pytorch.org/docs/stable/optim.html\n",
    "optimizer = torch.optim.RMSprop(CNN_LSTM_HAR_MODEL_WIP.parameters(), lr = learning_rate, weight_decay = 0.9)\n",
    "\n",
    "\n",
    "if Continue_training_from_last_saved_model == True:\n",
    "    CNN_LSTM_HAR_MODEL_WIP.load_state_dict(torch.load(Trained_Model_saving_path)['state_dict'])\n",
    "    optimizer.load_state_dict(torch.load(Trained_Model_saving_path)['optimizer'])\n",
    "    CNN_LSTM_HAR_MODEL_WIP.train()\n",
    "\n",
    "\n",
    "\n",
    "training_data_size = list(model_training_input.size())[0]\n",
    "torch.cuda.empty_cache()\n",
    "for i in range(1, epoch_size + 1):\n",
    "    optimizer.zero_grad()\n",
    "    torch.cuda.empty_cache()\n",
    "    training_no_of_right = 0\n",
    "    training_epoch_loss = 0\n",
    "    for Batch_no in range(int(training_data_size / BATCH_SIZE) + 1):\n",
    "        \"\"\"\n",
    "        if Batch_no % 80 == 1:\n",
    "            print(\"Batch {}/{}\".format(str(Batch_no), int(data_size / BATCH_SIZE)))\n",
    "        \"\"\"\n",
    "        batch_start = Batch_no * BATCH_SIZE\n",
    "        batch_end = min(training_data_size, (Batch_no + 1) * BATCH_SIZE)\n",
    "        \n",
    "        training_input = model_training_input[batch_start: batch_end].to(device)\n",
    "        training_target = model_training_target[batch_start: batch_end].to(device)\n",
    "\n",
    "        training_output = CNN_LSTM_HAR_MODEL_WIP(training_input).to(device)\n",
    "        training_loss = loss_functioin(training_output[:, -1, :], training_target.long()).to(device)\n",
    "        training_epoch_loss += training_loss\n",
    "        training_loss.backward()\n",
    "        if i == 1 or i % (steps_for_printing_out_loss) == 0:\n",
    "            #print('Loss (epoch: ' + str(i) + '): ' + str(loss.cpu().detach().numpy()))\n",
    "            \n",
    "            if Batch_no == 0:\n",
    "                training_All_output = training_output[:, -1, :].argmax(1)\n",
    "            else:\n",
    "                training_All_output = torch.cat((training_All_output, training_output[:, -1, :].argmax(1)), 0)\n",
    "            \n",
    "            for k in range(len(training_target)):\n",
    "                if training_output[:, -1, :].argmax(1)[k] == training_target[k]:\n",
    "                    #print('--------')\n",
    "                    #print(output[k])\n",
    "                    #print(target[k])\n",
    "                    training_no_of_right += 1\n",
    "    if i == 1 or i % (steps_for_printing_out_loss) == 0:\n",
    "        training_F_score = f1_score(model_training_target.numpy(), training_All_output.cpu().detach().numpy(), average='weighted')\n",
    "        \n",
    "        pd.DataFrame(model_training_target.numpy()).to_csv('export/' + 'y_target' + '.csv')\n",
    "        pd.DataFrame(training_All_output.cpu().detach().numpy()).to_csv('export/' + 'y_predict' + '.csv')\n",
    "        #print(CNN_LSTM_HAR_MODEL_WIP.cnn2d_3.weight.grad)\n",
    "        print(\"Epoch {}/{}, Loss: {:.6f}, F_score: {:.6f}, Accuracy: {:.6f}\".format(i, epoch_size, training_epoch_loss.cpu().detach().numpy(), training_F_score, training_no_of_right / training_data_size))\n",
    "    optimizer.step()\n",
    "torch.save({'state_dict': CNN_LSTM_HAR_MODEL_WIP.state_dict(),'optimizer': optimizer.state_dict()}, Trained_Model_saving_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-a96ba3aab008>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-16-a96ba3aab008>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    stop here\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "stop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########Script pending to be cleaned###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_OPPORTUNITY_DATASET_All_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPPORTUNITY_DATASET_FILE_LIST_Training\n",
    "OPPORTUNITY_DATASET_FILE_LIST_Validation\n",
    "OPPORTUNITY_DATASET_FILE_LIST_Testing\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = df_OPPORTUNITY_DATASET_Testing['33 Accelerometer LWR accY'].unique()\n",
    "for m in rr:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPPORTUNITY_DATASET_Training.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPPORTUNITY_DATASET_Testing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPPORTUNITY_DATASET_Validation.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPPORTUNITY_DATASET_Training['file_name'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPPORTUNITY_DATASET_All_describe['2 Accelerometer RKN^ accX']['count']\n",
    "df_OPPORTUNITY_DATASET_All_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPPORTUNITY_DATASET_Training['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#len(OPPORTUNITY_column_name_selected)\n",
    "#df_OPPORTUNITY_DATASET_WIP = df_OPPORTUNITY_DATASET[OPPORTUNITY_column_name_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPPORTUNITY_column_name_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_OPPORTUNITY_DATASET_Validation_WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Training_x, Training_y_1, Training_y_2 = data_generator(df_OPPORTUNITY_DATASET_Training_WIP, SILDE_WINDOW, SILDE_WINDOW_STEP, OPPORTUNITY_column_name_selected)\n",
    "Training_x = torch.reshape(Training_x, (Training_x.size()[0], 1, Training_x.size()[1], Training_x.size()[2])).float()\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Testing_x, Testing_y_1, Testing_y_2 = data_generator(df_OPPORTUNITY_DATASET_Testing_WIP, SILDE_WINDOW, SILDE_WINDOW_STEP, OPPORTUNITY_column_name_selected)\n",
    "Testing_x = torch.reshape(Testing_x, (Testing_x.size()[0], 1, Testing_x.size()[1], Testing_x.size()[2])).float()\n",
    "\n",
    "Validation_x, Validation_y_1, Validation_y_2 = data_generator(df_OPPORTUNITY_DATASET_Validation_WIP, SILDE_WINDOW, SILDE_WINDOW_STEP, OPPORTUNITY_column_name_selected)\n",
    "Validation_x = torch.reshape(Validation_x, (Validation_x.size()[0], 1, Validation_x.size()[1], Validation_x.size()[2])).float()\n",
    "\"\"\"\n",
    "#def batch_generator():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_OPPORTUNITY_DATASET_Validation_WIP[OPPORTUNITY_column_name_selected[1: -3]][1: 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_OPPORTUNITY_DATASET_WIP['22 Accelerometer RKN_ accZ'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train[0:5000]).to_csv('sdf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SILDE_WINDOW = 24\n",
    "SILDE_WINDOW_STEP = 12\n",
    "\n",
    "\n",
    "def data_generator(ARRAY_OPPORTUNITY_DATASET_x, ARRAY_OPPORTUNITY_DATASET_y, SILDE_WINDOW, SILDE_WINDOW_STEP):\n",
    "    row_no_start = 0\n",
    "    count_index = 0\n",
    "    while (row_no_start + SILDE_WINDOW - 1) <= len(ARRAY_OPPORTUNITY_DATASET_x):\n",
    "        count_index += 1\n",
    "        if count_index % 500 == 0:\n",
    "            print(count_index)\n",
    "        row_no_end = row_no_start + SILDE_WINDOW\n",
    "        df_input = ARRAY_OPPORTUNITY_DATASET_x[row_no_start: row_no_end]\n",
    "        current_input = torch.tensor([df_input])\n",
    "        df_output_Locomotion = ARRAY_OPPORTUNITY_DATASET_y[row_no_end - 1]\n",
    "        df_output_ML_Both_Arms = torch.tensor([df_output_Locomotion])\n",
    "        current_output_ML_Both_Arms = torch.tensor([df_output_ML_Both_Arms])\n",
    "\n",
    "\n",
    "        if row_no_start == 0:\n",
    "            input_tensor = current_input\n",
    "            output_ML_Both_Arms_tensor = current_output_ML_Both_Arms\n",
    "        else:\n",
    "            input_tensor = torch.cat((input_tensor, current_input), 0)\n",
    "            output_ML_Both_Arms_tensor = torch.cat((output_ML_Both_Arms_tensor, current_output_ML_Both_Arms), 0)\n",
    "        row_no_start += SILDE_WINDOW_STEP\n",
    "    return input_tensor, output_ML_Both_Arms_tensor\n",
    "\n",
    "\n",
    "#X_train_tensor_1, y_train_tensor_1 = data_generator(X_train[0: 40000], y_train[0: 40000], SILDE_WINDOW, SILDE_WINDOW_STEP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SILDE_WINDOW = 24\n",
    "SILDE_WINDOW_STEP = 12\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "X_train_tensor, y_train_tensor = data_generator(X_train, y_train, SILDE_WINDOW, SILDE_WINDOW_STEP)\n",
    "\n",
    "pd.DataFrame(y_train_tensor.numpy()).to_csv('export/' + 'y_target' + '.csv')\n",
    "\n",
    "df_target = pd.DataFrame(y_train_tensor.numpy())\n",
    "df_target.columns = ['classification']\n",
    "class_count = df_target['classification'].value_counts()\n",
    "class_count = class_count.sort_index(0)\n",
    "class_count = class_count.tolist()\n",
    "class_count = 1 / (np.array(class_count) / (sum(class_count) / len(class_count)))\n",
    "class_count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(class_count)\n",
    "#df_target['abc'].nunique()\n",
    "#df_target\n",
    "#print(X_train_tensor[0])\n",
    "#input_tensor.size()\n",
    "#output_tensor.size()\n",
    "#print(type(class_count))\n",
    "\n",
    "#Class_index = list(class_count.index.values)\n",
    "\n",
    "\n",
    "\n",
    "pd.DataFrame(X_train_tensor[0]).to_csv('0.csv')\n",
    "\n",
    "pd.DataFrame(y_train).to_csv('y.csv')\n",
    "\n",
    "Validation_x = X_train_tensor\n",
    "Validation_x = torch.reshape(Validation_x, (Validation_x.size()[0], 1, Validation_x.size()[1], Validation_x.size()[2])).float()\n",
    "\n",
    "Validation_y_2 = y_train_tensor\n",
    "\n",
    "\n",
    "pd.DataFrame(Validation_x[0, 0, :, :]).to_csv('0v.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor.unique()\n",
    "X_train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train_tensor.unique()\n",
    "#torch.save({'X_train_tensor': X_train_tensor, 'y_train_tensor': y_train_tensor}, 'C:/Users/HX/Magic/training_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.unique(right_classall_clean_array_y)\n",
    "#dict_SKODA_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data_loading = torch.load(\"D:/Installation/yolov4.pt\")\n",
    "X_train_tensor = data_loading['X_train_tensor']\n",
    "y_train_tensor = data_loading['y_train_tensor']\n",
    "data_loading = ''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_tensor.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pending items:\n",
    "    - batch size = 100\n",
    "    - sensor data were pre-processed to fill in missing values using linear interpolation\n",
    "    - model accuracy & loss function save into array\n",
    "    - Dashboard - to show performance\n",
    "    \n",
    "    - per channel normalization to interval [0,1]\n",
    "    https://stackoverflow.com/questions/26414913/normalize-columns-of-pandas-data-frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input = np.array([random() for i in range(24 * 113 * 1 * 6)]).reshape(6, 1, 24, 113)\n",
    "\n",
    "output = torch.Tensor([[1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                     [0.0, 1.0, 0.0, 0.0, 0.0],\n",
    "                     [0.0, 0.0, 0.0, 1.0, 0.0],\n",
    "                     [0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "                     [0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "                     [0.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "\n",
    "\n",
    "output = torch.Tensor([0, 1, 3, 2, 2, 4])\n",
    "\n",
    "final_input = torch.Tensor(input)\n",
    "final_output = torch.Tensor(output)\n",
    "\"\"\"\n",
    "\n",
    "class CNN_LSTM_HAR_MODEL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn2d_1 = nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size = (5, 1))\n",
    "        #self.conv_bn = nn.BatchNorm2d(64)\n",
    "        #self.relu = nn.ReLU()\n",
    "        self.cnn2d_2 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (5, 1))\n",
    "        self.cnn2d_3 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (5, 1))\n",
    "        self.cnn2d_4 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (5, 1))\n",
    "        \n",
    "        self.batch_size = 24 - 4 * (5 - 1)\n",
    "        self.hidden_dim = 128\n",
    "        self.input_size = 7232\n",
    "        self.input_size = 1920\n",
    "        self.n_layers = 2\n",
    "        \n",
    "        self.class_no = 11\n",
    "        \n",
    "        #self.lstm1 = nn.LSTM(batch_first = True, input_size  = 7232, hidden_size = self.hidden_dim, num_layers = self.n_layers)\n",
    "        self.lstm1 = nn.LSTM(batch_first = True, input_size  = self.input_size, hidden_size = self.hidden_dim, dropout = 0.5)\n",
    "        self.lstm2 = nn.LSTM(batch_first = True, input_size  = 128, hidden_size = self.hidden_dim, dropout = 0.5)\n",
    "        \n",
    "        #self.lstm1 = nn.LSTM(batch_first = True, input_size  = 7232, hidden_size = self.hidden_dim)\n",
    "        #self.lstm2 = nn.LSTM(batch_first = True, input_size  = 128, hidden_size = self.hidden_dim)\n",
    "        \n",
    "        #self.conv_bn2 = nn.BatchNorm1d(8)\n",
    "        #, dropout = 0.5\n",
    "        #self.input_size = 128\n",
    "        #self.n_layers = 1\n",
    "        #self.lstm2 = nn.LSTM(input_size  = self.input_size, hidden_size = self.hidden_dim, num_layers = self.n_layers, dropout = 0.5)\n",
    "        \n",
    "        #self.fc1 = nn.Linear(128, 18)\n",
    "        self.fc1 = nn.Linear(128, self.class_no)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        x = self.cnn2d_1(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.conv_bn(x)\n",
    "        #tensor_to_csv(x[0, -1, :, :], 'layer_1_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :, :], 'layer_1_data_100')\n",
    "        \n",
    "        x = self.cnn2d_2(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.conv_bn(x)\n",
    "        #tensor_to_csv(x[0, -1, :, :], 'layer_2_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :, :], 'layer_2_data_100')\n",
    "\n",
    "        x = self.cnn2d_3(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.conv_bn(x)\n",
    "        #tensor_to_csv(x[0, -1, :, :], 'layer_3_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :, :], 'layer_3_data_100')\n",
    "\n",
    "        x = self.cnn2d_4(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.conv_bn(x)\n",
    "        #tensor_to_csv(x[0, -1, :, :], 'layer_4_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :, :], 'layer_4_data_100')\n",
    "\n",
    "        #print(x.size())\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        #print(x.size())\n",
    "        x = torch.reshape(x, (x.size()[0], x.size()[1], -1))\n",
    "\n",
    "        #hidden = self.init_hidden()\n",
    "        #hidden = (torch.randn(2, x.size()[0], self.hidden_dim).cuda(), torch.randn(2, x.size()[0], self.hidden_dim).cuda())\n",
    "        #x, hidden = self.lstm1(x, hidden)\n",
    "        x, hidden = self.lstm1(x)\n",
    "        x, hidden = self.lstm2(x)\n",
    "        #print(x)\n",
    "        #x = self.conv_bn2(x)\n",
    "        #hidden = self.conv_bn2(hidden)\n",
    "        #tensor_to_csv(x[0, -1, :,], 'layer_lstm1_data_0')\n",
    "        #tensor_to_csv(x[5, -1, :], 'layer_lstm1_data_100')\n",
    "        #print(x)\n",
    "        #hidden2 = (torch.zeros(self.n_layers, 8, self.hidden_dim).requires_grad_(), torch.zeros(self.n_layers, 8, self.hidden_dim).requires_grad_())\n",
    "        ###############################\n",
    "        #x, hidden = self.lstm2(x, hidden)\n",
    "        #print(x)\n",
    "        #x = self.conv_bn2(x)\n",
    "        \n",
    "        #tensor_to_csv(x[0, -1, :,], 'layer_lstm2_data_0')\n",
    "        #tensor_to_csv(x[15, -1, :], 'layer_lstm2_data_100')\n",
    "        #print(x.size())\n",
    "        x = torch.reshape(x, (-1, 128))\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.softmax(x, dim = 1)\n",
    "        #x = F.softmax(x)\n",
    "        x = torch.reshape(x, (int(x.size()[0] / 8), 8, self.class_no))\n",
    "        x = x[:, -1, :]\n",
    "        #print(x.size())\n",
    "        tensor_to_csv(x, 'layer_fc1_data')\n",
    "        \n",
    "        \n",
    "        tensor_to_csv(x, 'output_data')\n",
    "        \n",
    "        \n",
    "        #print(x)\n",
    "        return x\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        hidden = (weight.new(self.n_layers, self.batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, self.batch_size, self.hidden_dim).zero_())\n",
    "        return hidden\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(Validation_y_2.size())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_csv(tensor_name):\n",
    "    x_np = tensor_name.numpy()\n",
    "    x_df = pd.DataFrame(x_np)\n",
    "    x_df.to_csv('tmp.csv')\n",
    "\n",
    "Validation_x.size()\n",
    "#output\n",
    "\n",
    "tensor_to_csv(output.detach())\n",
    "\n",
    "\n",
    "#Validation_x_to_csv = torch.reshape(Validation_x, (Validation_x.size()[0] * Validation_x.size()[1] * Validation_x.size()[2], Validation_x.size()[3])).float()\n",
    "#tensor_to_csv(Validation_x_to_csv.detach())\n",
    "#tensor_to_csv(Validation_x[1, 0,:, :])\n",
    "\n",
    "#tensor_to_csv(Validation_y_2)\n",
    "\n",
    "for row_no_end in range(0, 155):\n",
    "    print(row_no_end)\n",
    "    print(df_OPPORTUNITY_DATASET_Validation_WIP['250 ML_Both_Arms'][row_no_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input.size()\n",
    "target\n",
    "CNN_LSTM_HAR_MODEL_WIP.parameters()\n",
    "target\n",
    "Validation_y_2.sum()\n",
    "print(Validation_y_2)\n",
    "for i in Validation_y_2.long():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in CNN_LSTM_HAR_MODEL_WIP.parameters():\n",
    "    print(i.size())\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.isnan(Validation_x)\n",
    "\n",
    "\n",
    "\n",
    "x_np = Validation_x[0, 0,:, :].numpy()\n",
    "x_df = pd.DataFrame(x_np)\n",
    "x_df.to_csv('tmp.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> rnn = nn.LSTM(10, 20, 2)\n",
    ">>> input = torch.randn(5, 3, 10)\n",
    ">>> h0 = torch.randn(2, 3, 20)\n",
    ">>> c0 = torch.randn(2, 3, 20)\n",
    ">>> output, (hn, cn) = rnn(input, (h0, c0))\n",
    "\n",
    "output.size()\n",
    "#hn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = torch.tensor([[[k* 100 + j * 10 + i for i in range (10)] for j in range(10)] for k in range(10)])\n",
    "kk\n",
    "#mm = torch.reshape(kk, (kk.size()[0], 1, kk.size()[1], kk.size()[2])).float()\n",
    "mm = torch.reshape(kk, (kk.size()[0], -1))\n",
    "\n",
    "\n",
    "print(mm.size())\n",
    "\n",
    "print(mm[0])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "mm[:, 0, :, :] == kk\n",
    "mm[5, 0, 4, 3]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for para in CNN_LSTM_HAR_MODEL_WIP.parameters():\n",
    "    print(para.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPPORTUNITY_DATASET_Training_WIP[OPPORTUNITY_column_name_selected][0:2000].to_csv('def.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.2/0.8/17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in CNN_LSTM_HAR_MODEL_WIP.parameters():\n",
    "    print(i.size())\n",
    "    #print(i)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
